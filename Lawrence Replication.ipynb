{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of the paper by Lawrence et al., 2006\n",
    "\n",
    "https://papers.nips.cc/paper/3119-modelling-transcriptional-regulation-using-gaussian-processes.pdf\n",
    "\n",
    "#### Probesets\n",
    "\n",
    "The original paper restricted their interest to 5 known targets of p53: \n",
    "- DDB2 -------------- (probeset 203409_at)\n",
    "- p21 ----------------- (probeset 202284_s_at) (alias p21CIP1, CDKN1A)\n",
    "- SESN1/hPA26 -- (probeset 218346_s_at)\n",
    "- BIK ----------------- (probeset 205780_at)\n",
    "- TNFRSF10b ----- (probeset 209294_x_at, 209295_at, 210405_x_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gpflow\n",
    "from gpflow.utilities import print_summary, positive\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import mcmc\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from reggae.data_loaders import load_barenco_puma, DataHolder\n",
    "from reggae.gp import LinearResponseModel\n",
    "from reggae.gp.options import Options\n",
    "from reggae.plot.gp_plotters import Plotter\n",
    "from reggae.utilities import broadcast_tile, PI\n",
    "\n",
    "f64 = np.float64\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.4f}\".format(x)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_observed, f_observed, σ2_m_pre, σ2_f_pre, t = load_barenco_puma()\n",
    "\n",
    "m_df, m_observed = m_observed \n",
    "f_df, f_observed = f_observed\n",
    "# Shape of m_observed = (replicates, genes, times)\n",
    "m_observed = m_observed\n",
    "f_observed = f_observed\n",
    "data = (m_observed, f_observed)\n",
    "\n",
    "σ2_m_pre = f64(σ2_m_pre)\n",
    "σ2_f_pre = f64(σ2_f_pre)\n",
    "noise_data = (σ2_m_pre, σ2_f_pre)\n",
    "\n",
    "display(m_df)\n",
    "\n",
    "num_genes = m_observed.shape[1]\n",
    "N_m = m_observed.shape[2]\n",
    "granularity = 100\n",
    "τ = np.linspace(0, 12, granularity)\n",
    "time = (t, τ, None)\n",
    "data = DataHolder(data, noise_data, time)\n",
    "options = Options()\n",
    "\n",
    "model = LinearResponseModel(data, options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = Plotter(data, model, m_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granularity = 100\n",
    "pred_t = np.linspace(-2, 14, granularity, dtype='float64')\n",
    "plt.figure(figsize=(7, 4))\n",
    "mu_post = plotter.plot_tf(pred_t)\n",
    "\n",
    "k = model.internal_model.kernel\n",
    "K_xx = k.K(model.X, None)\n",
    "K_inv = tf.linalg.inv(K_xx)\n",
    "Kxf = k.K_xf(model.X, pred_t)\n",
    "KfxKxx = tf.matmul(tf.transpose(Kxf), K_inv)\n",
    "Kff = k.K_diag(pred_t)\n",
    "print(Kff.shape)\n",
    "Kff = Kff- tf.matmul(KfxKxx, Kxf)\n",
    "var = tf.sqrt(tf.linalg.diag_part(Kff))\n",
    "print(var.shape)\n",
    "mu_post = tf.reshape(mu_post, -1)\n",
    "plt.fill_between(pred_t, mu_post -var, mu_post+var, alpha=0.4)\n",
    "print(Kff.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plotter.plot_kinetics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 13))\n",
    "pred_t = np.linspace(0, 14, granularity, dtype='float64')\n",
    "plotter.plot_genes(pred_t);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probeset Combination\n",
    "\n",
    "TNFRSF10b has multiple probesets (probeset 209294_x_at, 209295_at, 210405_x_at) which should be combined.\n",
    "\n",
    "It can be observed below that the log intensities have a similar pattern. Thus a popular way to combine is to take the average of the log intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINE_MULTIPLE_PROBESETS = False\n",
    "if COMBINE_MULTIPLE_PROBESETS:\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.subplot(2,2,1)\n",
    "    p53 = df[df.index.isin(['211300_s_at', '201746_at'])][columns].astype(float)\n",
    "    for index, row in p53.iterrows():\n",
    "        p53.loc[index] = np.log(list(row))\n",
    "        plt.plot(list(row))\n",
    "\n",
    "    p53_mean = pd.Series(p53.mean(0), index=genes.columns, name='p53')\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    TNFRSF10b = df[df.index.isin(['209294_x_at', '209295_at', '210405_x_at'])][columns]\n",
    "    for index, row in TNFRSF10b.iterrows():\n",
    "        print(list(row))\n",
    "        TNFRSF10b.loc[index] = np.log(list(row))\n",
    "        plt.plot(list(row))\n",
    "\n",
    "    TNFRSF10b_mean = pd.Series(TNFRSF10b.mean(0), index=genes.columns, name='TNFRSF10b')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(p53_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We fix the sensitivity of p21 to be 1, and decay to be 0.8 as in Barenco et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            K_xx = np.zeros([X.shape[0],X.shape[0]], dtype='float64')\n",
    "            print(K_xx.shape)\n",
    "            for j in range(num_genes):\n",
    "                for k in range(num_genes):\n",
    "                    K_xx[j*block_size:(j+1)*block_size, \n",
    "                         k*block_size:(k+1)*block_size] = self.k_xx(j, k, X)\n",
    "            return K_xx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class ExpressionKernel(gpflow.kernels.Kernel):\n",
    "    def __init__(self):\n",
    "        super().__init__(active_dims=[0])\n",
    "        \n",
    "#         l_affine = tfb.AffineScalar(shift=tf.cast(1., tf.float64),\n",
    "#                             scale=tf.cast(4-1., tf.float64))\n",
    "#         l_sigmoid = tfb.Sigmoid()\n",
    "#         l_logistic = tfb.Chain([l_affine, l_sigmoid])\n",
    "\n",
    "        self.lengthscale = gpflow.Parameter(1.414, transform=positive())\n",
    "#         self.white_variance = gpflow.Parameter(0.1, transform=positive())\n",
    "\n",
    "        D_affine = tfb.AffineScalar(shift=tf.cast(0.1, tf.float64),\n",
    "                                    scale=tf.cast(1.5-0.1, tf.float64))\n",
    "        D_sigmoid = tfb.Sigmoid()\n",
    "        D_logistic = tfb.Chain([D_affine, D_sigmoid])\n",
    "        S_affine = tfb.AffineScalar(shift=tf.cast(0.1, tf.float64),\n",
    "                                    scale=tf.cast(4.-0.1, tf.float64))\n",
    "        S_sigmoid = tfb.Sigmoid()\n",
    "        S_logistic = tfb.Chain([S_affine, S_sigmoid])\n",
    "\n",
    "        self.D = gpflow.Parameter(np.random.uniform(0.9, 1, num_genes), transform=positive(), dtype=tf.float64)\n",
    "#         self.D[3].trainable = False\n",
    "#         self.D[3].assign(0.8)\n",
    "        self.kervar = gpflow.Parameter(np.float64(1), transform=positive())\n",
    "        self.S = gpflow.Parameter(np.random.uniform(1,1, num_genes), transform=positive(), dtype=tf.float64)\n",
    "#         self.S[3].trainable = False\n",
    "#         self.S[3].assign(1)\n",
    "        self.noise_term = gpflow.Parameter(0.1353*tf.ones(num_genes, dtype='float64'), transform=positive())\n",
    "        \n",
    "    def K(self, X, X2=None):\n",
    "        self.block_size = int(X.shape[0]/num_genes)\n",
    "        if X2 is None:\n",
    "            shape = [X.shape[0],X.shape[0]]\n",
    "            K_xx = tf.zeros(shape, dtype='float64')\n",
    "            K_xx = self.k_xx(X, 0, 0)#+self.noise_term*tf.linalg.eye(self.block_size, dtype='float64')\\n\",\n",
    "\n",
    "    #         if X2 is None:\n",
    "            shape = [X.shape[0],X.shape[0]]\n",
    "    #         K_xx = self.k_xx(X, 0,0)        \n",
    "            white = tf.linalg.diag(broadcast_tile(tf.reshape(self.noise_term, (1, -1)), 1, self.block_size)[0])\n",
    "            return K_xx + tf.linalg.diag((1e-5*tf.ones(X.shape[0], dtype='float64'))+model.Y_var) + white\n",
    "        else:\n",
    "            '''Calculate K_xf: no need to use tf.* since this part is not optimised'''\n",
    "            shape = [X.shape[0],X2.shape[0]]#self.block_size]\n",
    "\n",
    "            K_xf = tf.zeros(shape, dtype='float64')\n",
    "            for j in range(num_genes):\n",
    "                mask = np.ones(shape)\n",
    "                other = np.zeros(shape)\n",
    "                mask[j*self.block_size:(j+1)*self.block_size] = 0\n",
    "                pad_top = j*self.block_size\n",
    "                pad_bottom = 0 if j == num_genes-1 else shape[0]-self.block_size-pad_top\n",
    "                kxf = self.k_xf(j, X, X2)\n",
    "                other = tf.pad(kxf,\n",
    "                               tf.constant([[pad_top,pad_bottom],[0,0]]), 'CONSTANT'\n",
    "                              )\n",
    "\n",
    "                K_xf = K_xf * mask + other * (1 - mask)\n",
    "                #[j*self.block_size:(j+1)*self.block_size] = \n",
    "            return K_xf\n",
    "        \n",
    "    def k_xf(self, j, X, X2):\n",
    "        t_prime, t_, t_dist = self.get_distance_matrix(t_x=tf.reshape(X[:self.block_size], (-1,)), \n",
    "                                                       t_y=X2)\n",
    "        l = self.lengthscale\n",
    "        erf_term = tfm.erf(t_dist/l - self.gamma(j)) + tfm.erf(t_/l + self.gamma(j))\n",
    "\n",
    "        return self.S[j]*l*0.5*tfm.sqrt(PI)*tfm.exp(self.gamma(j)**2) *tfm.exp(-self.D[j]*t_dist)*erf_term \n",
    "\n",
    "    def gamma(self):\n",
    "        return self.D*self.lengthscale/2\n",
    "\n",
    "    def _h(self, X, k, j, t_y=None, primefirst=True):\n",
    "        l = self.lengthscale\n",
    "#         print(l, self.D[k], self.D[j])\n",
    "        t_x = tf.reshape(X[:self.block_size], (-1,))\n",
    "        t_prime, t, t_dist = self.get_distance_matrix(primefirst=primefirst, t_x=t_x, t_y=t_y)\n",
    "        multiplier = tfm.exp(self.gamma(k)**2) / (self.D[j]+self.D[k])\n",
    "        first_erf_term = tfm.erf(t_dist/l - self.gamma(k)) + tfm.erf(t/l + self.gamma(k))\n",
    "        second_erf_term = tfm.erf(t_prime/l - self.gamma(k)) + tfm.erf(self.gamma(k))\n",
    "        return multiplier * (tf.multiply(tfm.exp(-self.D[k]*t_dist) , first_erf_term) - \\\n",
    "                             tf.multiply(tfm.exp(-self.D[k]*t_prime-self.D[j]*t) , second_erf_term))\n",
    "    \n",
    "    def _gamma(self, k):\n",
    "        return self.D[k]*self.lengthscale/2\n",
    "    def _k_xx(self, X, j, k, t_y=None):\n",
    "        '''k_xx(t, tprime)'''\n",
    "        mult = self.S[j]*self.S[k]*self.lengthscale*0.5*tfm.sqrt(PI)\n",
    "        return self.kervar**2*mult*(self.h(X, k, j, t_y=t_y) + self.h(X, j, k, t_y=t_y, primefirst=False))\n",
    "    \n",
    "    def h(self, X, k, j, primefirst=True):\n",
    "        Dj = tf.reshape(self.D, (1, -1))\n",
    "        Dj = broadcast_tile(Dj, 1, 7)\n",
    "        Dj = tf.tile(Dj, [35, 1])\n",
    "        Dk = tf.reshape(self.D, (-1, 1)) \n",
    "        Dk = broadcast_tile(Dk, 7, 1)\n",
    "        Dk = tf.tile(Dk, [1, 35])\n",
    "        gk = tf.transpose(broadcast_tile(tf.reshape(self.gamma(), (-1, 1)), 7, 1))\n",
    "        gk = tf.tile(gk, [35, 1])\n",
    "        if not primefirst:\n",
    "            Dk, Dj = Dj, Dk\n",
    "            gk = tf.transpose(broadcast_tile(tf.reshape(self.gamma(), (1,-1)), 1, 7))\n",
    "            gk = tf.tile(gk, [1, 35])\n",
    "\n",
    "        l = self.lengthscale\n",
    "        t_x = tf.reshape(X[:self.block_size], (-1,))\n",
    "        t_prime, t, t_dist = self.get_distance_matrix(primefirst=primefirst, t_x=t_x)\n",
    "        t_prime = tf.tile(t_prime, [5, 5])\n",
    "        t = tf.tile(t, [5, 5])\n",
    "        t_dist = tf.tile(t_dist, [5, 5])\n",
    "        multiplier = tfm.exp(gk**2) / (Dj + Dk)\n",
    "        first_erf_term = tfm.erf(t_dist/l - gk) + tfm.erf(t/l + gk)\n",
    "        second_erf_term = tfm.erf(t_prime/l - gk) + tfm.erf(gk)\n",
    "        return multiplier * (tf.multiply(tfm.exp(-Dk*t_dist) , first_erf_term) - \\\n",
    "                             tf.multiply(tfm.exp(-Dk*t_prime-Dj*t) , second_erf_term))\n",
    "\n",
    "\n",
    "    def k_xx(self, X, j, k):\n",
    "        S_square = tf.matmul(tf.reshape(self.S, (-1, 1)), tf.reshape(self.S, (1, -1)))\n",
    "        S_square = broadcast_tile(S_square, 7, 7)\n",
    "        mult = S_square*self.lengthscale*0.5*tfm.sqrt(PI)\n",
    "        return self.kervar**2*mult*(self.h(X, k, j) + self.h(X, j, k, primefirst=False))\n",
    "\n",
    "    def get_distance_matrix(self, t_x, primefirst=True, t_y=None):\n",
    "        if t_y is None:\n",
    "            t_y = t_x\n",
    "        t_1 = tf.transpose(tf.reshape(tf.tile(t_x, [t_y.shape[0]]), [ t_y.shape[0], t_x.shape[0]]))\n",
    "        t_2 = tf.reshape(tf.tile(t_y, [t_x.shape[0]]), [ t_x.shape[0], t_y.shape[0]])\n",
    "        if primefirst:\n",
    "            return t_1, t_2, t_1-t_2\n",
    "        return t_2, t_1, t_2-t_1\n",
    "    \n",
    "    def K_diag(self, X):\n",
    "        print('k_diag')\n",
    "\n",
    "        \"\"\"I've used the fact that we call this method for K_ff when finding the covariance as a hack so\n",
    "        I know if I should return K_ff or K_xx. In this case we're returning K_ff!!\n",
    "        $K_{ff}^{post} = K_{ff} - K_{fx} K_{xx}^{-1} K_{xf}$\"\"\"\n",
    "        _,_,t_dist = self.get_distance_matrix(t_x=tf.reshape(X, (-1,)))\n",
    "        K_ff = tf.math.exp(-(t_dist**2)/(2*self.lengthscale**2))\n",
    "        return (K_ff)\n",
    "\n",
    "\n",
    "k_exp = ExpressionKernel()\n",
    "print_summary(k_exp, fmt='notebook')\n",
    "print(k_exp.K(model.X)[:7,:7])\n",
    "# print(k_exp.K(X, np.linspace(0, 12, 100), calc_Kxx=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reggae.gp import LinearResponseMeanFunction\n",
    "meanfunc_exp = LinearResponseMeanFunction(data, k_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpflow.logdensities import multivariate_normal\n",
    "class GPR(gpflow.models.GPR):\n",
    "    def log_marginal_likelihood(self) -> tf.Tensor:\n",
    "        r\"\"\"\n",
    "        Computes the log marginal likelihood.\n",
    "\n",
    "        .. math::\n",
    "            \\log p(Y | \\theta).\n",
    "\n",
    "        \"\"\"\n",
    "        X, Y = self.data\n",
    "        K = self.kernel(X)\n",
    "        m = self.mean_function(X)\n",
    "        var = tf.linalg.diag(self.likelihood.variance*tf.ones(35, dtype='float64'))\n",
    "        invK = tf.linalg.inv(K+var)\n",
    "        L = tf.linalg.cholesky(K+var)\n",
    "        log_prob = -0.5 * tf.linalg.matmul(tf.transpose(Y-m), tf.linalg.lstsq(tf.transpose(L), tf.linalg.lstsq(L, Y-m)))#tf.matmul(tf.matmul(tf.transpose(Y-m), invK), (Y-m))\n",
    "        log_prob -= 0.5 * tf.reduce_sum(tfm.log(tf.linalg.diag_part(L)))\n",
    "        log_prob -= np.float64(X.shape[0]/2) * tfm.log(2*PI)\n",
    "        return tf.reduce_sum(log_prob)\n",
    "\n",
    "\n",
    "m = gpflow.models.GPR(data=(model.X, model.Y), kernel=k_exp, mean_function=meanfunc_exp)\n",
    "\n",
    "m.likelihood.variance.assign(3)\n",
    "\n",
    "opt = gpflow.optimizers.Scipy()\n",
    "def objective_closure():\n",
    "    ret = - m.log_marginal_likelihood()\n",
    "    return ret\n",
    "\n",
    "\n",
    "start = timer()\n",
    "opt_logs = opt.minimize(objective_closure,\n",
    "                        m.trainable_variables,\n",
    "                        options=dict(maxiter=50, disp=True, eps=0.00000001), method='CG') # CG: 27.0\n",
    "end = timer()\n",
    "print(f'Time taken: {(end - start):.04f}s')\n",
    "\n",
    "print_summary(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpflow.config.set_default_jitter(np.float64(1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt = gpflow.optimizers.Scipy()\n",
    "opt_logs = opt.minimize(model.internal_model.training_loss, \n",
    "                        model.internal_model.trainable_variables, \n",
    "                        options=dict(maxiter=40, eps=1e-10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=0.03)\n",
    "\n",
    "def optimization_step(model):\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        loss = - model.log_marginal_likelihood()\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "for epoch in range(80):\n",
    "    l = optimization_step(model)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch ', epoch, ' loss', l)\n",
    "        print_summary(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Hamiltonian MCMC sampling takes place in an unconstrained space (where constrained parameters have been mapped via a bijector to an unconstrained space). This makes the optimization, as required in the gradient step, much easier.\n",
    "\n",
    "However, we often wish to sample the constrained parameter values, not the unconstrained one. The SamplingHelper helps us convert our unconstrained values to constrained parameter ones.\n",
    "\n",
    "In general, adaptation prevents the chain from reaching a stationary distribution, so obtaining consistent samples requires num_adaptation_steps be set to a value somewhat smaller than the number of burnin steps\n",
    "\n",
    "step_size: Larger step sizes lead to faster progress, but too-large step sizes make rejection exponentially more likely. When possible, it's often helpful to match per-variable step sizes to the standard deviations of the target distribution in each variable\n",
    "\n",
    "num_leapfrog_steps: Integer number of steps to run the leapfrog integrator for. Total progress per HMC step is roughly proportional to step_size * num_leapfrog_steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_param_dists = True\n",
    "\n",
    "if get_param_dists:\n",
    "    #https://mc-stan.org/docs/2_22/stan-users-guide/fit-gp-section.html --> priors for length-scale\n",
    "    model.kernel.lengthscale.prior = tfd.Uniform(f64(3.5), f64(144))\n",
    "    model.likelihood.variance.prior = tfd.Gamma(f64(1.0), f64(1.0))\n",
    "    for D in model.kernel.D:\n",
    "        D.prior = tfd.Gamma(f64(1), f64(1))#tfd.Normal(f64(D.numpy()), f64(0.12))\n",
    "    for S in model.kernel.S:\n",
    "        S.prior = tfd.Gamma(f64(1), f64(1))#tfd.Normal(f64(S.numpy()), f64(0.12))\n",
    "#     for B in model.mean_function.B:\n",
    "#         B.prior = tfd.Gamma(f64(1), f64(1))#tfd.Normal(f64(B.numpy()), f64(0.12))\n",
    "    print_summary(model)\n",
    "\n",
    "    num_samples = 100\n",
    "    num_burnin_steps = 10\n",
    "\n",
    "    hmc_helper = gpflow.optimizers.SamplingHelper(\n",
    "        target_log_prob_fn=model.log_marginal_likelihood, \n",
    "        model_parameters = model.trainable_parameters\n",
    "    )\n",
    "\n",
    "    hmc = mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=hmc_helper.target_log_prob_fn,\n",
    "        num_leapfrog_steps=5,\n",
    "        step_size=0.01\n",
    "    )\n",
    "    adaptive_hmc = mcmc.SimpleStepSizeAdaptation(\n",
    "        hmc, #mcmc.TransformedTransitionKernel(, bijectors),\n",
    "        num_adaptation_steps=int(num_burnin_steps*0.8),\n",
    "        target_accept_prob=f64(0.75),\n",
    "        adaptation_rate=0.1\n",
    "    )\n",
    "#     adaptive_hmc.bootstrap_results(hmc_helper.current_state)\n",
    "    \n",
    "# if False:\n",
    "    @tf.function\n",
    "    def run_chain_fn():\n",
    "#         def trace_fn(something, pkr):\n",
    "#             print(something)\n",
    "#             print_summary(model)\n",
    "#             return pkr.inner_results.is_accepted\n",
    "        return mcmc.sample_chain(\n",
    "            num_results=num_samples,\n",
    "            num_burnin_steps=num_burnin_steps,\n",
    "            current_state=hmc_helper.current_state,\n",
    "            kernel=adaptive_hmc,\n",
    "            trace_fn = lambda _, pkr: pkr.inner_results.is_accepted\n",
    "        )\n",
    "\n",
    "    samples, traces = run_chain_fn()\n",
    "    parameter_samples = hmc_helper.convert_constrained_values(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_to_name = {param: name for name, param in\n",
    "                 gpflow.utilities.parameter_dict(model).items()}\n",
    "\n",
    "D_mcmc = {f'.kernel.D[{i}]': 0 for i in range(num_genes)}\n",
    "S_mcmc = {f'.kernel.S[{i}]':0 for i in range(num_genes)}\n",
    "B_mcmc = {f'.mean_function.B[{i}]':0 for i in range(num_genes)}\n",
    "lengthscale = 0\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "for val, param in zip(parameter_samples, model.parameters):\n",
    "    name = param_to_name[param]\n",
    "    plt.plot(tf.squeeze(val), label=name)\n",
    "\n",
    "    if 'lengthscale' in name:\n",
    "        lengthscale = np.std(val)\n",
    "    elif 'kernel.D' in name:\n",
    "        D_mcmc[name] = np.std(val)\n",
    "    elif 'kernel.S' in name:\n",
    "        S_mcmc[name] = np.std(val)\n",
    "    elif 'function.B' in name:\n",
    "        B_mcmc[name] = np.std(val)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1., 1.))\n",
    "plt.xlabel('hmc iteration')\n",
    "plt.ylabel('parameter_values');\n",
    "\n",
    "print(B_mcmc, list(S_mcmc.values()))\n",
    "# plot_samples(samples, 'unconstrained_variables_values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear response in GRN Inference notebook\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate using https://gpflow.readthedocs.io/en/master/notebooks/advanced/varying_noise.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
