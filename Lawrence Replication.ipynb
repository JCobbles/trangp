{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gpflow\n",
    "from gpflow.utilities import print_summary, positive\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow import math as tm\n",
    "from sklearn import preprocessing\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import mcmc\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from reggae.data_loaders import load_barenco_puma\n",
    "PI = tf.constant(math.pi, dtype='float64')\n",
    "f64 = np.float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.4f}\".format(x)})\n",
    "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "# k_exp.D[2] = 1.2849\n",
    "# k_exp.D[3] = 1.0959\n",
    "# k_exp.lengthscale.assign(1.4110)\n",
    "# k_exp.h(2, 3).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of the paper by Lawrence et al., 2006\n",
    "\n",
    "https://papers.nips.cc/paper/3119-modelling-transcriptional-regulation-using-gaussian-processes.pdf\n",
    "\n",
    "#### Probesets\n",
    "\n",
    "The original paper restricted their interest to 5 known targets of p53: \n",
    "- DDB2 -------------- (probeset 203409_at)\n",
    "- p21 ----------------- (probeset 202284_s_at) (alias p21CIP1, CDKN1A)\n",
    "- SESN1/hPA26 -- (probeset 218346_s_at)\n",
    "- BIK ----------------- (probeset 205780_at)\n",
    "- TNFRSF10b ----- (probeset 209294_x_at, 209295_at, 210405_x_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_observed, f_observed, σ2_m_pre, σ2_f_pre, t = load_barenco_puma()\n",
    "\n",
    "m_df, m_observed = m_observed \n",
    "f_df, f_observed = f_observed\n",
    "# Shape of m_observed = (replicates, genes, times)\n",
    "m_observed = m_observed\n",
    "f_observed = f_observed\n",
    "σ2_m_pre = f64(σ2_m_pre)\n",
    "σ2_f_pre = f64(σ2_f_pre)\n",
    "\n",
    "display(m_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from matlab\n",
    "x = np.array([\n",
    "[0.0969 , 0.0785 ,  0.0451 , 0.0001 , 0.0800],\n",
    "[0.0998 , 0.0815 ,  0.0479 , 0.0014 , 0.0828],\n",
    "[0.1088 , 0.0907 ,  0.0568 , 0.0054 , 0.0917],\n",
    "[0.1245 , 0.1065 ,  0.0720 , 0.0123 , 0.1068],\n",
    "[0.1470 , 0.1290 ,  0.0938 , 0.0220 , 0.1286],\n",
    "[0.1766 , 0.1583 ,  0.1222 , 0.0346 , 0.1571],\n",
    "[0.2133 , 0.1941 ,  0.1571 , 0.0498 , 0.1921],\n",
    "[0.2568 , 0.2362 ,  0.1982 , 0.0676 , 0.2337],\n",
    "[0.3071 , 0.2843 ,  0.2454 , 0.0879 , 0.2814],\n",
    "[0.3638 , 0.3379 ,  0.2981 , 0.1103 , 0.3350],\n",
    "[0.4265 , 0.3967 ,  0.3561 , 0.1348 , 0.3940],\n",
    "[0.4950 , 0.4603 ,  0.4191 , 0.1611 , 0.4583],\n",
    "[0.5691 , 0.5283 ,  0.4866 , 0.1890 , 0.5274],\n",
    "[0.6486 , 0.6006 ,  0.5586 , 0.2186 , 0.6013],\n",
    "[0.7335 , 0.6771 ,  0.6350 , 0.2497 , 0.6799],\n",
    "[0.8239 , 0.7578 ,  0.7159 , 0.2825 , 0.7633],\n",
    "[0.9199 , 0.8430 ,  0.8013 , 0.3169 , 0.8517],\n",
    "[1.0218 , 0.9328 ,  0.8917 , 0.3530 , 0.9452],\n",
    "[1.1300 , 1.0276 ,  0.9871 , 0.3910 , 1.0442],\n",
    "[1.2445 , 1.1273 ,  1.0877 , 0.4310 , 1.1487],\n",
    "[1.3655 , 1.2322 ,  1.1936 , 0.4728 , 1.2589],\n",
    "[1.4928 , 1.3419 ,  1.3046 , 0.5165 , 1.3746],\n",
    "[1.6260 , 1.4559 ,  1.4203 , 0.5618 , 1.4952],\n",
    "[1.7642 , 1.5735 ,  1.5397 , 0.6083 , 1.6201],\n",
    "[1.9064 , 1.6934 ,  1.6618 , 0.6555 , 1.7481],\n",
    "[2.0508 , 1.8140 ,  1.7850 , 0.7028 , 1.8776],\n",
    "[2.1956 , 1.9333 ,  1.9074 , 0.7492 , 2.0067],\n",
    "[2.3385 , 2.0493 ,  2.0268 , 0.7939 , 2.1333],\n",
    "[2.4770 , 2.1594 ,  2.1411 , 0.8358 , 2.2550],\n",
    "[2.6086 , 2.2613 ,  2.2477 , 0.8740 , 2.3695],\n",
    "[2.7308 , 2.3527 ,  2.3444 , 0.9075 , 2.4743],\n",
    "[2.8412 , 2.4315 ,  2.4290 , 0.9355 , 2.5672],\n",
    "[2.9378 , 2.4958 ,  2.4998 , 0.9572 , 2.6465],\n",
    "[3.0189 , 2.5445 ,  2.5554 , 0.9722 , 2.7107],\n",
    "[3.0836 , 2.5767 ,  2.5951 , 0.9802 , 2.7589],\n",
    "[3.1312 , 2.5925 ,  2.6186 , 0.9814 , 2.7908],\n",
    "[3.1620 , 2.5922 ,  2.6263 , 0.9759 , 2.8066],\n",
    "[3.1766 , 2.5770 ,  2.6191 , 0.9644 , 2.8072],\n",
    "[3.1762 , 2.5483 ,  2.5983 , 0.9475 , 2.7939],\n",
    "[3.1625 , 2.5082 ,  2.5660 , 0.9262 , 2.7684],\n",
    "[3.1377 , 2.4589 ,  2.5241 , 0.9014 , 2.7328],\n",
    "[3.1039 , 2.4029 ,  2.4750 , 0.8744 , 2.6895],\n",
    "[3.0635 , 2.3427 ,  2.4211 , 0.8460 , 2.6407],\n",
    "[3.0190 , 2.2807 ,  2.3648 , 0.8175 , 2.5888],\n",
    "[2.9727 , 2.2193 ,  2.3084 , 0.7898 , 2.5362],\n",
    "[2.9268 , 2.1606 ,  2.2539 , 0.7637 , 2.4849],\n",
    "[2.8833 , 2.1064 ,  2.2031 , 0.7400 , 2.4367],\n",
    "[2.8438 , 2.0582 ,  2.1577 , 0.7193 , 2.3933],\n",
    "[2.8099 , 2.0174 ,  2.1188 , 0.7021 , 2.3560],\n",
    "[2.7827 , 1.9848 ,  2.0876 , 0.6888 , 2.3259],\n",
    "[2.7632 , 1.9612 ,  2.0646 , 0.6795 , 2.3038],\n",
    "[2.7519 , 1.9468 ,  2.0503 , 0.6743 , 2.2900],\n",
    "[2.7491 , 1.9417 ,  2.0449 , 0.6732 , 2.2850],\n",
    "[2.7549 , 1.9456 ,  2.0481 , 0.6760 , 2.2884],\n",
    "[2.7689 , 1.9580 ,  2.0595 , 0.6824 , 2.3000],\n",
    "[2.7904 , 1.9779 ,  2.0783 , 0.6919 , 2.3189],\n",
    "[2.8185 , 2.0042 ,  2.1033 , 0.7040 , 2.3441],\n",
    "[2.8517 , 2.0352 ,  2.1331 , 0.7180 , 2.3742],\n",
    "[2.8884 , 2.0692 ,  2.1659 , 0.7330 , 2.4075],\n",
    "[2.9265 , 2.1039 ,  2.1997 , 0.7481 , 2.4420],\n",
    "[2.9637 , 2.1371 ,  2.2323 , 0.7623 , 2.4754],\n",
    "[2.9977 , 2.1664 ,  2.2613 , 0.7747 , 2.5055],\n",
    "[3.0258 , 2.1893 ,  2.2843 , 0.7840 , 2.5297],\n",
    "[3.0457 , 2.2033 ,  2.2991 , 0.7894 , 2.5458],\n",
    "[3.0551 , 2.2065 ,  2.3034 , 0.7900 , 2.5516],\n",
    "[3.0519 , 2.1971 ,  2.2955 , 0.7851 , 2.5452],\n",
    "[3.0345 , 2.1736 ,  2.2740 , 0.7742 , 2.5252],\n",
    "[3.0020 , 2.1354 ,  2.2382 , 0.7570 , 2.4907],\n",
    "[2.9537 , 2.0823 ,  2.1878 , 0.7336 , 2.4414],\n",
    "[2.8900 , 2.0148 ,  2.1231 , 0.7041 , 2.3776],\n",
    "[2.8117 , 1.9340 ,  2.0451 , 0.6693 , 2.3002],\n",
    "[2.7200 , 1.8417 ,  1.9555 , 0.6299 , 2.2106],\n",
    "[2.6171 , 1.7399 ,  1.8562 , 0.5868 , 2.1109],\n",
    "[2.5053 , 1.6314 ,  1.7499 , 0.5414 , 2.0035],\n",
    "[2.3874 , 1.5191 ,  1.6391 , 0.4947 , 1.8912],\n",
    "[2.2665 , 1.4060 ,  1.5270 , 0.4482 , 1.7769],\n",
    "[2.1455 , 1.2950 ,  1.4164 , 0.4031 , 1.6634],\n",
    "[2.0274 , 1.1893 ,  1.3101 , 0.3607 , 1.5537],\n",
    "[1.9150 , 1.0912 ,  1.2107 , 0.3220 , 1.4504],\n",
    "[1.8107 , 1.0030 ,  1.1205 , 0.2879 , 1.3558],\n",
    "[1.7166 , 0.9265 ,  1.0411 , 0.2591 , 1.2716],\n",
    "[1.6342 , 0.8629 ,  0.9740 , 0.2360 , 1.1994],\n",
    "[1.5645 , 0.8129 ,  0.9198 , 0.2189 , 1.1398],\n",
    "[1.5079 , 0.7766 ,  0.8788 , 0.2077 , 1.0933],\n",
    "[1.4645 , 0.7535 ,  0.8506 , 0.2021 , 1.0596],\n",
    "[1.4337 , 0.7429 ,  0.8346 , 0.2017 , 1.0382],\n",
    "[1.4145 , 0.7434 ,  0.8296 , 0.2058 , 1.0278],\n",
    "[1.4056 , 0.7534 ,  0.8341 , 0.2138 , 1.0272],\n",
    "[1.4054 , 0.7711 ,  0.8464 , 0.2247 , 1.0347],\n",
    "[1.4121 , 0.7945 ,  0.8646 , 0.2378 , 1.0485],\n",
    "[1.4238 , 0.8217 ,  0.8869 , 0.2520 , 1.0666],\n",
    "[1.4387 , 0.8505 ,  0.9113 , 0.2667 , 1.0872],\n",
    "[1.4548 , 0.8793 ,  0.9360 , 0.2809 , 1.1086],\n",
    "[1.4705 , 0.9063 ,  0.9594 , 0.2940 , 1.1291],\n",
    "[1.4844 , 0.9300 ,  0.9802 , 0.3054 , 1.1473],\n",
    "[1.4951 , 0.9495 ,  0.9971 , 0.3146 , 1.1619],\n",
    "[1.5016 , 0.9637 ,  1.0093 , 0.3214 , 1.1721],\n",
    "[1.5032 , 0.9721 ,  1.0162 , 0.3254 , 1.1772],\n",
    "[1.4994 , 0.9744 ,  1.0173 , 0.3267 , 1.1768],\n",
    "[1.4900 , 0.9706 ,  1.0126 , 0.3252 , 1.1708]])\n",
    "\n",
    "f = [0.0000, 0.0230,  0.0492,  0.0782,  0.1094,  0.1424,  0.1765,  0.2114,  0.2466,  0.2819,  0.3172,  0.3527,  0.3886,  0.4253,  0.4632,  0.5027,  0.5443,  0.5882,  0.6342,  0.6821,  0.7312,  0.7805,  0.8287,  0.8741,  0.9151,  0.9498,  0.9765,  0.9938,  1.0006,  0.9961,  0.9804,  0.9539,  0.9176,  0.8730,  0.8220,  0.7668,  0.7097,  0.6530,  0.5990,  0.5494,  0.5058,  0.4694,  0.4408,  0.4206,  0.4086,  0.4046,  0.4082,  0.4187,  0.4353,  0.4573,  0.4837,  0.5136,  0.5458,  0.5791,  0.6122,  0.6436,  0.6718,  0.6951,  0.7120,  0.7210,  0.7208,  0.7104,  0.6893,  0.6573,  0.6150,  0.5632,  0.5035,  0.4377,  0.3682,  0.2974,  0.2278,  0.1621,  0.1026,  0.0512,  0.0097, -0.0209, -0.0400, -0.0475, -0.0438, -0.0300, -0.0072,  0.0228,  0.0582,  0.0971,  0.1376,  0.1777,  0.2157,  0.2502,  0.2799,  0.3040,  0.3219,  0.3332,  0.3381,  0.3368,  0.3299,  0.3179,  0.3018,  0.2824,  0.2605,  0.2371]\n",
    "\n",
    "\n",
    "b=[0.0020,0.0830,0.1508,0.2024,0.2373,0.2559,0.2591,0.2488,0.2283,0.2028,0.1804,0.1719,0.1851,0.2179,0.2612,0.3069,0.3493,0.3846,0.4102,0.4247,0.4277,0.4196,0.4021,0.3778,0.3510,0.3271,0.3123,0.3119,0.3270,0.3545,0.3887,0.4237,0.4548,0.4784,0.4920,0.4944,0.4852,0.4650,0.4355,0.3998,0.3622,0.3290,0.3077,0.3047,0.3214,0.3534,0.3935,0.4346,0.4716,0.5006,0.5191,0.5256,0.5197,0.5016,0.4729,0.4358,0.3943,0.3535,0.3207,0.3037,0.3075,0.3307,0.3668,0.4079,0.4472,0.4801,0.5033,0.5144,0.5125,0.4971,0.4689,0.4294,0.3811,0.3286,0.2787,0.2426,0.2334,0.2564,0.3031,0.3606,0.4194,0.4732,0.5179,0.5506,0.5693,0.5728,0.5606,0.5327,0.4899,0.4340,0.3683,0.2993,0.2407,0.2194,0.2578,0.3429,0.4521,0.5724,0.6971,0.8222]\n",
    "fig = plt.figure(figsize=(14, 17))\n",
    "for j in range(5):\n",
    "    ax = plt.subplot(531+j)\n",
    "    plt.plot(x[:,j], color='grey')\n",
    "    plt.title(genes[0].index[j])\n",
    "    plt.scatter(np.arange(0, 100, 100/7), Y[7*j:(j+1)*7], marker='x', label='Observed')\n",
    "    plt.xticks([100/6 * i for i in range(7)])\n",
    "    ax.axes.set_xticklabels(t)\n",
    "    plt.xlabel('Time (h)')\n",
    "plt.tight_layout()\n",
    "    \n",
    "\n",
    "\n",
    "# if plot_barenco:\n",
    "#     barenco_f, _ = scaled_barenco_data(np.mean(f_samples[-10:], axis=0))\n",
    "#     plt.scatter(τ[common_indices], barenco_f, marker='x', s=60, linewidth=3, label='Barenco et al.')\n",
    "f = np.array(f)\n",
    "b = np.array(b)\n",
    "# plt.fill_between(τ, bounds[:, 0], bounds[:, 1], color='grey', alpha=0.3, label='95% credibility interval')\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "plt.xticks([100/6 * i for i in range(7)])\n",
    "fig.axes[0].set_xticklabels(t)\n",
    "plt.ylim((-0.4,1.4))\n",
    "plt.xlabel('Time (h)')\n",
    "plt.plot(f, c='cadetblue')\n",
    "plt.fill_between(np.arange(0, 100), f+b, f-b, color='grey', alpha=0.3, label='2 x st. dev')\n",
    "\n",
    "# print(genes[1])\n",
    "print(np.arange(len(f))[::15].shape, tfs[1][0].shape)\n",
    "tf = tfs[1][0]/np.mean(tfs[1][0]) * np.mean(f)\n",
    "plt.scatter([100/6 * i for i in range(7)], tf, marker='x', s=70, label='Observed')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay = np.array([0.331, 0.416, 0.392, 0.8, 0.441])[[0, 4, 2, 3, 1]]\n",
    "sensi = np.array([0.825, 0.4846, 0.338, 1, 0.3292])[[0, 4, 2, 3, 1]]\n",
    "basal = np.array([0.15, 0.05, 0.005, 0.005, 0.004])[[0, 4, 2, 3, 1]]\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(np.arange(5)-0.3, basal, width=0.3, label='Basal rate')\n",
    "plt.bar(np.arange(5), sensi, width=0.3, tick_label=genes[0].index, label='Sensitivity')\n",
    "plt.bar(np.arange(5)+0.3, decay, width=0.3, color='blue', align='center', label='Decay rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probeset Combination\n",
    "\n",
    "TNFRSF10b has multiple probesets (probeset 209294_x_at, 209295_at, 210405_x_at) which should be combined.\n",
    "\n",
    "It can be observed below that the log intensities have a similar pattern. Thus a popular way to combine is to take the average of the log intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINE_MULTIPLE_PROBESETS = False\n",
    "if COMBINE_MULTIPLE_PROBESETS:\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.subplot(2,2,1)\n",
    "    p53 = df[df.index.isin(['211300_s_at', '201746_at'])][columns].astype(float)\n",
    "    for index, row in p53.iterrows():\n",
    "        p53.loc[index] = np.log(list(row))\n",
    "        plt.plot(list(row))\n",
    "\n",
    "    p53_mean = pd.Series(p53.mean(0), index=genes.columns, name='p53')\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    TNFRSF10b = df[df.index.isin(['209294_x_at', '209295_at', '210405_x_at'])][columns]\n",
    "    for index, row in TNFRSF10b.iterrows():\n",
    "        print(list(row))\n",
    "        TNFRSF10b.loc[index] = np.log(list(row))\n",
    "        plt.plot(list(row))\n",
    "\n",
    "    TNFRSF10b_mean = pd.Series(TNFRSF10b.mean(0), index=genes.columns, name='TNFRSF10b')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(p53_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genes = 5 #X.shape[1]\n",
    "num_times = 7\n",
    "replica = 0\n",
    "print(m_observed.shape)\n",
    "Y = m_observed[replica]\n",
    "Y_var = σ2_m_pre[replica]\n",
    "Y_orig_shape = Y.shape\n",
    "Y = np.array([0,*Y.reshape(-1)]).reshape(-1, 1)\n",
    "Y_var = Y_var.reshape(-1)\n",
    "Sigma =tf.linalg.tensor_diag(tf.concat([[np.float64(0)], (Y_var.reshape(-1))], 0))\n",
    "print(Sigma.dtype)\n",
    "X = np.arange(num_times, dtype='float64')*2\n",
    "X = np.c_[[X for _ in range(num_genes)]].reshape(-1, 1)\n",
    "print(X[:8])\n",
    "print(Y_var[:8])\n",
    "Y = m_observed[0].reshape((-1, 1))\n",
    "\n",
    "print(Y[:8])\n",
    "print(X.shape,Y.shape, Y_var.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We fix the sensitivity of p21 to be 1, and decay to be 0.8 as in Barenco et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            K_xx = np.zeros([X.shape[0],X.shape[0]], dtype='float64')\n",
    "            print(K_xx.shape)\n",
    "            for j in range(num_genes):\n",
    "                for k in range(num_genes):\n",
    "                    K_xx[j*block_size:(j+1)*block_size, \n",
    "                         k*block_size:(k+1)*block_size] = self.k_xx(j, k, X)\n",
    "            return K_xx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def broadcast_tile(a, h, w):\n",
    "    x, y = a.shape\n",
    "    m, n = x * h, y * w\n",
    "    return tf.reshape(tf.broadcast_to(\n",
    "        tf.reshape(a, (x, m//(h*x), y, n//(w*y))), (m//h, h, n//w, w)\n",
    "    ), (m, n))\n",
    "\n",
    "class ExpressionKernel(gpflow.kernels.Kernel):\n",
    "    def __init__(self):\n",
    "        super().__init__(active_dims=[0])\n",
    "        \n",
    "#         l_affine = tfb.AffineScalar(shift=tf.cast(1., tf.float64),\n",
    "#                             scale=tf.cast(4-1., tf.float64))\n",
    "#         l_sigmoid = tfb.Sigmoid()\n",
    "#         l_logistic = tfb.Chain([l_affine, l_sigmoid])\n",
    "\n",
    "        self.lengthscale = gpflow.Parameter(1.414, transform=positive())\n",
    "#         self.white_variance = gpflow.Parameter(0.1, transform=positive())\n",
    "\n",
    "        D_affine = tfb.AffineScalar(shift=tf.cast(0.1, tf.float64),\n",
    "                                    scale=tf.cast(1.5-0.1, tf.float64))\n",
    "        D_sigmoid = tfb.Sigmoid()\n",
    "        D_logistic = tfb.Chain([D_affine, D_sigmoid])\n",
    "        S_affine = tfb.AffineScalar(shift=tf.cast(0.1, tf.float64),\n",
    "                                    scale=tf.cast(4.-0.1, tf.float64))\n",
    "        S_sigmoid = tfb.Sigmoid()\n",
    "        S_logistic = tfb.Chain([S_affine, S_sigmoid])\n",
    "\n",
    "        self.D = gpflow.Parameter(np.random.uniform(0.9, 1, num_genes), transform=positive(), dtype=tf.float64)\n",
    "#         self.D[3].trainable = False\n",
    "#         self.D[3].assign(0.8)\n",
    "        self.kervar = gpflow.Parameter(np.float64(1), transform=positive())\n",
    "        self.S = gpflow.Parameter(np.random.uniform(1,1, num_genes), transform=positive(), dtype=tf.float64)\n",
    "#         self.S[3].trainable = False\n",
    "#         self.S[3].assign(1)\n",
    "        self.noise_term = gpflow.Parameter(0.1353*tf.ones(num_genes, dtype='float64'), transform=positive())\n",
    "        \n",
    "    def K(self, X, X2=None):\n",
    "        self.block_size = int(X.shape[0]/num_genes)\n",
    "#         tf.print(self.D, self.S)\n",
    "#         interp = interp1d(np.arange(Y_var.shape[0]), Y_var.reshape(-1), kind='linear')\n",
    "#         Sigma = interp(np.linspace(0,Y_var.shape[0]-1, X.shape[0]))\n",
    "#         Sigma = tf.linalg.tensor_diag(Sigma)\n",
    "#         White = tf.linalg.diag(\n",
    "#                     tf.fill(tf.shape(X)[:-1], tf.squeeze(self.white_variance)))\n",
    "\n",
    "#         if X2 is None:\n",
    "        shape = [X.shape[0],X.shape[0]]\n",
    "        K_xx = self.k_xx(X, 0,0)        \n",
    "        white = tf.linalg.diag(broadcast_tile(tf.reshape(self.noise_term, (1, -1)), 1, 7)[0])\n",
    "        return K_xx + tf.linalg.diag((1e-5*tf.ones(X.shape[0], dtype='float64'))+Y_var) + white\n",
    "#         else:\n",
    "#             '''Calculate K_xf: no need to use tf.* since this part is not optimised'''\n",
    "#             shape = [X.shape[0],X2.shape[0]]#self.block_size]\n",
    "\n",
    "#             K_xf = tf.zeros(shape, dtype='float64')\n",
    "#             for j in range(num_genes):\n",
    "#                 mask = np.ones(shape)\n",
    "#                 other = np.zeros(shape)\n",
    "#                 mask[j*self.block_size:(j+1)*self.block_size] = 0\n",
    "#                 pad_top = j*self.block_size\n",
    "#                 pad_bottom = 0 if j == num_genes-1 else shape[0]-self.block_size-pad_top\n",
    "#                 kxf = self.k_xf(j, X, X2)\n",
    "#                 other = tf.pad(kxf,\n",
    "#                                tf.constant([[pad_top,pad_bottom],[0,0]]), 'CONSTANT'\n",
    "#                               )\n",
    "\n",
    "#                 K_xf = K_xf * mask + other * (1 - mask)\n",
    "#                 #[j*self.block_size:(j+1)*self.block_size] = \n",
    "#             return K_xf\n",
    "        \n",
    "    def k_xf(self, j, X, X2):\n",
    "        t_prime, t_, t_dist = self.get_distance_matrix(t_x=X[:self.block_size].reshape(-1), \n",
    "                                                       t_y=X2)\n",
    "        l = self.lengthscale\n",
    "        erf_term = tm.erf(t_dist/l - self.gamma(j)) + tm.erf(t_/l + self.gamma(j))\n",
    "\n",
    "        return self.S[j]*l*0.5*tm.sqrt(PI)*tm.exp(self.gamma(j)**2) *tm.exp(-self.D[j]*t_dist)*erf_term \n",
    "\n",
    "    def gamma(self):\n",
    "        return self.D*self.lengthscale/2\n",
    "\n",
    "    def h_(self, X, k, j, primefirst=True):\n",
    "        l = self.lengthscale\n",
    "#         print(l, self.D[k], self.D[j])\n",
    "        t_x = X[:self.block_size].reshape(-1)\n",
    "        t_prime, t, t_dist = self.get_distance_matrix(primefirst=primefirst, t_x=t_x)\n",
    "        multiplier = tm.exp(self.gamma(k)**2) / (self.D[j]+self.D[k])\n",
    "        first_erf_term = tm.erf(t_dist/l - self.gamma(k)) + tm.erf(t/l + self.gamma(k))\n",
    "        second_erf_term = tm.erf(t_prime/l - self.gamma(k)) + tm.erf(self.gamma(k))\n",
    "        return multiplier * (tf.multiply(tm.exp(-self.D[k]*t_dist) , first_erf_term) - \\\n",
    "                             tf.multiply(tm.exp(-self.D[k]*t_prime-self.D[j]*t) , second_erf_term))\n",
    "    def h(self, X, k, j, primefirst=True):\n",
    "        Dj = tf.reshape(self.D, (1, -1))\n",
    "        Dj = broadcast_tile(Dj, 1, 7)\n",
    "        Dj = tf.tile(Dj, [35, 1])\n",
    "        Dk = tf.reshape(self.D, (-1, 1)) \n",
    "        Dk = broadcast_tile(Dk, 7, 1)\n",
    "        Dk = tf.tile(Dk, [1, 35])\n",
    "        gk = tf.transpose(broadcast_tile(tf.reshape(self.gamma(), (-1, 1)), 7, 1))\n",
    "        gk = tf.tile(gk, [35, 1])\n",
    "        if not primefirst:\n",
    "            Dk, Dj = Dj, Dk\n",
    "            gk = tf.transpose(broadcast_tile(tf.reshape(self.gamma(), (1,-1)), 1, 7))\n",
    "            gk = tf.tile(gk, [1, 35])\n",
    "\n",
    "        l = self.lengthscale\n",
    "        t_x = tf.reshape(X[:self.block_size], (-1,))\n",
    "        t_prime, t, t_dist = self.get_distance_matrix(primefirst=primefirst, t_x=t_x)\n",
    "        t_prime = tf.tile(t_prime, [5, 5])\n",
    "        t = tf.tile(t, [5, 5])\n",
    "        t_dist = tf.tile(t_dist, [5, 5])\n",
    "        multiplier = tm.exp(gk**2) / (Dj + Dk)\n",
    "        first_erf_term = tm.erf(t_dist/l - gk) + tm.erf(t/l + gk)\n",
    "        second_erf_term = tm.erf(t_prime/l - gk) + tm.erf(gk)\n",
    "        return multiplier * (tf.multiply(tm.exp(-Dk*t_dist) , first_erf_term) - \\\n",
    "                             tf.multiply(tm.exp(-Dk*t_prime-Dj*t) , second_erf_term))\n",
    "\n",
    "\n",
    "    def k_xx(self, X, j, k):\n",
    "        S_square = tf.matmul(tf.reshape(self.S, (-1, 1)), tf.reshape(self.S, (1, -1)))\n",
    "        S_square = broadcast_tile(S_square, 7, 7)\n",
    "        mult = S_square*self.lengthscale*0.5*tm.sqrt(PI)\n",
    "        return self.kervar**2*mult*(self.h(X, k, j) + self.h(X, j, k, primefirst=False))\n",
    "\n",
    "    def get_distance_matrix(self, t_x, primefirst=True, t_y=None):\n",
    "        if t_y is None:\n",
    "            t_y = t_x\n",
    "        t_1 = tf.transpose(tf.reshape(tf.tile(t_x, [t_y.shape[0]]), [ t_y.shape[0], t_x.shape[0]]))\n",
    "        t_2 = tf.reshape(tf.tile(t_y, [t_x.shape[0]]), [ t_x.shape[0], t_y.shape[0]])\n",
    "        if primefirst:\n",
    "            return t_1, t_2, t_1-t_2\n",
    "        return t_2, t_1, t_2-t_1\n",
    "    \n",
    "    def K_diag(self, X):\n",
    "        print('k_diag')\n",
    "\n",
    "        \"\"\"I've used the fact that we call this method for K_ff when finding the covariance as a hack so\n",
    "        I know if I should return K_ff or K_xx. In this case we're returning K_ff!!\n",
    "        $K_{ff}^{post} = K_{ff} - K_{fx} K_{xx}^{-1} K_{xf}$\"\"\"\n",
    "        _,_,t_dist = self.get_distance_matrix(t_x=X.reshape(-1))\n",
    "        K_ff = tf.math.exp(-(t_dist**2)/(2*self.lengthscale**2))\n",
    "        return (K_ff)\n",
    "\n",
    "\n",
    "k_exp = ExpressionKernel()\n",
    "print_summary(k_exp, fmt='notebook')\n",
    "print(k_exp.K(X)[:7,:7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanFunction(gpflow.mean_functions.MeanFunction):\n",
    "    def __init__(self):\n",
    "        affine = tfb.AffineScalar(shift=tf.cast(0., tf.float64),\n",
    "                                  scale=tf.cast(2.-0., tf.float64))\n",
    "        sigmoid = tfb.Sigmoid()\n",
    "        logistic = tfb.Chain([affine, sigmoid])\n",
    "\n",
    "        self.B = [gpflow.Parameter(np.mean(row)*k_exp.D[i], transform=positive()) for i, row in enumerate(Y.reshape(Y_orig_shape))]\n",
    "\n",
    "    def __call__(self, X):\n",
    "        ret = tf.zeros(0, dtype='double')\n",
    "        block_size = int(X.shape[0]/num_genes)\n",
    "        for j in range(num_genes):\n",
    "            ret = tf.concat([ret, tf.repeat(self.B[j]/k_exp.D[j], block_size)], axis=0)\n",
    "#         ret = tf.divide(self.B, (k_exp.D)).repeat(7)\n",
    "\n",
    "        return ret[:,None]\n",
    "\n",
    "meanfunc_exp = MeanFunction()\n",
    "# meanfunc_exp(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpflow.logdensities import multivariate_normal\n",
    "class GPR(gpflow.models.GPR):\n",
    "    def log_marginal_likelihood(self) -> tf.Tensor:\n",
    "        r\"\"\"\n",
    "        Computes the log marginal likelihood.\n",
    "\n",
    "        .. math::\n",
    "            \\log p(Y | \\theta).\n",
    "\n",
    "        \"\"\"\n",
    "        X, Y = self.data\n",
    "        K = self.kernel(X)\n",
    "        m = self.mean_function(X)\n",
    "        var = tf.linalg.diag(self.likelihood.variance*tf.ones(35, dtype='float64'))\n",
    "        invK = tf.linalg.inv(K+var)\n",
    "        L = tf.linalg.cholesky(K+var)\n",
    "        log_prob = -0.5 * tf.linalg.matmul(tf.transpose(Y-m), tf.linalg.lstsq(tf.transpose(L), tf.linalg.lstsq(L, Y-m)))#tf.matmul(tf.matmul(tf.transpose(Y-m), invK), (Y-m))\n",
    "        tf.print(log_prob)\n",
    "        log_prob -= 0.5 * tf.reduce_sum(tm.log(tf.linalg.diag_part(L)))\n",
    "        log_prob -= np.float64(35/2) * tm.log(2*PI)\n",
    "#         num_data = Y.shape[0]\n",
    "#         k_diag = tf.linalg.diag_part(K)\n",
    "#         s_diag = tf.fill([num_data], self.likelihood.variance)\n",
    "#         ks = tf.linalg.set_diag(K, k_diag + s_diag)\n",
    "#         L = tf.linalg.cholesky(ks)\n",
    "#         m = self.mean_function(X)\n",
    "#         print(m)\n",
    "        # [R,] log-likelihoods for each independent dimension of Y\n",
    "        \n",
    "#         log_prob = multivariate_normal(Y, m, L)\n",
    "        return tf.reduce_sum(log_prob)\n",
    "\n",
    "    def nll_stable(theta):\n",
    "        # Numerically more stable implementation of Eq. (7) as described\n",
    "        # in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf, Section\n",
    "        # 2.2, Algorithm 2.1.\n",
    "        K = kernel(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\\n",
    "            noise**2 * np.eye(len(X_train))\n",
    "        L = cholesky(K)\n",
    "        return np.sum(np.log(np.diagonal(L))) + \\\n",
    "               0.5 * Y_train.T.dot(lstsq(L.T, lstsq(L, Y_train)[0])[0]) + \\\n",
    "               0.5 * len(X_train) * np.log(2*np.pi)\n",
    "\n",
    "model = GPR(data=(X, Y), kernel=k_exp, mean_function=meanfunc_exp)\n",
    "\n",
    "model.likelihood.variance.assign(3)\n",
    "model.log_marginal_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt = gpflow.optimizers.Scipy()\n",
    "def objective_closure():\n",
    "#     global prev_ret\n",
    "#     try:\n",
    "    ret = - model.log_marginal_likelihood()\n",
    "#     except:\n",
    "\n",
    "\n",
    "#     Ds.append([s.numpy() for s in model.kernel.D])\n",
    "\n",
    "    tf.print(ret)\n",
    "    return ret\n",
    "opt_logs = opt.minimize(objective_closure,\n",
    "                        model.trainable_variables,\n",
    "                        options=dict(maxiter=14, disp=True, eps=0.00000001),\n",
    "                        method='CG')\n",
    "\n",
    "print_summary(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = gpflow.models.GPR(data=(X, Y), kernel=k_exp, mean_function=meanfunc_exp)\n",
    "\n",
    "gpflow.config.set_default_jitter(np.float64(1e-3))\n",
    "\n",
    "opt = gpflow.optimizers.Scipy()\n",
    "print_summary(model)\n",
    "Ds = list()\n",
    "\n",
    "prev_ret = 0\n",
    "def objective_closure():\n",
    "#     global prev_ret\n",
    "#     try:\n",
    "    ret = - model.log_marginal_likelihood()\n",
    "#     except:\n",
    "\n",
    "\n",
    "#     Ds.append([s.numpy() for s in model.kernel.D])\n",
    "\n",
    "    tf.print(ret)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt_logs = opt.minimize(model.training_loss, model.trainable_variables, options=dict(maxiter=4, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        X, Y = self.data\n",
    "        K = self.kernel(X)\n",
    "        num_data = Y.shape[0]\n",
    "        k_diag = tf.linalg.diag_part(K)\n",
    "        s_diag = tf.fill([num_data], self.likelihood.variance)\n",
    "        ks = tf.linalg.set_diag(K, k_diag + s_diag)\n",
    "        L = tf.linalg.cholesky(ks)\n",
    "        \n",
    "        Linv = tf.linalg.triangular_solve(L, tf.eye((num_data), dtype='float64'), lower=True)\n",
    "        Kinv = tf.transpose(Linv)*Linv \n",
    "\n",
    "#         m = self.mean_function(X)\n",
    "# #         print(m)\n",
    "#         # [R,] log-likelihoods for each independent dimension of Y\n",
    "        \n",
    "#         log_prob = multivariate_normal(Y, m, L)\n",
    "        \n",
    "        n = Y.shape[0];\n",
    "        log_prob = -n*tf.math.log(2*PI) - tm.log(tf.reduce_prod(tf.linalg.diag_part(L))**2) - tf.transpose(Y)*Kinv*Y\n",
    "        log_prob = log_prob*0.5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=0.03)\n",
    "\n",
    "def optimization_step(model):\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        loss = - model.log_marginal_likelihood()\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "for epoch in range(80):\n",
    "    l = optimization_step(model)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch ', epoch, ' loss', l)\n",
    "        print_summary(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_summary(model)\n",
    "print(model.kernel(X)[:7, :7])\n",
    "# tf.linalg.cholesky(model.kernel(X)+100*tf.eye(35, dtype='float64'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Hamiltonian MCMC sampling takes place in an unconstrained space (where constrained parameters have been mapped via a bijector to an unconstrained space). This makes the optimization, as required in the gradient step, much easier.\n",
    "\n",
    "However, we often wish to sample the constrained parameter values, not the unconstrained one. The SamplingHelper helps us convert our unconstrained values to constrained parameter ones.\n",
    "\n",
    "In general, adaptation prevents the chain from reaching a stationary distribution, so obtaining consistent samples requires num_adaptation_steps be set to a value somewhat smaller than the number of burnin steps\n",
    "\n",
    "step_size: Larger step sizes lead to faster progress, but too-large step sizes make rejection exponentially more likely. When possible, it's often helpful to match per-variable step sizes to the standard deviations of the target distribution in each variable\n",
    "\n",
    "num_leapfrog_steps: Integer number of steps to run the leapfrog integrator for. Total progress per HMC step is roughly proportional to step_size * num_leapfrog_steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_param_dists = True\n",
    "\n",
    "if get_param_dists:\n",
    "    #https://mc-stan.org/docs/2_22/stan-users-guide/fit-gp-section.html --> priors for length-scale\n",
    "    model.kernel.lengthscale.prior = tfd.Uniform(f64(3.5), f64(144))\n",
    "    model.likelihood.variance.prior = tfd.Gamma(f64(1.0), f64(1.0))\n",
    "    for D in model.kernel.D:\n",
    "        D.prior = tfd.Gamma(f64(1), f64(1))#tfd.Normal(f64(D.numpy()), f64(0.12))\n",
    "    for S in model.kernel.S:\n",
    "        S.prior = tfd.Gamma(f64(1), f64(1))#tfd.Normal(f64(S.numpy()), f64(0.12))\n",
    "#     for B in model.mean_function.B:\n",
    "#         B.prior = tfd.Gamma(f64(1), f64(1))#tfd.Normal(f64(B.numpy()), f64(0.12))\n",
    "    print_summary(model)\n",
    "\n",
    "    num_samples = 100\n",
    "    num_burnin_steps = 10\n",
    "\n",
    "    hmc_helper = gpflow.optimizers.SamplingHelper(\n",
    "        target_log_prob_fn=model.log_marginal_likelihood, \n",
    "        model_parameters = model.trainable_parameters\n",
    "    )\n",
    "\n",
    "    hmc = mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=hmc_helper.target_log_prob_fn,\n",
    "        num_leapfrog_steps=5,\n",
    "        step_size=0.01\n",
    "    )\n",
    "    adaptive_hmc = mcmc.SimpleStepSizeAdaptation(\n",
    "        hmc, #mcmc.TransformedTransitionKernel(, bijectors),\n",
    "        num_adaptation_steps=int(num_burnin_steps*0.8),\n",
    "        target_accept_prob=f64(0.75),\n",
    "        adaptation_rate=0.1\n",
    "    )\n",
    "#     adaptive_hmc.bootstrap_results(hmc_helper.current_state)\n",
    "    \n",
    "# if False:\n",
    "    @tf.function\n",
    "    def run_chain_fn():\n",
    "#         def trace_fn(something, pkr):\n",
    "#             print(something)\n",
    "#             print_summary(model)\n",
    "#             return pkr.inner_results.is_accepted\n",
    "        return mcmc.sample_chain(\n",
    "            num_results=num_samples,\n",
    "            num_burnin_steps=num_burnin_steps,\n",
    "            current_state=hmc_helper.current_state,\n",
    "            kernel=adaptive_hmc,\n",
    "            trace_fn = lambda _, pkr: pkr.inner_results.is_accepted\n",
    "        )\n",
    "\n",
    "    samples, traces = run_chain_fn()\n",
    "    parameter_samples = hmc_helper.convert_constrained_values(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_to_name = {param: name for name, param in\n",
    "                 gpflow.utilities.parameter_dict(model).items()}\n",
    "\n",
    "D_mcmc = {f'.kernel.D[{i}]': 0 for i in range(num_genes)}\n",
    "S_mcmc = {f'.kernel.S[{i}]':0 for i in range(num_genes)}\n",
    "B_mcmc = {f'.mean_function.B[{i}]':0 for i in range(num_genes)}\n",
    "lengthscale = 0\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "for val, param in zip(parameter_samples, model.parameters):\n",
    "    name = param_to_name[param]\n",
    "    plt.plot(tf.squeeze(val), label=name)\n",
    "\n",
    "    if 'lengthscale' in name:\n",
    "        lengthscale = np.std(val)\n",
    "    elif 'kernel.D' in name:\n",
    "        D_mcmc[name] = np.std(val)\n",
    "    elif 'kernel.S' in name:\n",
    "        S_mcmc[name] = np.std(val)\n",
    "    elif 'function.B' in name:\n",
    "        B_mcmc[name] = np.std(val)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1., 1.))\n",
    "plt.xlabel('hmc iteration')\n",
    "plt.ylabel('parameter_values');\n",
    "\n",
    "print(B_mcmc, list(S_mcmc.values()))\n",
    "# plot_samples(samples, 'unconstrained_variables_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See page 130 Bayesian Stats for credible intervals (not implemented here yet)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(3, 3, 1)\n",
    "B = np.array([s.numpy() for s in model.mean_function.B])\n",
    "S = np.array([s.numpy() for s in model.kernel.S])\n",
    "D = np.array([d.numpy() for d in model.kernel.D])\n",
    "B_barenco = np.array([2.6, 1.5, 0.5, 0.2, 1.35])\n",
    "B_barenco = (B_barenco/np.mean(B_barenco)*np.mean(B))[[0, 4, 2, 3, 1]]\n",
    "S_barenco = (np.array([3, 0.8, 0.7, 1.8, 0.7])/1.8)[[0, 4, 2, 3, 1]]\n",
    "D_barenco = (np.array([1.2, 1.6, 1.75, 3.2, 2.3])*0.8/3.2)[[0, 4, 2, 3, 1]]\n",
    "\n",
    "\n",
    "data = [B, S, D]\n",
    "barenco_data = [B_barenco,  S_barenco, D_barenco]\n",
    "vars = [0, 0,0]#[ S_mcmc, D_mcmc]\n",
    "labels = ['Basal rates', 'Sensitivities', 'Decay rates']\n",
    "\n",
    "plotnum = 331\n",
    "for A, B, var, label in zip(data, barenco_data, vars, labels):\n",
    "    plt.subplot(plotnum)\n",
    "    plotnum+=1\n",
    "    plt.bar(np.arange(5)-0.2, A, width=0.4, tick_label=genes[0].index)\n",
    "    plt.bar(np.arange(5)+0.2, B, width=0.4, color='blue', align='center')\n",
    "\n",
    "    plt.title(label)\n",
    "#     plt.errorbar(np.arange(5)-0.2, A, yerr=list(var.values()), fmt='none', color='green', capsize=2)\n",
    "# plt.bar(range(5), S, tick_label=genes.index[:num_genes])\n",
    "# plt.title('')\n",
    "# plt.errorbar(range(5), S, list(S_mcmc.values()), color='green')\n",
    "\n",
    "# plt.subplot(3, 3, 3)\n",
    "# plt.bar(range(5), D, tick_label=genes.index[:num_genes])\n",
    "# plt.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granularity = 100\n",
    "pred_t = np.linspace(0, 12, granularity, dtype='float64')\n",
    "print(int(X.shape[0]/num_genes))\n",
    "k = model.kernel\n",
    "Kxx = k.K(X, None)\n",
    "K_inv = tf.linalg.inv(Kxx)\n",
    "Kxf = k.K(X, pred_t)\n",
    "print(Kxf.shape, K_inv.shape)\n",
    "# x = tf.convert_to_tensor(Y) - np.repeat(B/np.array(D), 7).reshape(-1, 1)\n",
    "KfxKxx = tf.matmul(tf.transpose(Kxf), K_inv)\n",
    "mu_post = tf.matmul(KfxKxx, Y) #replace with x?\n",
    "\n",
    "print(tf.transpose(Kxf).shape, '*', K_inv.shape,'==', KfxKxx.shape)\n",
    "print('mu shape', mu_post.shape)\n",
    "\n",
    "mu_post = mu_post.numpy()\n",
    "mu_post -= mu_post[0]\n",
    "plt.plot(mu_post)\n",
    "\n",
    "scale_pred = np.sqrt(np.var(mu_post));\n",
    "barencof = np.array([[0.0, 200.52011, 355.5216125, 205.7574913, 135.0911372, 145.1080997, 130.7046969],\n",
    "                     [0.0, 184.0994134, 308.47592, 232.1775328, 153.6595161, 85.7272235, 168.0910562],\n",
    "                     [0.0, 230.2262511, 337.5994811, 276.941654, 164.5044287, 127.8653452, 173.6112139]])\n",
    "\n",
    "\n",
    "barencof = barencof[0]/(np.sqrt(np.var(barencof[0])))*scale_pred;\n",
    "lb = len(barencof)\n",
    "plt.scatter(np.arange(lb) * granularity/lb, barencof, marker='x')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = X.shape[0]\n",
    "k_diag = tf.linalg.diag_part(Kxx)\n",
    "s_diag = tf.fill([num_data], model.likelihood.variance)\n",
    "ks = tf.linalg.set_diag(Kxx, k_diag + s_diag)\n",
    "L = tf.linalg.cholesky(ks)\n",
    "mu = model.mean_function(X)\n",
    "#         print(m)\n",
    "        # [R,] log-likelihoods for each independent dimension of Y\n",
    "d = Y - mu\n",
    "alpha = tf.linalg.triangular_solve(L, d, lower=True)\n",
    "plt.figure(figsize=(13, 13))\n",
    "\n",
    "for j in range(5):\n",
    "    plt.subplot(531+j)\n",
    "    plt.scatter(np.arange(7), Y[7*j:(j+1)*7])\n",
    "    plt.plot(alpha[7*j:(j+1)*7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pred = np.sqrt(np.var(mu_post));\n",
    "barencof = np.array([[0.0, 200.52011, 355.5216125, 205.7574913, 135.0911372, 145.1080997, 130.7046969],\n",
    "                     [0.0, 184.0994134, 308.47592, 232.1775328, 153.6595161, 85.7272235, 168.0910562],\n",
    "                     [0.0, 230.2262511, 337.5994811, 276.941654, 164.5044287, 127.8653452, 173.6112139]])\n",
    "\n",
    "\n",
    "barencof = barencof[0]/(np.sqrt(np.var(barencof[0])))*scale_pred;\n",
    "# measured_p53 = df[df.index.isin(['211300_s_at', '201746_at'])][columns].astype(float)\n",
    "# measured_p53 = measured_p53.mean(0)\n",
    "# measured_p53 = measured_p53*scale_pred\n",
    "\n",
    "lb = len(barencof)\n",
    "# plt.scatter(np.arange(lb) * 100/lb, barencof, marker='x')\n",
    "# for j in range(num_genes):\n",
    "#     plt.figure()\n",
    "#     plt.plot(mu_post[j*7:(j+1)*7])\n",
    "#     plt.scatter(range(7), Y[j*7:(j+1)*7])\n",
    "\n",
    "\n",
    "# plt.ylim(-1, 4);\n",
    "\n",
    "# Kff = k.K_diag(X)\n",
    "# K_post = Kff - KfxKxx * Kxf\n",
    "# print('K shape', K_post.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean, var = model.predict_f(X)\n",
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(mean)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(genes[genes.index == 'p53'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear response in GRN Inference notebook\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate using https://gpflow.readthedocs.io/en/master/notebooks/advanced/varying_noise.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
