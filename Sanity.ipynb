{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "from tensorflow import math as tfm\n",
    "import tensorflow_probability as tfp\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from reggae.data_loaders import load_covid, DataHolder, scaled_barenco_data\n",
    "from reggae.data_loaders.artificial import artificial_dataset\n",
    "from reggae.mcmc import create_chains, MetropolisHastings, Parameter\n",
    "from reggae.utilities import discretise, logit, LogisticNormal, jitter_cholesky, inverse_positivity, logistic\n",
    "from reggae.plot import plotters\n",
    "from reggae.models import TranscriptionLikelihood, Options, TranscriptionMixedSampler\n",
    "from reggae.models.results import GenericResults, SampleResults\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "plt.rcParams['animation.ffmpeg_path'] = 'C:\\\\Users\\\\Jacob\\\\Documents\\\\ffmpeg-static\\\\bin\\\\ffmpeg.exe'\n",
    "f64 = np.float64\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/reggae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genes = 12\n",
    "num_tfs = 3\n",
    "tf.random.set_seed(1)\n",
    "w = tf.random.normal([num_genes, num_tfs], mean=0.5, stddev=0.71, seed=42, dtype='float64')\n",
    "\n",
    "Δ_delay = tf.constant([0, 0, 0], dtype='float64')\n",
    "\n",
    "w_0 = tf.zeros(num_genes, dtype='float64')\n",
    "\n",
    "true_kbar = logistic((np.array([\n",
    "    [1.6319434062, 1.3962113525, 0.8245041865, 2.2684353378],\n",
    "    [1.7080045137, 3.3992868747, 2.0189033658, 3.7460822389],\n",
    "    [2.4189525448, 1.8480506624, 0.6805040228, 3.1039094120],\n",
    "    [1.7758426875, 0.1907625023, 0.1925539427, 1.8306885751],\n",
    "    [1.7207442227, 0.1252089546, 0.6297333943, 3.2567248923],\n",
    "    [1.4878806850, 3.8623843570, 2.4816128746, 4.3931294404],\n",
    "    [2.5853079514, 2.5115446790, 0.6560607356, 3.0945313562],\n",
    "    [1.6144843688, 1.8651409657, 0.7785363895, 2.6845058360],\n",
    "    [1.4858223122, 0.5396687493, 0.5842698019, 3.0026805243],\n",
    "    [1.6610647522, 2.0486340884, 0.9863876546, 1.4300094581],\n",
    "    [1.6027276189, 1.4320302060, 0.7175033248, 3.2151637970],\n",
    "    [2.1912882714, 2.7935526605, 1.2438786874, 4.3944794204],\n",
    "    [1.3894114279, 1.4726280947, 0.7356719860, 2.2316019158],\n",
    " [1.7927833839, 1.0405867396, 0.4055775218, 2.9888350247],\n",
    " [1.0429721112, 0.1011544950, 0.7330443670, 3.1936843755],\n",
    " [1.2519286771, 2.0617880701, 1.0759649567, 3.9406060364],\n",
    " [1.4297185709, 1.3578824015, 0.6037986912, 2.6512418604],\n",
    " [1.9344878813, 1.4235867760, 0.8226320338, 4.2847217252],\n",
    " [1.4325562449, 1.1940752177, 1.0556928599, 4.1850449557],\n",
    " [0.8911103971, 1.3560009300, 0.5643954823, 3.4300182328],\n",
    " [1.0269654997, 1.0788097511, 0.5268448648, 4.4793299593],\n",
    " [0.8378220502, 1.8148234459, 1.0167440138, 4.4903387696]]\n",
    ")))\n",
    "true_kbar = true_kbar[:num_genes]\n",
    "opt = Options(preprocessing_variance=False, \n",
    "              tf_mrna_present=True, \n",
    "              kinetic_exponential=True,\n",
    "              weights=True,\n",
    "              initial_step_sizes={'logistic': 0.00001, 'latents': 6},\n",
    "              delays=True)\n",
    "\n",
    "\n",
    "data, fbar, kinetics = artificial_dataset(opt, num_genes=num_genes, weights=(w, w_0), delays=Δ_delay.numpy(), true_kbar=true_kbar)\n",
    "true_kbar, true_k_fbar = kinetics\n",
    "f_i = inverse_positivity(fbar)\n",
    "t, τ, common_indices = data.t, data.τ, data.common_indices\n",
    "common_indices = common_indices.numpy()\n",
    "N_p = τ.shape[0]\n",
    "N_m = t.shape[0]\n",
    "\n",
    "def expand(x):\n",
    "    return np.expand_dims(x, 0)\n",
    "true_results = SampleResults(opt, expand(fbar), expand(true_kbar), expand(true_k_fbar), Δ_delay, \n",
    "                             None, expand(logistic(w)), expand(logistic(w_0)), None, None)\n",
    "model = TranscriptionMixedSampler(data, opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Δ_nodelay = tf.constant([0, 0, 0], dtype='float64')\n",
    "w_ = w.numpy()\n",
    "w_[0] = np.array([0.0065790198, 0.00473748, 1.00084])\n",
    "print(w[0])\n",
    "m_pred = model.likelihood.predict_m(true_kbar, true_k_fbar, logistic(w), fbar, logistic(w_0), Δ_delay)\n",
    "m_pred_ = model.likelihood.predict_m(true_kbar, true_k_fbar, logistic(w_), fbar, logistic(w_0), Δ_delay)\n",
    "print(m_pred.shape)\n",
    "\n",
    "plt.plot(m_pred[0, 0])\n",
    "plt.plot(m_pred_[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_probability as tfp\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "dtype = np.float32\n",
    "true_mean = dtype([0, 0])\n",
    "true_cov = dtype([[1, 0.5],\n",
    "                  [0.5, 1]])\n",
    "num_results = 100\n",
    "num_chains = 2\n",
    "\n",
    "# Target distribution is defined through the Cholesky decomposition `L`:\n",
    "L = tf.linalg.cholesky(true_cov)\n",
    "target = tfd.MultivariateNormalTriL(loc=true_mean, scale_tril=L)\n",
    "\n",
    "# Initial state of the chain\n",
    "init_state = np.ones([num_chains, 2], dtype=dtype)\n",
    "\n",
    "# Run Random Walk Metropolis with normal proposal for `num_results`\n",
    "# iterations for `num_chains` independent chains:\n",
    "nuts = tfp.mcmc.NoUTurnSampler(\n",
    "        target_log_prob_fn=target.log_prob,\n",
    "        step_size=1,\n",
    "        seed=54)\n",
    "rwm = tfp.mcmc.RandomWalkMetropolis(\n",
    "        target_log_prob_fn=target.log_prob,\n",
    "        seed=54)\n",
    "\n",
    "sampless = list()\n",
    "\n",
    "start = timer()\n",
    "\n",
    "for sampler in [rwm, nuts]:\n",
    "    print(sampler)\n",
    "    samples, _ = tfp.mcmc.sample_chain(\n",
    "        num_results=num_results,\n",
    "        current_state=init_state,\n",
    "        kernel=sampler,\n",
    "        num_burnin_steps=50,\n",
    "        trace_fn=lambda _, pkr: pkr.is_accepted,\n",
    "        num_steps_between_results=1,  # Thinning.\n",
    "        parallel_iterations=1)\n",
    "\n",
    "    sampless.append(samples)\n",
    "end = timer()\n",
    "print(f'Time taken: {(end - start):.04f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.ma as ma\n",
    "from numpy.random import uniform, seed\n",
    "from matplotlib import cm\n",
    "def gauss(x,y,Sigma,mu):\n",
    "    X=np.vstack((x,y)).T\n",
    "#     print(X.shape, target.prob(X).shape)\n",
    "    mat_multi=np.dot((X-mu[None,...]).dot(np.linalg.inv(Sigma)),(X-mu[None,...]).T)\n",
    "#     print(mat_multi.shape)\n",
    "    return  np.diag(np.exp(-1*(mat_multi)))\n",
    "\n",
    "def plot_countour(x,y,z, title):\n",
    "    # define grid.\n",
    "    xi = np.linspace(-2.1, 2.1, 100)\n",
    "    yi = np.linspace(-2.1, 2.1, 100)\n",
    "    ## grid the data.\n",
    "    zi = griddata((x, y), z, (xi[None,:], yi[:,None]), method='cubic')\n",
    "    levels = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    # contour the gridded data, plotting dots at the randomly spaced data points.\n",
    "    CS = plt.contour(xi,yi,zi,len(levels),linewidths=0.5,colors='k', levels=levels)\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.title(title)\n",
    "plt.style.use('ggplot')\n",
    "rwm = sampless[0][::4, :, :]\n",
    "nuts = sampless[1]\n",
    "samplist = [rwm, nuts]\n",
    "titles = ['', '']\n",
    "print(rwm.shape, nuts.shape)\n",
    "plt.figure(figsize=(5, 3))\n",
    "for i in range(2):\n",
    "#     plt.subplot(2, 2, i+1)\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    seed(1234)\n",
    "    npts = 1000\n",
    "    x = uniform(-2, 2, npts)\n",
    "    y = uniform(-2, 2, npts)\n",
    "    z = gauss(x, y, Sigma=np.asarray(true_cov), mu=np.asarray(true_mean))\n",
    "\n",
    "    samp = tf.reduce_mean(samplist[i], axis=1)\n",
    "    plt.plot(samp[:, 0], samp[:, 1], c='slategrey')\n",
    "    plot_countour(x, y, z, titles[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Our 2-dimensional distribution will be over variables X and Y\n",
    "N = 60\n",
    "X = np.linspace(-3, 3, N)\n",
    "Y = np.linspace(-3, 4, N)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "# Mean vector and covariance matrix\n",
    "mu = np.array([0., 1.])\n",
    "Sigma = np.array([[ 1. , -0.5], [-0.5,  1.5]])\n",
    "\n",
    "# Pack X and Y into a single 3-dimensional array\n",
    "pos = np.empty(X.shape + (2,))\n",
    "pos[:, :, 0] = X\n",
    "pos[:, :, 1] = Y\n",
    "\n",
    "def multivariate_gaussian(pos, mu, Sigma):\n",
    "    \"\"\"Return the multivariate Gaussian distribution on array pos.\n",
    "\n",
    "    pos is an array constructed by packing the meshed arrays of variables\n",
    "    x_1, x_2, x_3, ..., x_k into its _last_ dimension.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n = mu.shape[0]\n",
    "    Sigma_det = np.linalg.det(Sigma)\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "    N = np.sqrt((2*np.pi)**n * Sigma_det)\n",
    "    # This einsum call calculates (x-mu)T.Sigma-1.(x-mu) in a vectorized\n",
    "    # way across all the input variables.\n",
    "    fac = np.einsum('...k,kl,...l->...', pos-mu, Sigma_inv, pos-mu)\n",
    "\n",
    "    return np.exp(-fac / 2) / N\n",
    "\n",
    "# The distribution on the variables X, Y packed into pos.\n",
    "Z = multivariate_gaussian(pos, mu, Sigma)\n",
    "\n",
    "# Create a surface plot and projected filled contour plot under it.\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, rstride=3, cstride=3, linewidth=1, antialiased=True,\n",
    "                cmap=cm.viridis)\n",
    "\n",
    "cset = ax.contourf(X, Y, Z, zdir='z', offset=-0.15, cmap=cm.viridis)\n",
    "plt.axhline(0.3, color='red', linewidth=1000)\n",
    "# Adjust the limits, ticks and view angle\n",
    "ax.set_zlim(-0.15,0.2)\n",
    "ax.set_zticks(np.linspace(0,0.2,5))\n",
    "ax.view_init(27, -21)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = tf.ones(3, dtype='float64')\n",
    "num_tfs = current_state.shape[0]\n",
    "new_state = current_state\n",
    "Δrange = np.arange(0, 10+1, dtype='float64')\n",
    "Δrange_tf = tf.range(0, 10+1, dtype='float64')\n",
    "\n",
    "for i in range(num_tfs):\n",
    "    # Generate normalised cumulative distribution\n",
    "    probs = list()\n",
    "    mask = np.zeros((num_tfs, ), dtype='float64')\n",
    "    mask[i] = 1\n",
    "\n",
    "    for Δ in Δrange:\n",
    "        test_state = (1-mask) * new_state + mask * Δ\n",
    "\n",
    "        probs.append(f64(1+i*Δ)) #+ tf.reduce_sum(self.prior.log_prob(Δ)))\n",
    "\n",
    "    probs =  tf.stack(probs) - tfm.reduce_max(probs)\n",
    "    probs = tfm.exp(probs)\n",
    "    probs = probs / tfm.reduce_sum(probs)\n",
    "    cumsum = tfm.cumsum(probs)\n",
    "    # tf.print(cumsum)\n",
    "    u = np.random.uniform()\n",
    "    index = tf.where(cumsum == tf.reduce_min(cumsum[(cumsum - u) > 0]))\n",
    "    chosen = Δrange_tf[index[0][0]]\n",
    "    new_state = (1-mask) * new_state + mask * chosen\n",
    "    print(cumsum)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_prior = tfd.Normal(0.0, 0.1)\n",
    "bias_prior = tfd.Normal(0.0, 1.0)  # near-uniform\n",
    "\n",
    "def get_initial_state(weight_prior, bias_prior, num_features, layers=None):\n",
    "    \"\"\"generate starting point for creating Markov chain\n",
    "        of weights and biases for fully connected NN\n",
    "    Keyword Arguments:\n",
    "        layers {tuple} -- number of nodes in each layer of the network\n",
    "    Returns:\n",
    "        list -- architecture of FCNN with weigths and bias tensors for each layer\n",
    "    \"\"\"\n",
    "    # make sure the last layer has two nodes, so that output can be split into\n",
    "    # predictive mean and learned loss attenuation (see https://arxiv.org/abs/1703.04977)\n",
    "    # which the network learns individually\n",
    "    if layers is not None:\n",
    "        assert layers[-1] == 2\n",
    "    if layers is None:\n",
    "        layers = (\n",
    "            num_features,\n",
    "            num_features // 2,\n",
    "            num_features // 5,\n",
    "            num_features // 10,\n",
    "            2,\n",
    "        )\n",
    "    else:\n",
    "        layers.insert(0, num_features)\n",
    "\n",
    "    architecture = []\n",
    "    for idx in range(len(layers) - 1):\n",
    "        weigths = weight_prior.sample((layers[idx], layers[idx + 1]))\n",
    "        biases = bias_prior.sample((layers[idx + 1]))\n",
    "        # weigths = tf.zeros((layers[idx], layers[idx + 1]))\n",
    "        # biases = tf.zeros((layers[idx + 1]))\n",
    "        architecture.extend((weigths, biases))\n",
    "    return architecture\n",
    "\n",
    "initial_state = get_initial_state(weight_prior, bias_prior, 4)\n",
    "\n",
    "print(len(state))\n",
    "print(state[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow\n",
    "def plotkernelsample(k, ax, xmin=-15, xmax=15):\n",
    "    xx = np.linspace(xmin, xmax, 100)[:, None]\n",
    "    K = k(xx)\n",
    "    ax.plot(xx, np.random.multivariate_normal(np.zeros(100), K, 3).T)\n",
    "    ax.set_title(k.__class__.__name__)\n",
    "\n",
    "\n",
    "np.random.seed(27)\n",
    "f, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True, sharey=True)\n",
    "plotkernelsample(gpflow.kernels.RBF(), axes[0])\n",
    "plotkernelsample(gpflow.kernels.ArcCosine(0, weight_variances=1.0, bias_variance=0.5), axes[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from F kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_observed, f_observed, t = load_covid()\n",
    "\n",
    "m_df, m_observed = m_observed \n",
    "f_df, f_observed = f_observed\n",
    "# Shape of m_observed = (replicates, genes, times)\n",
    "m_observed = m_observed\n",
    "f_observed = f_observed\n",
    "\n",
    "num_genes = m_observed.shape[0]\n",
    "τ, common_indices = discretise(t, num_disc=10)\n",
    "N_p = τ.shape[0]\n",
    "N_m = m_observed.shape[1]\n",
    "\n",
    "data = (m_observed, f_observed)\n",
    "time = (t, τ, tf.constant(common_indices))\n",
    "\n",
    "data = DataHolder(data, None, time)\n",
    "N_p = τ.shape[0]\n",
    "\n",
    "opt = Options(preprocessing_variance=False, \n",
    "              tf_mrna_present=True, \n",
    "              delays=False, \n",
    "              initial_step_sizes={'nuts': 0.00005, 'fbar': 0.01},\n",
    "              kernel='rbf')\n",
    "\n",
    "model = TranscriptionMixedSampler(data, opt)\n",
    "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})\n",
    "print(N_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = τ.reshape((-1,1))\n",
    "X_norm = np.sum(X ** 2, axis = -1)\n",
    "def kf(v, l):\n",
    "    return v * np.exp(-(f64(1)/l) * (X_norm[:,None] + X_norm[None,:] - 2 * np.dot(X, X.T)))\n",
    "K = kf(1, 10)\n",
    "plt.imshow(K)\n",
    "iK = tf.linalg.inv(K+tf.linalg.diag(1*np.ones(N_p, dtype='float64')))\n",
    "tf.linalg.cholesky(iK)\n",
    "\n",
    "def add_diag(A, B):\n",
    "    C= A + tf.linalg.diag(tf.linalg.diag_part(B))\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kernel = model.kernel_selector()\n",
    "\n",
    "@tf.function\n",
    "def run():\n",
    "    step_size = 0.1 *tf.ones((N_p), dtype='float64')\n",
    "\n",
    "    current_state =  0*tf.ones((1, 2, N_p), dtype='float64')\n",
    "\n",
    "    S = tf.linalg.diag(step_size)\n",
    "\n",
    "    kernel_params = (f64(1)*tf.ones(2, dtype='float64'), f64(5)*tf.ones(2, dtype='float64'))\n",
    "    _, K = kernel(*kernel_params)#kf(*kernel_params)\n",
    "\n",
    "    K = K+tf.linalg.diag(1e-7*tf.ones(N_p, dtype='float64'))\n",
    "    print(K.shape)\n",
    "    # Propose new params\n",
    "    v = model.kernel_selector.proposal(0, tf.ones(2, dtype='float64')).sample()\n",
    "    l2 = model.kernel_selector.proposal(1, 2*tf.ones(2, dtype='float64')).sample()\n",
    "    m_, K_ = kernel(v, l2)\n",
    "    K_ = K_+tf.linalg.diag(1e-7*tf.ones(N_p, dtype='float64'))\n",
    "\n",
    "    # current_state = tf.zeros(N_p, dtype='float64')#tfd.MultivariateNormalFullCovariance(0, K).sample()\n",
    "    new_state = tf.identity(current_state)\n",
    "\n",
    "    # plt.plot(current_state)\n",
    "    # plt.figure()\n",
    "    fbar = current_state\n",
    "    fstar = tf.zeros_like(fbar)\n",
    "    for r in range(1):\n",
    "        fbar = new_state[r]\n",
    "\n",
    "        iK = tf.linalg.inv(K)\n",
    "        iK_ = tf.linalg.inv(K_)\n",
    "\n",
    "        U_invR = tf.linalg.cholesky(add_diag(iK, 1/S))\n",
    "        U_invR = tf.transpose(U_invR, [0, 2, 1])\n",
    "        U_invR_ = jitter_cholesky(add_diag(iK_, 1/S))\n",
    "        U_invR_ = tf.transpose(U_invR_, [0, 2, 1])\n",
    "\n",
    "        gg = tfd.MultivariateNormalDiag(fbar, step_size).sample()\n",
    "        print('gg', gg.shape)\n",
    "        print('U_invR', U_invR.shape)\n",
    "\n",
    "        Sinv_g = gg / step_size\n",
    "\n",
    "        # f = tf.zeros((self.num_tfs, 1), dtype='float64')\n",
    "        nu = tf.linalg.matvec(U_invR, fbar) - tf.squeeze(tf.linalg.solve(tf.transpose(U_invR, [0, 2, 1]), tf.expand_dims(Sinv_g, -1)), -1)\n",
    "        f = tf.linalg.solve(U_invR_, tf.expand_dims(nu, -1)) + tf.linalg.cholesky_solve(tf.transpose(U_invR_, [0, 2, 1]), tf.expand_dims(Sinv_g, -1))\n",
    "        f = tf.squeeze(f, -1)\n",
    "            # mask = np.zeros((self.num_tfs, 1), dtype='float64')\n",
    "            # mask[i] = 1\n",
    "            # f = (1-mask) * f + mask * f_i\n",
    "\n",
    "        mask = np.zeros((3, 1, 1), dtype='float64')\n",
    "        mask[r] = 1\n",
    "        new_state = (1-mask) * new_state + mask * f\n",
    "    hyp = [v, l2]\n",
    "    for i in range(2):\n",
    "        tf.print(f.shape)\n",
    "        prob = tf.reduce_sum(new_state[i])\n",
    "        other_prob = tf.reduce_sum(nu)\n",
    "        tf.print(prob, other_prob)\n",
    "        is_accepted = tf.less(other_prob, prob)\n",
    "        return_value = tf.zeros((1, 2, N_p), dtype='float64')\n",
    "        tf.print(is_accepted)\n",
    "        is_accepted = tf.random.uniform((1,), dtype='float64') < tf.math.minimum(f64(1), prob)\n",
    "        tf.print('here', is_accepted[0])\n",
    "        if not is_accepted[0]:\n",
    "            hyp[0] = f64(3)\n",
    "            tf.print('hello')\n",
    "            mask = np.zeros((1, 2, 1), dtype='float64')\n",
    "            mask[:, i] = 1\n",
    "            test_state = (1-mask) * current_state + mask * new_state\n",
    "            return_value = tf.ones(10, dtype='float64')\n",
    "        tf.print(return_value)\n",
    "run()\n",
    "\n",
    "plt.figure()\n",
    "print(new_state.shape)\n",
    "plt.plot(τ, new_state[0, 0])\n",
    "plt.figure()\n",
    "plt.plot(τ, new_state[0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_mask = np.zeros((2,), dtype='float64')\n",
    "hyp_mask[1] =1\n",
    "(1-hyp_mask) * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1 *tf.ones((155), dtype='float64')\n",
    "\n",
    "# Untransformed tf mRNA vectors F (Step 1)\n",
    "old_probs = list()\n",
    "new_state = tf.ones((1, 1, 155), dtype='float64')\n",
    "current_state = 0.1*tf.ones((1, 1, 155), dtype='float64')\n",
    "\n",
    "S = tf.linalg.diag(step_size)\n",
    "\n",
    "kernel_params = (f64(10), f64(5))\n",
    "m, K = model.kernel_selector()(*kernel_params)\n",
    "for r in range(1):\n",
    "    # Gibbs step\n",
    "    fbar = current_state[r]\n",
    "    z_i = tfd.MultivariateNormalDiag(fbar, step_size).sample()\n",
    "    fstar = tf.zeros_like(fbar)\n",
    "\n",
    "    for i in range(1):\n",
    "        invKsigmaK = tf.matmul(tf.linalg.inv(K[i]+tf.linalg.diag(step_size)), K[i]) # (C_i + hI)C_i\n",
    "        L = jitter_cholesky(K[i]-tf.matmul(K[i], invKsigmaK))\n",
    "        c_mu = tf.matmul(z_i[i, None], invKsigmaK)\n",
    "        nu = tf.random.normal((1, L.shape[0]), dtype='float64')\n",
    "        fstar_i = tf.linalg.matvec(L, nu) + c_mu # 0.5 1.5\n",
    "        mask = np.zeros((1, 1), dtype='float64')\n",
    "        mask[i] = 1\n",
    "        fstar = (1-mask) * fstar + mask * fstar_i\n",
    "\n",
    "    mask = np.zeros((1, 1, 1), dtype='float64')\n",
    "    mask[r] = 1\n",
    "    test_state = (1-mask) * new_state + mask * fstar\n",
    "    plt.plot(inverse_positivity(fstar[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticNormal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reggae.utilities import LogisticNormal\n",
    "class LogisticNormal():\n",
    "    def __init__(self, a, b, loc=f64(0), scale=f64(1), allow_nan_stats=True):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.dist = tfd.LogitNormal(loc, scale, allow_nan_stats=allow_nan_stats)\n",
    "#         super().__init__(loc, scale, allow_nan_stats=allow_nan_stats)\n",
    "    def log_prob(self, x):\n",
    "        x = (x-self.a)/(self.b-self.a)\n",
    "        log_prob = self.dist.log_prob(x)\n",
    "        log_prob = tf.where(\n",
    "            tf.math.is_nan(log_prob),\n",
    "            -1e2*tf.ones([], log_prob.dtype),\n",
    "            log_prob)\n",
    "\n",
    "        return log_prob\n",
    "    def prob(self, x):\n",
    "        x = (x-self.a)/(self.b-self.a)\n",
    "        log_prob = self.dist.prob(x)\n",
    "        log_prob = tf.where(\n",
    "            tf.math.is_nan(log_prob),\n",
    "            -1e2*tf.ones([], log_prob.dtype),\n",
    "            log_prob)\n",
    "\n",
    "        return log_prob\n",
    "x = np.linspace(-2, 2, 100)\n",
    "# x = logit(x)\n",
    "dist = LogisticNormal(-1, 1)\n",
    "y = dist.prob(x)\n",
    "plt.ylim(-4, 4)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse transform sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = norm.cdf(x)\n",
    "u = 0.68\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(x, y)\n",
    "plt.scatter(0.47, 0.68, color='red', marker='.', s=120)\n",
    "plt.axhline(u, color='grey')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('CDF')\n",
    "# plt.xticks(x[np.linspace(-3, 3, 7)])\n",
    "# fig.axes[0].set_xticklabels(np.arange(-3, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(-1, 1)\n",
    "k = gpflow.kernels.Matern52()\n",
    "m = gpflow.models.GPR(data=(X, y), kernel=k, mean_function=None)\n",
    "opt = gpflow.optimizers.Scipy()\n",
    "def objective_closure():\n",
    "    return - m.log_marginal_likelihood()\n",
    "\n",
    "opt_logs = opt.minimize(objective_closure,\n",
    "                        m.trainable_variables,\n",
    "                        options=dict(maxiter=20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.predict_f(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpr.kernel_(X).shape, gpr.kernel_)#K = self.kernel_(self.X_train_)\n",
    "print (X.shape, gpr.X_train_.shape)\n",
    "\n",
    "dists = pdist(X / 1, metric='sqeuclidean')\n",
    "K = np.exp(-.5 * dists)\n",
    "print(K.shape)\n",
    "# convert from upper-triangular matrix to square matrix\n",
    "K = squareform(K)\n",
    "print(K.shape)\n",
    "np.fill_diagonal(K, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.cast(tf.range(4)*2, tf.float64)\n",
    "t_dist = tf.expand_dims(t, axis=0) - tf.expand_dims(t, axis=1)\n",
    "t_ = tf.transpose(tf.reshape(tf.tile(t, [4]), [ 4, tf.shape(t)[0]]))\n",
    "t_prime = tf.reshape(tf.tile(t, [4]), [ 4, tf.shape(t)[0]])\n",
    "\n",
    "D = tf.ones(4)\n",
    "\n",
    "print(t)\n",
    "print(t_dist)\n",
    "print(t_)\n",
    "print(t_prime-t_)\n",
    "\n",
    "m = [1,2,3,4]\n",
    "# Compute m[i] * (t'-t) + t' for all i, t, t'\n",
    "result = np.zeros((4, 4))\n",
    "for i, t_ in enumerate(t):\n",
    "    for j, t_prime in enumerate(t):\n",
    "        for mk in m:\n",
    "            result[i, j] += mk * (t_prime - t_) + t_prime\n",
    "    \n",
    "print('Result 1:')\n",
    "print(result)\n",
    "print()\n",
    "print('Result 2:')\n",
    "\n",
    "add = tf.transpose(tf.reshape(tf.tile(t, [4]), [ 4, tf.shape(t)[0]]))\n",
    "result = np.zeros((4, 4))\n",
    "result += m*t_dist + add\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "t = 3\n",
    "tprime = 3\n",
    "l = 2\n",
    "np.exp(-((t-tprime)**2)/(l**2))\n",
    "\n",
    "times   =   np.array([2.0,4.0, 6.0, 8.0])[:,None]\n",
    "times.shape\n",
    "times[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_times=3\n",
    "num_genes=2\n",
    "from gpflow.utilities import print_summary, positive\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from tensorflow import math as tm\n",
    "import math\n",
    "PI = tf.constant(math.pi, dtype='float64')\n",
    "\n",
    "class Kern(gpflow.kernels.Kernel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(active_dims=[0])\n",
    "        self.lengthscale = gpflow.Parameter(1.0, transform=positive())\n",
    "#         B = tf.ones(5)\n",
    "#         self.B = gpflow.Parameter(B)\n",
    "#         self.D = gpflow.Parameter(np.random.uniform(0.5, 1, 5), transform=positive())\n",
    "#         S = tf.ones(5)\n",
    "        affine = tfb.AffineScalar(shift=tf.cast(0., tf.float64),\n",
    "                                  scale=tf.cast(3.-0., tf.float64))\n",
    "        sigmoid = tfb.Sigmoid()\n",
    "        logistic = tfb.Chain([affine, sigmoid])\n",
    "\n",
    "        self.D = [gpflow.Parameter(0.7, transform=logistic, dtype=tf.float64) for _ in range(num_genes)]\n",
    "        self.D[0].trainable = False\n",
    "        self.D[0].assign(0.8)\n",
    "\n",
    "        self.S = [gpflow.Parameter(0.7, transform=logistic, dtype=tf.float64) for _ in range(num_genes)]\n",
    "        self.S[0].trainable = False\n",
    "        self.S[0].assign(1)\n",
    "\n",
    "    def K(self, X, X2=None):\n",
    "        block_size = num_times\n",
    "        if X2 is None:\n",
    "            shape = [X.shape[0],X.shape[0]]\n",
    "            K_xx = tf.zeros(shape, dtype='float64')\n",
    "            for j in range(num_genes):\n",
    "                for k in range(num_genes):\n",
    "                    mask = np.ones(shape)\n",
    "                    other = np.zeros(shape)\n",
    "                    mask[j*block_size:(j+1)*block_size, \n",
    "                         k*block_size:(k+1)*block_size] = 0\n",
    "\n",
    "                    pad_top = j*block_size\n",
    "                    pad_left = k*block_size\n",
    "                    pad_right = 0 if k == num_genes-1 else shape[0]-block_size-pad_left\n",
    "                    pad_bottom = 0 if j == num_genes-1 else shape[0]-block_size-pad_top\n",
    "                    other = tf.pad(self.k_xx(j, k),\n",
    "                                   tf.constant([\n",
    "                                       [pad_top,pad_bottom],\n",
    "                                       [pad_left,pad_right]\n",
    "                                   ]), 'CONSTANT'\n",
    "                                  )\n",
    "    #                     print(j, k, pad_right, pad_bottom, other.shape)\n",
    "                    K_xx = K_xx * mask + other * (1 - mask)\n",
    "\n",
    "\n",
    "            return K_xx\n",
    "        else:\n",
    "            print('K not none K_xf\\n')\n",
    "            shape = [X.shape[0],num_times]\n",
    "            K_xf = tf.zeros(shape, dtype='float64')\n",
    "            for j in range(num_genes):\n",
    "                mask = np.ones(shape)\n",
    "                other = np.zeros(shape)\n",
    "                mask[j*block_size:(j+1)*block_size] = 0\n",
    "                other[j*block_size:(j+1)*block_size] = self.k_xf(j, X)\n",
    "\n",
    "                K_xf = K_xf * mask + other * (1-mask) \n",
    "            return K_xf\n",
    "\n",
    "\n",
    "    def gamma(self, k):\n",
    "        return self.D[k]*self.lengthscale/2\n",
    "\n",
    "    def h(self, k, j, tprime, t):\n",
    "        l = self.lengthscale\n",
    "\n",
    "        multiplier = tm.exp(self.gamma(k))**2 / (self.D[j]+self.D[k])\n",
    "        first_erf_term = tm.erf((tprime-t)/l - self.gamma(k)) + tm.erf(t/l + self.gamma(k))\n",
    "        second_erf_term = tm.erf(tprime/l - self.gamma(k)) + tm.erf(self.gamma(k))\n",
    "        return multiplier * (tm.exp(-self.D[k]*(tprime-t)) * first_erf_term - \\\n",
    "                             tm.exp(-self.D[k]*tprime-self.D[j]) * second_erf_term)\n",
    "\n",
    "\n",
    "    def h_quick(self, k, j, primefirst=True):\n",
    "        l = self.lengthscale\n",
    "        t_prime, t_, t_dist = self.get_distance_matrix(primefirst=primefirst, size=num_times)\n",
    "            \n",
    "        multiplier = tm.exp(self.gamma(k))**2 / (self.D[j]+self.D[k])\n",
    "        first_erf_term = tm.erf(t_dist/l - self.gamma(k)) + tm.erf(t_/l + self.gamma(k))\n",
    "        second_erf_term = tm.erf(t_prime/l - self.gamma(k)) + tm.erf(self.gamma(k))\n",
    "        \n",
    "        return multiplier * (tf.multiply(tm.exp(-tm.multiply(self.D[k],t_dist)) , first_erf_term) - \\\n",
    "                             tf.multiply(tm.exp(-tm.multiply(self.D[k],t_prime)-self.D[j]) , second_erf_term))\n",
    "    \n",
    "\n",
    "    def k_xx(self, j, k):\n",
    "        '''k_xx(t, tprime)'''\n",
    "        mult = self.S[j]*self.S[k]*self.lengthscale*0.5*tm.sqrt(PI)\n",
    "        return mult*(self.h_quick(k, j) + self.h_quick(j, k, primefirst=False))\n",
    "\n",
    "    def k_xx_(self, j, k):\n",
    "        '''k_xx(t, tprime)'''\n",
    "        k_xx = np.zeros((num_times, num_times))\n",
    "        for tprime in range(num_times):\n",
    "            for t in range(num_times):\n",
    "                mult = self.S[j]*self.S[k]*self.lengthscale*0.5*tm.sqrt(PI)\n",
    "                k_xx[t,tprime] = mult*(self.h(k, j, tprime*2, t*2) + self.h(j, k, t*2, tprime*2))\n",
    "        print(k_xx)\n",
    "        return k_xx\n",
    "\n",
    "    def get_distance_matrix(self, primefirst=True, size=7):\n",
    "        t = tf.cast(tf.range(size)*2, tf.float64)\n",
    "        t_ = tf.transpose(tf.reshape(tf.tile(t, [size]), [ size, tf.shape(t)[0]]))\n",
    "        t_prime = tf.reshape(tf.tile(t, [size]), [ size, tf.shape(t)[0]])\n",
    "        if not primefirst:\n",
    "            t_prime = tf.transpose(tf.reshape(tf.tile(t, [size]), [ size, tf.shape(t)[0]]))\n",
    "            t_ = tf.reshape(tf.tile(t, [size]), [ size, tf.shape(t)[0]])\n",
    "\n",
    "        return t_prime, t_, t_prime-t_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = np.arange(num_times, dtype='float64')*2\n",
    "X = np.c_[[X for _ in range(num_genes)]].reshape(-1)\n",
    "print(X)\n",
    "k = Kern()\n",
    "display(k.K(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array([[0.        , 0.19252085, 0.03952372, 0.        , 0.15377981, 0.03846892],\n",
    "       [0.19252085, 1.09337319, 0.3065648 , 0.13141296, 0.78650799, 0.24865445],\n",
    "       [0.03952372, 0.3065648 , 1.34475164, 0.02697853, 0.21804878, 0.98498977],\n",
    "       [0.        , 0.13141296, 0.02697853, 0.        , 0.10543729, 0.02637576],\n",
    "       [0.15377981, 0.78650799, 0.21804878, 0.10543729, 0.56618178, 0.17712899],\n",
    "       [0.03846892, 0.24865445, 0.98498977, 0.02637576, 0.17712899, 0.72327064]])>\n",
    "6, 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
