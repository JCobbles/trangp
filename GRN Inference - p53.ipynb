{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p53 network - REGGaE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## TensorFlow SETUP\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "from tensorflow import math as tfm\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from reggae.data_loaders import load_barenco_puma, load_3day_dros, DataHolder, scaled_barenco_data\n",
    "from reggae.mcmc import create_chains, MetropolisHastings, Parameter\n",
    "from reggae.utilities import discretise, logit, LogisticNormal, inverse_positivity\n",
    "from reggae.plot import plotters\n",
    "from reggae.models import TranscriptionLikelihood, Options, TranscriptionMixedSampler\n",
    "from reggae.models.results import GenericResults\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "f64 = np.float64\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.4f}\".format(x)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_observed, f_observed, σ2_m_pre, σ2_f_pre, t = load_barenco_puma()\n",
    "\n",
    "# m_observed, f_observed, t = load_3day_dros()\n",
    "\n",
    "replicate = 0\n",
    "\n",
    "m_df, m_observed = m_observed \n",
    "f_df, f_observed = f_observed\n",
    "# Shape of m_observed = (replicates, genes, times)\n",
    "m_observed = m_observed\n",
    "f_observed = f_observed\n",
    "σ2_m_pre = f64(σ2_m_pre[0])\n",
    "σ2_f_pre = f64(σ2_f_pre[0])\n",
    "\n",
    "num_genes = m_observed.shape[1]\n",
    "τ, common_indices = discretise(t, num_disc=13)\n",
    "N_p = τ.shape[0]\n",
    "N_m = m_observed.shape[1]\n",
    "\n",
    "data = (m_observed, f_observed)\n",
    "noise_data = (σ2_m_pre, σ2_f_pre)\n",
    "time = (t, τ, tf.constant(common_indices))\n",
    "\n",
    "data = DataHolder(data, noise_data, time)\n",
    "N_p = τ.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = Options(preprocessing_variance=True, \n",
    "              tf_mrna_present=True, \n",
    "              delays=False,\n",
    "              weights=False,\n",
    "              initial_step_sizes={'logistic': 0.00009},\n",
    "              kernel='rbf')\n",
    "\n",
    "model = TranscriptionMixedSampler(data, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = timer()\n",
    "samples, is_accepted = model.sample(T=200, burn_in=0)\n",
    "end = timer()\n",
    "print(f'Time taken: {(end - start):.04f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first clear C:\\Users\\Jacob\\AppData\\Local\\Temp\\.tensorboard-info\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/reggae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples, is_accepted = model.sample(T=1, burn_in=0, profile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><th>Processor</th><th># Iterations</th><th>Time</th><th>Note</th></tr>\n",
    "    <tr><td>CPU</td><td>20</td><td>54.7s</td><td></td></tr>\n",
    "    <tr><td>CPU</td><td>20</td><td>38.4s</td><td>Merged weight and kinetics</td></tr>\n",
    "    <tr><td>CPU</td><td>200</td><td>436.2s</td><td></td></tr>\n",
    "    <tr><td>CPU</td><td>200</td><td>396.1s</td><td>No prob update</td></tr>\n",
    "    <tr><td>CPU</td><td>800</td><td>901.9s</td><td>No prob update, merged w,k</td></tr>\n",
    "    <tr><td>CPU</td><td>800</td><td>816.2s</td><td>No prob update, merged w,k</td></tr>\n",
    "    <tr><td>GPU</td><td>2</td><td>40.8s</td><td></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([20, 200], [54.7, 436.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = timer()\n",
    "samples, is_accepted = model.sample(T=200, burn_in=0)\n",
    "end = timer()\n",
    "print(f'Time taken: {(end - start):.04f}s') # Time in seconds, e.g. 5.38091952400282\n",
    "# CPU: Time taken: 55.5497s (20 iter: 54.7s)\n",
    "# GPU: Time taken: 319.8316s (2 iter: 40.8s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pcs = list()\n",
    "for i, param in enumerate(model.state_indices):\n",
    "    print(i)\n",
    "    pcs.append(tf.reduce_mean(tf.cast(is_accepted[i], dtype=tf.float32)).numpy())\n",
    "\n",
    "display(pd.DataFrame([[f'{100*pc:.02f}%' for pc in pcs]], columns=list(model.state_indices)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "σ2_f = None\n",
    "results = model.results()\n",
    "results.k_fbar = np.expand_dims(results.k_fbar, 2)\n",
    "\n",
    "m_preds = list()\n",
    "for i in range(1, 20):\n",
    "    m_preds.append(model.predict_m_with_results(results, i))\n",
    "m_preds = np.array(m_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotters.generate_report(data, results, m_preds, plot_barenco=True, \n",
    "                         gene_names=m_df.index, num_hpd=40, replicate=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(results.f[:, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "num_genes = kbar.shape[1]\n",
    "k_latest = np.mean(logit(kbar[-10:]), axis=0)\n",
    "print(k_latest)\n",
    "B = k_latest[:,1]\n",
    "D = k_latest[:,2]\n",
    "S = k_latest[:,3]\n",
    "\n",
    "plt.bar(np.arange(num_genes)-0.2, B, width=0.2, tick_label=m_df.index, label='Basal rate')\n",
    "plt.bar(np.arange(num_genes), D, width=0.2, tick_label=m_df.index, label='Sensitivity')\n",
    "plt.bar(np.arange(num_genes)+0.2, S, width=0.2, tick_label=m_df.index, label='Decay rate')\n",
    "plt.yscale('log')\n",
    "plt.title('Mechanistic Parameters')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.title('Noise variances')\n",
    "for i, j in enumerate(range(num_genes)):\n",
    "    ax = plt.subplot(num_genes, num_genes-2, i+1)\n",
    "    plt.title(m_df.index[j])\n",
    "    plt.plot(σ2_m[:,j])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keys = job[0].acceptance_rates.keys()\n",
    "\n",
    "variables = {key : np.empty((0, T, *job[0].samples[key].get().shape[1:])) for key in keys}\n",
    "\n",
    "for res in job:\n",
    "    for key in keys:\n",
    "        variables[key] = np.append(variables[key], np.expand_dims(res.samples[key].get(), 0), axis=0)\n",
    "\n",
    "plt.plot(variables['L'][:,-100:].T)\n",
    "\n",
    "mixes = {key: arviz.convert_to_inference_data(variables[key]) for key in keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rhat\n",
    "Rhat is the ratio of posterior variance and within-chain variance. If the ratio exceeds 1.1 then we consider the chains have not mixed well. As the between-chain variance tends to the within-chain then R tends to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Rhat = arviz.rhat(mixes['fbar'])\n",
    "\n",
    "Rhats = np.array([np.mean(arviz.rhat(mixes[key]).x.values) for key in keys])\n",
    "\n",
    "rhat_df = pd.DataFrame([[*Rhats], [*(Rhats < 1.1)]], columns=keys)\n",
    "\n",
    "display(rhat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank plots\n",
    "\n",
    "Rank plots are histograms of the ranked posterior draws (ranked over all\n",
    "    chains) plotted separately for each chain.\n",
    "    If all of the chains are targeting the same posterior, we expect the ranks in each chain to be\n",
    "    uniform, whereas if one chain has a different location or scale parameter, this will be\n",
    "    reflected in the deviation from uniformity. If rank plots of all chains look similar, this\n",
    "    indicates good mixing of the chains.\n",
    "\n",
    "Rank-normalization, folding, and localization: An improved R-hat\n",
    "    for assessing convergence of MCMC. arXiv preprint https://arxiv.org/abs/1903.08008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arviz.plot_rank(L_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effective sample sizes\n",
    "\n",
    "Plot quantile, local or evolution of effective sample sizes (ESS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arviz.plot_ess(L_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte-Carlo Standard Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arviz.plot_mcse(L_mix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Plot\n",
    "Plot parallel coordinates plot showing posterior points with and without divergences.\n",
    "\n",
    "Described by https://arxiv.org/abs/1709.01449, suggested by Ari Hartikainen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arviz.plot_parallel(azl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step size is standard dev, too small means it takes long time to reach high density areas. too long means we reject many of samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit374b75da0e1b40de8b7922d3f142c01d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
