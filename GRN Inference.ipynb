{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from collections import namedtuple\n",
    "from ipywidgets import IntProgress\n",
    "# from IPython.display import display\n",
    "\n",
    "import gpflow\n",
    "from gpflow.utilities import print_summary, positive\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import mcmc\n",
    "\n",
    "import arviz\n",
    "\n",
    "from reggae.data_loaders import load_barenco_puma, load_3day_dros\n",
    "from reggae.parameter import Parameter\n",
    "from reggae.utilities import get_rbf_dist, exp, mult, ArrayList, discretise\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "PI = tf.constant(math.pi, dtype='float64')\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df, genes, genes_se, m_observed, f_observed, σ2_m_pre, σ2_f_pre, t = load_barenco_puma()\n",
    "\n",
    "m_observed, f_observed, t = load_3day_dros()\n",
    "m_df, m_observed = m_observed\n",
    "f_df, f_observed = f_observed\n",
    "N_m = t.shape[0]      # Number of observations\n",
    "I = f_observed.shape[0] # Number of TFs\n",
    "print(I)\n",
    "num_genes = m_observed.shape[0]\n",
    "f_observed = np.atleast_2d(f_observed)\n",
    "print(m_observed)\n",
    "τ, common_indices = discretise(t)\n",
    "N_p = τ.shape[0]\n",
    "min_dist = min(t[1:]-t[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis Hastings Custom MCMC Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})\n",
    "\n",
    "def jitter_cholesky(A):\n",
    "    try:\n",
    "        jitter1 = tf.linalg.diag(1e-7 * np.ones(A.shape[0]))\n",
    "        return tf.linalg.cholesky(A + jitter1)\n",
    "    except:\n",
    "        jitter2 = tf.linalg.diag(1e-5 * np.ones(A.shape[0]))\n",
    "        return tf.linalg.cholesky(A + jitter2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.seterr(all='raise')\n",
    "\n",
    "class MetropolisHastings():\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.clear_samples()\n",
    "\n",
    "    def clear_samples(self):\n",
    "        self.samples = {param.name: ArrayList(param.value.shape) for param in self.params}\n",
    "        self.samples['acc_rates'] = list()\n",
    "\n",
    "    def sample(self, T=20000, store_every=10, burn_in=1000, report_every=100, tune_every=50):\n",
    "        print('----- Metropolis Begins -----')\n",
    "                \n",
    "        self.acceptance_rates = {param.name: 0. for param in self.params} # Reset acceptance rates\n",
    "        self.samples['acc_rates'] = list()\n",
    "        f = IntProgress(description='Running', min=0, max=T) # instantiate the bar\n",
    "        display(f)\n",
    "        for iteration_number in range(T):\n",
    "            if iteration_number % report_every == 0:\n",
    "                f.value = iteration_number \n",
    "            if iteration_number >= 1 and iteration_number % tune_every == 0:\n",
    "                for param in self.params:\n",
    "                    acc = self.acceptance_rates[param.name]/iteration_number\n",
    "                    param.step_size = self.tune(param.step_size, acc)\n",
    "                    #print(f'Updating {param.name} to {param.step_size} due to acceptance rate {acc}')\n",
    "\n",
    "            self.iterate()\n",
    "    \n",
    "            if iteration_number >= burn_in and iteration_number % store_every == 0:\n",
    "                # for j in range(num_genes):\n",
    "                for param in self.params:\n",
    "                    if param.value.ndim > 1:\n",
    "                        self.samples[param.name].add(param.value.copy())\n",
    "                    else:\n",
    "                        self.samples[param.name].add(param.value)\n",
    "                self.samples['acc_rates'].append(list(self.acceptance_rates.values()))\n",
    "\n",
    "        for key in self.acceptance_rates:\n",
    "            self.acceptance_rates[key] /= T\n",
    "        rates = np.array(self.samples['acc_rates']).T/np.arange(1, T-burn_in+1, store_every)\n",
    "        self.samples['acc_rates'] = rates\n",
    "        f.value = T\n",
    "        print('----- Finished -----')\n",
    "\n",
    "    def iterate(self):\n",
    "        raise NotImplementedError('iterate() must be implemented')\n",
    "        \n",
    "    '''MH accept function'''\n",
    "    def is_accepted(self, new_log_prob, old_log_prob):\n",
    "        alpha = exp(new_log_prob - old_log_prob)\n",
    "        if tf.is_tensor(alpha):\n",
    "            alpha = alpha.numpy()\n",
    "        return not np.isnan(alpha) and random.random() < min(1, alpha)\n",
    "\n",
    "    def tune(self, scale, acc_rate):\n",
    "        \"\"\"\n",
    "        Tunes the scaling parameter for the proposal distribution\n",
    "        according to the acceptance rate over the last tune_interval:\n",
    "        Rate    Variance adaptation\n",
    "        ----    -------------------\n",
    "        <0.001        x 0.1\n",
    "        <0.05         x 0.5\n",
    "        <0.2          x 0.9\n",
    "        >0.5          x 1.1\n",
    "        >0.75         x 2\n",
    "        >0.95         x 10\n",
    "        \"\"\"\n",
    "        if acc_rate < 0.001:\n",
    "            return scale * 0.1\n",
    "        elif acc_rate < 0.05:\n",
    "            return scale * 0.5\n",
    "        elif acc_rate < 0.2:\n",
    "            return scale * 0.9\n",
    "        elif acc_rate > 0.95:\n",
    "            return scale * 10.0\n",
    "        elif acc_rate > 0.75:\n",
    "            return scale * 2.0\n",
    "        elif acc_rate > 0.5:\n",
    "            return scale * 1.1\n",
    "\n",
    "        return scale\n",
    "\n",
    "\n",
    "class TranscriptionLikelihood():\n",
    "    def predict_m(self, kbar, δbar, w, fbar, w_0):\n",
    "        # Take relevant parameters out of log-space\n",
    "        a_j, b_j, d_j, s_j = (np.exp(kbar[:, i]).reshape(-1, 1) for i in range(4))\n",
    "        δ = np.exp(δbar)\n",
    "        f_i = np.log(1+np.exp(fbar))\n",
    "    #     print('f_i', f_i)\n",
    "\n",
    "        # Calculate p_i vector\n",
    "        p_i = np.zeros(N_p) # TODO it seems the ODE translation model has params A, S see gpmtfComputeTFODE\n",
    "        Δ = τ[1]-τ[0]\n",
    "        sum_term = mult(exp(δ*τ), f_i)\n",
    "        p_i[1:] = 0.5*Δ*np.cumsum(sum_term[:-1] + sum_term[1:]) # Trapezoid rule\n",
    "#         try:\n",
    "        p_i = mult(exp(-δ*τ), p_i)\n",
    "#         except:\n",
    "#             print(exp(-δ*τ), p_i)\n",
    "    #     print('pi', p_i)\n",
    "\n",
    "        # Calculate m_pred\n",
    "        integrals = np.zeros((num_genes, N_p))\n",
    "        interactions = w[:, 0][:, None]*np.log(p_i+1e-100) + w_0\n",
    "        G = expit(interactions) # TF Activation Function (sigmoid)\n",
    "        sum_term = G * exp(d_j*τ)\n",
    "        integrals[:, 1:] = 0.5*Δ*np.cumsum(sum_term[:, :-1] + sum_term[:, 1:], axis=1) # Trapezoid rule\n",
    "        exp_dt = exp(-d_j*τ)\n",
    "        integrals = mult(exp_dt, integrals)\n",
    "        m_pred = b_j/d_j + mult((a_j-b_j/d_j), exp_dt) + s_j*integrals\n",
    "\n",
    "        return m_pred\n",
    "\n",
    "    def genes(self, params, δbar=None,\n",
    "                     fbar=None, \n",
    "                     kbar=None, \n",
    "                     w=None,\n",
    "                     σ2_m=None):\n",
    "        '''\n",
    "        Computes likelihood of the genes.\n",
    "        If any of the optional args are None, they are replaced by their current value in params.\n",
    "        '''\n",
    "        if δbar is None:\n",
    "            δbar = params.δbar.value\n",
    "        if fbar is None:\n",
    "            fbar = params.fbar.value\n",
    "        if kbar is None:\n",
    "            kbar = params.kbar.value\n",
    "        w = params.w.value if w is None else w\n",
    "        σ2_m = params.σ2_m.value if σ2_m is None else σ2_m\n",
    "\n",
    "        w_0 = 0 # TODO no hardcode this!\n",
    "        m_pred = self.predict_m(kbar, δbar, w, fbar, w_0)\n",
    "\n",
    "        log_lik = np.zeros(num_genes)\n",
    "        sq_diff = np.square(m_observed - m_pred[:, common_indices])\n",
    "        variance = σ2_m.reshape(-1, 1) + σ2 # add PUMA variance\n",
    "        log_lik = -0.5*np.log(2*np.pi*(variance)) - 0.5*sq_diff/variance\n",
    "        log_lik = np.sum(log_lik, axis=1)\n",
    "\n",
    "        return log_lik\n",
    "\n",
    "    def tfs(self, fbar, i=0): \n",
    "        '''\n",
    "        Computes log-likelihood of the transcription factors.\n",
    "        TODO this should be for the i-th TF\n",
    "        '''\n",
    "        f_pred = np.log(1+np.exp(fbar))\n",
    "        f_pred = np.atleast_2d(f_pred[common_indices])\n",
    "        sq_diff = np.square(f_observed[i] - f_pred[i])\n",
    "        log_lik = -0.5*sum(np.log(2*np.pi*σ2_f[i])) - 0.5*sum(sq_diff/σ2_f[i])\n",
    "        return log_lik\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f64 = np.float64\n",
    "class TranscriptionMCMC(MetropolisHastings):\n",
    "    def __init__(self):\n",
    "        self.likelihood = TranscriptionLikelihood()\n",
    "        # Adaptable variances\n",
    "        a = tf.constant(-0.5, dtype='float64')\n",
    "        b2 = tf.constant(2., dtype='float64')\n",
    "        self.h_f = 0.35*tf.ones(N_p, dtype='float64')\n",
    "\n",
    "        # Interaction weights\n",
    "        w_0 = Parameter('w_0', tfd.Normal(0, 2), np.zeros(num_genes), step_size=0.5*tf.ones(num_genes, dtype='float64'))\n",
    "        w_0.proposal_dist=lambda mu, j:tfd.Normal(mu, w_0.step_size[j])\n",
    "        w = Parameter('w', tfd.Normal(0, 2), 1*np.ones((num_genes, I)), step_size=0.5*tf.ones(num_genes, dtype='float64'))\n",
    "        w.proposal_dist=lambda mu, j:tfd.Normal(mu, w.step_size[j]) #) w_j) # At the moment this is the same as w_j0 (see pg.8)\n",
    "        # Latent function\n",
    "        fbar = Parameter('fbar', self.fbar_prior, 0.5*np.ones(N_p))\n",
    "\n",
    "        # GP hyperparameters\n",
    "        V = Parameter('V', tfd.InverseGamma(f64(0.01), f64(0.01)), f64(1), step_size=0.05)\n",
    "        V.proposal_dist=lambda v: tfd.TruncatedNormal(v, V.step_size, low=0, high=100) #v_i Fix to 1 if translation model is not used (pg.8)\n",
    "        L = Parameter('L', tfd.Uniform(f64(3.5), f64(144)), f64(4), step_size=0.05)\n",
    "        L.proposal_dist=lambda l2: tfd.TruncatedNormal(l2, L.step_size, low=0, high=100) #l2_i\n",
    "        self.t_dist = get_rbf_dist(τ, N_p)\n",
    "\n",
    "        # Translation kinetic parameters\n",
    "        δbar = Parameter('δbar', tfd.Normal(a, b2), f64(-0.3), step_size=0.3)\n",
    "        δbar.proposal_dist=lambda mu:tfd.Normal(mu, δbar.step_size)\n",
    "        # White noise for genes\n",
    "        σ2_m = Parameter('σ2_m', tfd.InverseGamma(f64(0.01), f64(0.01)), 1e-4*np.ones(num_genes), step_size=tf.constant(0.5, dtype='float64'))\n",
    "        σ2_m.proposal_dist=lambda mu: tfd.TruncatedNormal(mu, σ2_m.step_size, low=0, high=5)\n",
    "        # Transcription kinetic parameters\n",
    "        def constrain_kbar(kbar, gene):\n",
    "            '''Constrains a given row in kbar'''\n",
    "#             if gene == 3:\n",
    "#                 kbar[2] = np.log(0.8)\n",
    "#                 kbar[3] = np.log(1.0)\n",
    "            kbar[kbar < -10] = -10\n",
    "            kbar[kbar > 2] = 2\n",
    "            return kbar\n",
    "        kbar_initial = -0.1*np.float64(np.c_[\n",
    "            np.ones(num_genes), # a_j\n",
    "            np.ones(num_genes), # b_j\n",
    "            np.ones(num_genes), # d_j\n",
    "            np.ones(num_genes)  # s_j\n",
    "        ])\n",
    "        for j, k in enumerate(kbar_initial):\n",
    "            kbar_initial[j] = constrain_kbar(k, j)\n",
    "        kbar = Parameter('kbar',\n",
    "            tfd.Normal(a, b2), \n",
    "            kbar_initial,\n",
    "            constraint=constrain_kbar, step_size=0.25*tf.ones(4, dtype='float64'))\n",
    "        kbar.proposal_dist=lambda mu: tfd.MultivariateNormalDiag(mu, kbar.step_size)\n",
    "        params = namedtuple('parameters', ['fbar','δbar','kbar','σ2_m','w','w_0','L','V'])\n",
    "        super().__init__(params(fbar, δbar, kbar, σ2_m, w, w_0, L, V))\n",
    "        \n",
    "    def fbar_prior_params(self, v, l2):\n",
    "    #     print('vl2', v, l2)\n",
    "        jitter = tf.linalg.diag(1e-5 * np.ones(N_p))\n",
    "        K = mult(v, exp(-np.square(self.t_dist)/(2*l2))) + jitter\n",
    "        m = np.zeros(N_p)\n",
    "        return m, K\n",
    "\n",
    "    def fbar_prior(self, fbar, v, l2):\n",
    "        m, K = self.fbar_prior_params(v, l2)\n",
    "    \n",
    "        try:\n",
    "            return tfd.MultivariateNormalFullCovariance(m, K).log_prob(fbar)\n",
    "        except:\n",
    "            jitter = tf.linalg.diag(1e-4 * np.ones(N_p))\n",
    "            try:\n",
    "                return np.float64(tfd.MultivariateNormalFullCovariance(m, K+jitter).log_prob(fbar))\n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "\n",
    "    def iterate(self):\n",
    "        params = self.params\n",
    "        # Compute likelihood for comparison\n",
    "        old_m_likelihood = self.likelihood.genes(params)\n",
    "        \n",
    "        # Untransformed tf mRNA vectors F\n",
    "        fbar = params.fbar.value\n",
    "        for i in range(I):\n",
    "            # Gibbs step\n",
    "            z_i = tf.reshape(tfd.MultivariateNormalDiag(fbar, self.h_f).sample(), (1, -1))\n",
    "            # MH\n",
    "            m, K = self.fbar_prior_params(params.V.value, params.L.value)\n",
    "            invKsigmaK = tf.matmul(tf.linalg.inv(K+tf.linalg.diag(self.h_f)), K) # (C_i + hI)C_i\n",
    "            L = jitter_cholesky(K-tf.matmul(K, invKsigmaK))\n",
    "            c_mu = tf.matmul(z_i, invKsigmaK)\n",
    "            fstar = tf.matmul(tf.random.normal((1, L.shape[0]), dtype='float64'), L) + c_mu\n",
    "            fstar = tf.reshape(fstar, (-1, ))\n",
    "            new_m_likelihood = self.likelihood.genes(params, fbar=fstar)\n",
    "            new_prob = np.sum(new_m_likelihood) + self.likelihood.tfs(fstar)\n",
    "            old_prob = np.sum(old_m_likelihood) + self.likelihood.tfs(fbar)\n",
    "            if self.is_accepted(new_prob, old_prob):\n",
    "                params.fbar.value = fstar\n",
    "                old_m_likelihood = new_m_likelihood\n",
    "                self.acceptance_rates['fbar'] += 1/I\n",
    "\n",
    "\n",
    "        # Log of translation ODE degradation rates\n",
    "        δbar = params.δbar.value\n",
    "        for i in range(I):# TODO make for I > 1\n",
    "            # Proposal distribution\n",
    "            δstar = params.δbar.propose(δbar) # δstar is in log-space, i.e. δstar = δbar*\n",
    "            new_prob = np.sum(self.likelihood.genes(params, δbar=δstar)) + params.δbar.prior.log_prob(δstar)\n",
    "            old_prob = np.sum(old_m_likelihood) + params.δbar.prior.log_prob(δbar)\n",
    "#             print(δstar, params.δbar.prior.log_prob(δstar))\n",
    "#             print(new_prob, old_prob)\n",
    "            if self.is_accepted(new_prob, old_prob):\n",
    "                params.δbar.value = δstar\n",
    "                self.acceptance_rates['δbar'] += 1/I\n",
    "\n",
    "        # Log of transcription ODE kinetic params\n",
    "        kbar = params.kbar.value\n",
    "        kstar = kbar.copy()\n",
    "        for j in range(num_genes):\n",
    "            sample = params.kbar.propose(kstar[j])\n",
    "            sample = params.kbar.constrain(sample, j)\n",
    "\n",
    "            kstar[j] = sample\n",
    "            new_prob = self.likelihood.genes(params, kbar=kstar)[j] + sum(params.kbar.prior.log_prob(sample))\n",
    "            old_prob = old_m_likelihood[j] + sum(params.kbar.prior.log_prob(kbar[j]))\n",
    "            if self.is_accepted(new_prob, old_prob):\n",
    "                test = params.kbar.value\n",
    "                test[j]=sample\n",
    "                params.kbar.value = test\n",
    "                self.acceptance_rates['kbar'] += 1/num_genes\n",
    "            else:\n",
    "                kstar[j] = params.kbar.value[j]\n",
    "\n",
    "\n",
    "        # Interaction weights and biases (note: should work for I > 1)\n",
    "        w = params.w.value\n",
    "        w_0 = params.w_0.value\n",
    "        wstar = w.copy()\n",
    "        for j in range(num_genes):\n",
    "            sample_0 = params.w_0.propose(w_0[j], j)\n",
    "            sample = params.w.propose(wstar[j], j)\n",
    "            wstar[j] = sample\n",
    "            new_prob = self.likelihood.genes(params, w=wstar)[j] + sum(params.w.prior.log_prob(sample)) + params.w_0.prior.log_prob(sample_0)\n",
    "            old_prob = old_m_likelihood[j] + sum(params.w.prior.log_prob(w[j,:])) + params.w_0.prior.log_prob(w_0[j])\n",
    "            if self.is_accepted(new_prob, old_prob):\n",
    "                params.w.value[j] = sample\n",
    "                params.w_0.value[j] = sample_0\n",
    "                self.acceptance_rates['w'] += 1/num_genes\n",
    "                self.acceptance_rates['w_0'] += 1/num_genes\n",
    "            else:\n",
    "                wstar[j] = params.w.value[j]\n",
    "\n",
    "        # Noise variances\n",
    "        σ2_m = params.σ2_m.value\n",
    "        σ2_mstar = σ2_m.copy()\n",
    "        for j in range(num_genes):\n",
    "            sample = params.σ2_m.propose(σ2_m[j])\n",
    "            σ2_mstar[j] = sample\n",
    "            old_q = params.σ2_m.proposal_dist(σ2_mstar[j]).log_prob(σ2_m[j])\n",
    "            new_prob = self.likelihood.genes(params, σ2_m=σ2_mstar)[j] +params.σ2_m.prior.log_prob(σ2_mstar[j])\n",
    "            \n",
    "            new_q = params.σ2_m.proposal_dist(σ2_m[j]).log_prob(σ2_mstar[j])\n",
    "            old_prob = self.likelihood.genes(params, σ2_m=σ2_m)[j] + params.σ2_m.prior.log_prob(σ2_m[j])\n",
    "                \n",
    "            if self.is_accepted(new_prob + old_q, old_prob + new_q):\n",
    "                params.σ2_m.value[j] = sample\n",
    "                self.acceptance_rates['σ2_m'] += 1/num_genes\n",
    "            else:\n",
    "                σ2_mstar[j] = σ2_m[j]\n",
    "\n",
    "        # Length scales and variances of GP kernels\n",
    "        l2 = params.L.value\n",
    "        v = params.V.value\n",
    "        for i in range(I):\n",
    "            # Proposal distributions\n",
    "            Q_v = params.V.proposal_dist\n",
    "            Q_l = params.L.proposal_dist\n",
    "            vstar = params.V.propose(v)\n",
    "            l2star = params.L.propose(l2)\n",
    "            # Acceptance probabilities\n",
    "            new_fbar_prior = params.fbar.prior(params.fbar.value, vstar, l2star)\n",
    "            old_q = Q_v(vstar).log_prob(v) + Q_l(l2star).log_prob(l2) # Q(old|new)\n",
    "            new_prob = new_fbar_prior + params.V.prior.log_prob(vstar) + params.L.prior.log_prob(l2star)\n",
    "            \n",
    "            new_q = Q_v(v).log_prob(vstar) + Q_l(l2).log_prob(l2star) # Q(new|old)\n",
    "            old_prob = params.fbar.prior(params.fbar.value, v, l2) + params.V.prior.log_prob(v) + params.L.prior.log_prob(l2)\n",
    "            accepted = self.is_accepted(new_prob + old_q, old_prob + new_q)\n",
    "            if accepted:\n",
    "                params.V.value = vstar\n",
    "                params.L.value = l2star\n",
    "                self.acceptance_rates['V'] += 1/I\n",
    "                self.acceptance_rates['L'] += 1/I\n",
    "\n",
    "    def predict_m(self):\n",
    "        return self.likelihood.predict_m(self.params.kbar.value, \n",
    "                                         self.params.δbar.value, \n",
    "                                         self.params.w.value, \n",
    "                                         self.params.fbar.value, 0)\n",
    "\n",
    "transcription_model = TranscriptionMCMC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin MCMC\n",
    "T = 1000\n",
    "store_every = 1\n",
    "burn_in = 0\n",
    "report_every = 20\n",
    "\n",
    "transcription_model.sample(T, store_every, burn_in, report_every)\n",
    "\n",
    "print(transcription_model.acceptance_rates)\n",
    "\n",
    "samples = transcription_model.samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step size is standard dev, too small means it takes long time to reach high density areas. too long means we reject many of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## samples = transcription_model.samples\n",
    "plt.figure(figsize=(10,14))\n",
    "parameter_names = transcription_model.acceptance_rates.keys()\n",
    "acc_rates = samples['acc_rates']\n",
    "\n",
    "for i, name in enumerate(parameter_names):\n",
    "    plt.subplot(len(parameter_names), 3, i+1)\n",
    "    deltas = acc_rates[i]\n",
    "    plt.plot(deltas)\n",
    "    plt.title(name)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decay\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, param in enumerate(['δbar', 'L', 'V']):\n",
    "    ax = plt.subplot(331+i)\n",
    "    plt.plot(samples[param].get())\n",
    "    ax.set_title(param)\n",
    "#'σ', 'w']):\n",
    "\n",
    "plt.figure()\n",
    "for j in range(num_genes):\n",
    "    plt.plot(samples['w'].get()[:, j], label=df.index[j])\n",
    "plt.legend()\n",
    "plt.title('Interaction weights')\n",
    "\n",
    "plt.figure()\n",
    "for j in range(num_genes):\n",
    "    plt.plot(samples['w_0'].get()[:,j])\n",
    "plt.title('Interaction bias')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot transcription ODE kinetic params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 14))\n",
    "plt.title('Transcription ODE kinetic parameters')\n",
    "labels = ['a', 'b', 'd', 's']\n",
    "for j in range(num_genes):\n",
    "    ax = plt.subplot(num_genes, 2, j+1)\n",
    "    k_param = samples['kbar'].get()[:, j]\n",
    "#     print(k_param)\n",
    "    \n",
    "    for k in range(4):\n",
    "        plt.plot(k_param[-20000:, k], label=labels[k])\n",
    "    plt.axhline(np.mean(k_param[-20000:, 3]))\n",
    "    plt.legend()\n",
    "    ax.set_title(f'Gene {j}')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_kinetics(kbar):\n",
    "    plt.figure(figsize=(14, 14))\n",
    "    k_latest = np.exp(np.mean(kbar[-100:], axis=0))\n",
    "    print(k_latest)\n",
    "    B = k_latest[:,1]\n",
    "    D = k_latest[:,2]\n",
    "    S = k_latest[:,3]\n",
    "    B_barenco = np.array([2.6, 1.5, 0.5, 0.2, 1.35])# From Martino paper ... but don't know the scale\n",
    "    B_barenco = B_barenco/np.mean(B_barenco)*np.mean(B)# do a rough rescaling so that the scales match.\n",
    "    S_barenco = np.array([3, 0.8, 0.7, 1.8, 0.7])/1.8\n",
    "    S_barenco = S_barenco/np.mean(S_barenco)*np.mean(S)# do a rough rescaling so that the scales match.\n",
    "    D_barenco = np.array([1.2, 1.6, 1.75, 3.2, 2.3])*0.8/3.2\n",
    "    D_barenco = D_barenco/np.mean(D_barenco)*np.mean(D)# do a rough rescaling so that the scales match.\n",
    "\n",
    "\n",
    "    data = [B, S, D]\n",
    "    barenco_data = [B_barenco, S_barenco, D_barenco]\n",
    "    labels = ['Basal rates', 'Sensitivities', 'Decay rates']\n",
    "\n",
    "    plotnum = 331\n",
    "    for A, B, label in zip(data, barenco_data, labels):\n",
    "        plt.subplot(plotnum)\n",
    "        plotnum+=1\n",
    "        plt.bar(np.arange(5)-0.2, A, width=0.4, tick_label=df.index[:-1])\n",
    "        plt.bar(np.arange(5)+0.2, B, width=0.4, color='blue', align='center')\n",
    "\n",
    "        plt.title(label)\n",
    "\n",
    "kbar = samples['kbar'].get()\n",
    "plot_kinetics(kbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot genes\n",
    "plt.figure(figsize=(14, 17))\n",
    "m_pred = transcription_model.predict_m()\n",
    "print(2000000*np.sqrt(Y_var[1]))\n",
    "for j in range(num_genes):\n",
    "    ax = plt.subplot(531+j)\n",
    "    plt.title(df.index[j])\n",
    "    plt.scatter([n*10+n for n in range(7)], m_observed[j], marker='x')\n",
    "    plt.errorbar([n*10+n for n in range(7)], Y[j], 2*np.sqrt(Y_var[j]), fmt='none', capsize=5)\n",
    "    plt.plot(m_pred[j,:], color='grey')\n",
    "    plt.xticks(np.arange(N_p)[common_indices])\n",
    "    ax.set_xticklabels(np.arange(N_m)*2)\n",
    "    plt.xlabel('Time (h)');\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.title('Noise variances')\n",
    "for i, j in enumerate(range(num_genes)):\n",
    "    ax = plt.subplot(num_genes, num_genes-2, i+1)\n",
    "    plt.title(df.index[j])\n",
    "    plt.plot(samples['σ2_m'].get()[:,j])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_barenco_data(f):\n",
    "    scale_pred = np.sqrt(np.var(f))\n",
    "    barencof = np.array([[0.0, 200.52011, 355.5216125, 205.7574913, 135.0911372, 145.1080997, 130.7046969],\n",
    "                         [0.0, 184.0994134, 308.47592, 232.1775328, 153.6595161, 85.7272235, 168.0910562],\n",
    "                         [0.0, 230.2262511, 337.5994811, 276.941654, 164.5044287, 127.8653452, 173.6112139]])\n",
    "\n",
    "    barencof = barencof[0]/(np.sqrt(np.var(barencof[0])))*scale_pred\n",
    "    measured_p53 = df[df.index.isin(['211300_s_at', '201746_at'])]\n",
    "    measured_p53 = measured_p53.mean(0)\n",
    "    measured_p53 = measured_p53*scale_pred\n",
    "    \n",
    "    return barencof, measured_p53\n",
    "\n",
    "def plot_f(f):\n",
    "    fig = plt.figure(figsize=(13, 7))\n",
    "\n",
    "    barencof = scaled_barenco_data(f)\n",
    "    lb = len(barencof)\n",
    "    plt.plot(np.arange(N_p), f, color='grey')\n",
    "    plt.scatter(np.arange(0, N_p)[common_indices], barencof, marker='x')\n",
    "    plt.xticks(np.arange(N_p)[common_indices])\n",
    "    fig.axes[0].set_xticklabels(np.arange(N_m)*2)\n",
    "    plt.xlabel('Time (h)')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 7))\n",
    "f_samples = np.log(1+np.exp(np.array(samples['fbar'].get()[-50:])))\n",
    "\n",
    "bounds = arviz.hpd(f_samples, credible_interval=0.95)\n",
    "\n",
    "for i in range(1,20):\n",
    "    f_i = f_samples[-i]\n",
    "#     plt.plot(f_i)\n",
    "#     f_i[0] = 0\n",
    "    plt.plot(f_i, c='blue', alpha=0.5)\n",
    "\n",
    "    \n",
    "barenco_f, _ = scaled_barenco_data(f_samples[-1])\n",
    "plt.scatter(np.arange(N_p)[common_indices], barenco_f, marker='x', s=60, linewidth=3, label='Barenco')\n",
    "plt.scatter(np.arange(N_p)[common_indices], f_observed[0], marker='x', s=60, linewidth=3, label='Observed')\n",
    "plt.errorbar(np.arange(N_p)[common_indices], f_observed[0], 2*np.sqrt(σ2_f[0]), fmt='none', capsize=5, color='blue')\n",
    "\n",
    "plt.fill_between(np.arange(N_p), bounds[:, 0], bounds[:, 1], color='grey', alpha=0.5)\n",
    "plt.xticks(np.arange(N_p)[common_indices])\n",
    "fig.axes[0].set_xticklabels(np.arange(N_m)*2)\n",
    "plt.xlabel('Time (h)')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit374b75da0e1b40de8b7922d3f142c01d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}