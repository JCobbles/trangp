{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis Hastings Custom HMC MC Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from reggae.data_loaders import load_barenco_puma, load_3day_dros, DataHolder, scaled_barenco_data\n",
    "from reggae.mcmc import create_chains, MetropolisHastings, Parameter\n",
    "from reggae.utilities import get_rbf_dist, discretise\n",
    "from reggae.plot import plotters\n",
    "from reggae.models import TranscriptionLikelihood, Options\n",
    "from reggae.models.results import GenericResults\n",
    "from reggae.models.kernels import MixedKernel, FKernel, KbarKernel\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "f64 = np.float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LogisticNormal(tfp.distributions.LogitNormal):\n",
    "    def __init__(self, a, b, loc=f64(0), scale=f64(1.78)):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        super().__init__(loc, scale)\n",
    "    def log_prob(self, x):\n",
    "        x = (x-self.a)/(self.b-self.a)\n",
    "        return super().log_prob(x)\n",
    "    \n",
    "l = LogisticNormal(-2, 2)\n",
    "\n",
    "xs = np.linspace(-2, 2, 100)\n",
    "ll = np.nan_to_num([l.log_prob(x).numpy() for x in xs], 0)\n",
    "print(ll)\n",
    "plt.plot(xs, np.exp(ll))\n",
    "# plt.ylim(0, 0.5)\n",
    "plt.ylim(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df, genes, genes_se, m_observed, f_observed, σ2_m_pre, σ2_f_pre, t = load_barenco_puma()\n",
    "m_observed, f_observed, σ2_m_pre, σ2_f_pre, t = load_barenco_puma()\n",
    "\n",
    "# m_observed, f_observed, t = load_3day_dros()\n",
    "\n",
    "replicate = 0\n",
    "\n",
    "m_df, m_observed = m_observed \n",
    "f_df, f_observed = f_observed\n",
    "\n",
    "# Shape of m_observed = (replicates, genes, times)\n",
    "m_observed = m_observed[replicate]\n",
    "f_observed = np.atleast_2d(f_observed[replicate])\n",
    "σ2_m_pre = σ2_m_pre[0]\n",
    "σ2_f_pre = σ2_f_pre[0]\n",
    "\n",
    "num_genes = m_observed.shape[0]\n",
    "τ, common_indices = discretise(t)\n",
    "N_p = τ.shape[0]\n",
    "N_m = m_observed.shape[1]\n",
    "\n",
    "data = (m_observed, f_observed)\n",
    "noise_data = (σ2_m_pre, σ2_f_pre)\n",
    "time = (t, τ, tf.constant(common_indices))\n",
    "\n",
    "data = DataHolder(data, noise_data, time)\n",
    "N_p = τ.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options(preprocessing_variance=True, tf_mrna_present=True)\n",
    "lik = TranscriptionLikelihood(data, opt)\n",
    "\n",
    "print(max(np.var(data.f_obs, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GibbsKernel(tfp.mcmc.TransitionKernel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "TupleParams_pre = collections.namedtuple('TupleParams_pre', [\n",
    "    'fbar','δbar','kbar','σ2_m','w','w_0','L','V','σ2_f'\n",
    "])\n",
    "TupleParams = collections.namedtuple('TupleParams', [\n",
    "    'fbar','δbar','kbar','σ2_m','w','w_0','L','V'\n",
    "])\n",
    "\n",
    "class TranscriptionCustom():\n",
    "    '''\n",
    "    Data is a tuple (m, f) of shapes (num, time)\n",
    "    time is a tuple (t, τ, common_indices)\n",
    "    '''\n",
    "    def __init__(self, data: DataHolder, options: Options):\n",
    "        self.data = data\n",
    "        min_dist = min(data.t[1:]-data.t[:-1])\n",
    "        self.N_p = data.τ.shape[0]\n",
    "        self.N_m = data.t.shape[0]      # Number of observations\n",
    "\n",
    "        self.num_tfs = data.f_obs.shape[0] # Number of TFs\n",
    "        self.num_genes = data.m_obs.shape[0]\n",
    "\n",
    "        self.likelihood = TranscriptionLikelihood(data, options)\n",
    "        self.options = options\n",
    "        # Adaptable variances\n",
    "        a = tf.constant(-0.5, dtype='float64')\n",
    "        b2 = tf.constant(2., dtype='float64')\n",
    "        self.state_indices = {\n",
    "            'δbar': 0,\n",
    "            'kbar': 1,\n",
    "            'fbar': 2, \n",
    "            'rbf_params': 3,\n",
    "            'σ2_m': 4,\n",
    "            'w': 5,\n",
    "        }\n",
    "        logistic_step_size = 0.01\n",
    "\n",
    "        # Interaction weights\n",
    "        def w_log_prob(all_states):\n",
    "            def w_log_prob_fn(wstar, w_0star):\n",
    "                new_prob = tf.reduce_sum(self.likelihood.genes(\n",
    "                    all_states=all_states, \n",
    "                    state_indices=self.state_indices,\n",
    "                     w=wstar))\n",
    "                new_prob += tf.reduce_sum(self.params.w.prior.log_prob(wstar)) \n",
    "                new_prob += tf.reduce_sum(self.params.w_0.prior.log_prob(w_0star))\n",
    "                return tf.reduce_sum(new_prob)\n",
    "            return w_log_prob_fn\n",
    "        w = Parameter('w', tfd.Normal(f64(0), f64(2)), \n",
    "                      1*tf.ones((self.num_genes, self.num_tfs), dtype='float64'), \n",
    "                      step_size=0.04, hmc_log_prob=w_log_prob, requires_all_states=True)\n",
    "        w_0 = Parameter('w_0', tfd.Normal(f64(0), f64(2)), tf.zeros(self.num_genes, dtype='float64'))\n",
    "\n",
    "        # Latent function\n",
    "        fbar_kernel = FKernel(self.likelihood, \n",
    "                              self.fbar_prior_params, \n",
    "                              self.num_tfs, self.num_genes, \n",
    "                              self.options.tf_mrna_present, \n",
    "                              self.state_indices,\n",
    "                              0.3*tf.ones(N_p, dtype='float64'))\n",
    "        fbar = Parameter('fbar', self.fbar_prior, 0.5*np.ones((self.num_tfs, self.N_p)), kernel=fbar_kernel, requires_all_states=False)\n",
    "\n",
    "        # GP hyperparameters\n",
    "        def rbf_params_log_prob(all_states):\n",
    "            def rbf_params_log_prob(vstar, l2star):\n",
    "#                 tf.print(all_states[fbar_state_index])\n",
    "                new_prob = self.params.fbar.prior(all_states[self.state_indices['fbar']], vstar, l2star)\n",
    "                new_prob += self.params.V.prior.log_prob(vstar)\n",
    "                new_prob += self.params.L.prior.log_prob(l2star)\n",
    "#                 tf.print(new_prob)\n",
    "                return tf.reduce_sum(new_prob)\n",
    "            return rbf_params_log_prob\n",
    "\n",
    "        V = Parameter('V', LogisticNormal(f64(1e-4), f64(max(np.var(data.f_obs, axis=1)))), tf.constant(f64(0.5)), step_size=logistic_step_size, \n",
    "                      fixed=not options.tf_mrna_present, hmc_log_prob=rbf_params_log_prob, requires_all_states=True)\n",
    "        L = Parameter('L', LogisticNormal(f64(min_dist**2-0.2), f64(data.t[-1]**2)), tf.constant(f64(4)))\n",
    "\n",
    "        self.t_dist = get_rbf_dist(data.τ, self.N_p)\n",
    "\n",
    "        # Translation kinetic parameters\n",
    "        def δbar_log_prob(all_states):\n",
    "            def δbar_log_prob_fn(state):\n",
    "                new_prob = tf.reduce_sum(self.likelihood.genes(\n",
    "                    all_states=all_states, \n",
    "                    state_indices=self.state_indices,\n",
    "                    δbar=state\n",
    "                ))\n",
    "                new_prob += self.params.δbar.prior.log_prob(state)\n",
    "                return new_prob\n",
    "            return δbar_log_prob_fn\n",
    "        δbar = Parameter('δbar', LogisticNormal(-2, 2), f64(-0.3), step_size=logistic_step_size, \n",
    "        δbar = Parameter('δbar', tfd.Normal(a, b2), tf.reshape(f64(-0.3), (self.num_tfs,)), step_size=0.1, \n",
    "                         hmc_log_prob=δbar_log_prob, requires_all_states=True)\n",
    "\n",
    "        # White noise for genes\n",
    "        def σ2_m_log_prob(all_states):\n",
    "            def σ2_m_log_prob_fn(σ2_mstar):\n",
    "#                 tf.print('star:',σ2_mstar)\n",
    "                new_prob = self.likelihood.genes(\n",
    "                    all_states=all_states, \n",
    "                    state_indices=self.state_indices,\n",
    "                    σ2_m=σ2_mstar \n",
    "                ) + self.params.σ2_m.prior.log_prob(σ2_mstar)\n",
    "#                 tf.print('prob', tf.reduce_sum(new_prob))\n",
    "                return tf.reduce_sum(new_prob)                \n",
    "            return σ2_m_log_prob_fn\n",
    "        σ2_m = Parameter('σ2_m', LogisticNormal(f64(1e-5), f64(max(np.var(data.f_obs, axis=1)))), 1e-4*tf.ones(self.num_genes, dtype='float64'), \n",
    "                         hmc_log_prob=σ2_m_log_prob, requires_all_states=True, step_size=logistic_step_size)\n",
    "        # Transcription kinetic parameters\n",
    "        def constrain_kbar(kbar, gene):\n",
    "            '''Constrains a given row in kbar'''\n",
    "#             if gene == 3:\n",
    "#                 kbar[2] = np.log(0.8)\n",
    "#                 kbar[3] = np.log(1.0)\n",
    "            kbar[kbar < -10] = -10\n",
    "            kbar[kbar > 3] = 3\n",
    "            return kbar\n",
    "        kbar_initial = -0.1*np.float64(np.c_[\n",
    "            np.ones(self.num_genes), # a_j\n",
    "            np.ones(self.num_genes), # b_j\n",
    "            np.ones(self.num_genes), # d_j\n",
    "            np.ones(self.num_genes)  # s_j\n",
    "        ])\n",
    "        def kbar_log_prob(all_states):\n",
    "            def kbar_log_prob_fn(kstar):\n",
    "#                 tf.print(kstar)\n",
    "                new_prob = self.likelihood.genes(\n",
    "                    all_states=all_states, \n",
    "                    state_indices=self.state_indices,\n",
    "                    kbar=kstar, \n",
    "                ) + tf.reduce_sum(self.params.kbar.prior.log_prob(kstar))\n",
    "#                 tf.print(new_prob)\n",
    "                return tf.reduce_sum(new_prob)\n",
    "            return kbar_log_prob_fn\n",
    "        for j, k in enumerate(kbar_initial):\n",
    "            kbar_initial[j] = constrain_kbar(k, j)\n",
    "        kbar = Parameter('kbar', LogisticNormal(-2, 2), \n",
    "                         kbar_initial,\n",
    "                         hmc_log_prob=kbar_log_prob,\n",
    "                         constraint=constrain_kbar, step_size=logistic_step_size, requires_all_states=True)\n",
    "        \n",
    "        if not options.preprocessing_variance:\n",
    "            σ2_f = Parameter('σ2_f', tfd.InverseGamma(f64(0.01), f64(0.01)), 1e-4*np.ones(self.num_tfs), step_size=tf.constant(0.5, dtype='float64'))\n",
    "            self.params = TupleParams_pre(fbar, δbar, kbar, σ2_m, w, w_0, L, V, σ2_f)\n",
    "        else:\n",
    "            self.params = TupleParams(fbar, δbar, kbar, σ2_m, w, w_0, L, V)\n",
    "            \n",
    "    def fbar_prior_params(self, v, l2):\n",
    "    #     print('vl2', v, l2)\n",
    "        jitter = tf.linalg.diag(1e-10 * tf.ones(self.N_p, dtype='float64'))\n",
    "        K = tfm.multiply(v, tfm.exp(-tfm.square(self.t_dist)/(2*l2))) + jitter\n",
    "        m = np.zeros((self.num_tfs, self.N_p))\n",
    "        return m, K\n",
    "\n",
    "    def fbar_prior(self, fbar, v, l2):\n",
    "#         tf.print(v, l2)\n",
    "        m, K = self.fbar_prior_params(v, l2)\n",
    "        try:\n",
    "            return tfd.MultivariateNormalTriL(loc=m, scale_tril=tf.linalg.cholesky(K)).log_prob(fbar)\n",
    "        except:\n",
    "            jitter = tf.linalg.diag(1e-4 *tf.ones(self.N_p))\n",
    "            try:\n",
    "                return np.float64(tfd.MultivariateNormalFullCovariance(m, K+jitter).log_prob(fbar))\n",
    "            except:\n",
    "                tf.print(\"error\")\n",
    "                return tf.constant(-np.inf)\n",
    "\n",
    "\n",
    "    def sample(self, T=2000, store_every=10, burn_in=1000, report_every=100):\n",
    "        print('----- Sampling Begins -----')\n",
    "        \n",
    "        f = IntProgress(description='Running', min=0, max=T) # instantiate the bar\n",
    "        display(f)\n",
    "        params = self.params\n",
    "        progbar = tf.keras.utils.Progbar(\n",
    "            100, width=30, verbose=1, interval=0.05, stateful_metrics=None,\n",
    "            unit_name='step'\n",
    "        )\n",
    "\n",
    "        print(params)\n",
    "        active_params = [\n",
    "            params.δbar,\n",
    "            params.kbar,\n",
    "            params.fbar,\n",
    "            params.V,\n",
    "            params.σ2_m,\n",
    "            #params.w,\n",
    "        ]\n",
    "        kernels = [param.kernel for param in active_params]\n",
    "#         if self.options.tf_mrna_present:\n",
    "        send_all_states = [param.requires_all_states for param in active_params]\n",
    "\n",
    "        current_state = [\n",
    "            params.δbar.value, \n",
    "            params.kbar.value, \n",
    "            params.fbar.value, \n",
    "            [params.V.value, params.L.value],\n",
    "            params.σ2_m.value,\n",
    "            #[params.w.value, params.w_0.value]\n",
    "        ]\n",
    "        mixed_kern = MixedKernel(kernels, send_all_states)\n",
    "        \n",
    "        print(self.fbar_prior(params.fbar.value, params.V.value, params.L.value))\n",
    "        def trace_fn(a, previous_kernel_results):\n",
    "            if hasattr(previous_kernel_results.inner_results[0], 'is_accepted'):\n",
    "                return previous_kernel_results.inner_results[0].is_accepted\n",
    "            return previous_kernel_results.inner_results[0].inner_results.is_accepted\n",
    "\n",
    "        # Run the chain (with burn-in).\n",
    "        @tf.function\n",
    "        def run_chain():\n",
    "            # Run the chain (with burn-in).\n",
    "            samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "                  num_results=T,\n",
    "                  num_burnin_steps=burn_in,\n",
    "                  current_state=current_state,\n",
    "                  kernel=mixed_kern,\n",
    "                  trace_fn=trace_fn)\n",
    "\n",
    "            return samples, is_accepted\n",
    "\n",
    "        samples, is_accepted = run_chain()\n",
    "        \n",
    "        self.is_accepted = is_accepted\n",
    "        self.samples = samples\n",
    "        \n",
    "        is_accepted = tf.reduce_mean(tf.cast(is_accepted, dtype=tf.float32))\n",
    "\n",
    "        # for key in self.acceptance_rates:\n",
    "        #     self.acceptance_rates[key] /= T\n",
    "        # rates = np.array(self.samples['acc_rates']).T/np.arange(1, T-burn_in+1, store_every)\n",
    "        # self.samples['acc_rates'] = rates\n",
    "        f.value = T\n",
    "        print('----- Finished -----')\n",
    "        return samples, is_accepted\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "store_every = 1\n",
    "burn_in = 0\n",
    "report_every = 20\n",
    "num_chains = 4\n",
    "tune_every = 50\n",
    "\n",
    "model = TranscriptionCustom(data, opt)\n",
    "\n",
    "samples, is_accepted = model.sample(T=100, burn_in=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbar = model.samples[model.state_indices['kbar']]\n",
    "δbar = model.samples[model.state_indices['δbar']]\n",
    "fbar = model.samples[model.state_indices['fbar']]\n",
    "σ2_m = model.samples[model.state_indices['σ2_m']]\n",
    "rbf_params = model.samples[model.state_indices['rbf_params']]\n",
    "# w = model.samples[model.state_indices['w']][0]\n",
    "# w_0 = model.samples[model.state_indices['w']][1]\n",
    "\n",
    "w = [1*tf.ones((num_genes, 1), dtype='float64')] # TODO\n",
    "w_0 = [tf.zeros(num_genes, dtype='float64')] # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barenco = True\n",
    "plt.plot(fbar[:, 0])\n",
    "fig = plt.figure(figsize=(13, 7))\n",
    "f_samples = np.log(1+np.exp(fbar[-100:, 0,:]))\n",
    "print(f_samples.shape)\n",
    "if 'σ2_f' in model.params._fields:\n",
    "    σ2_f = model.params.σ2_f.value\n",
    "    plt.errorbar(τ[common_indices], f_observed[0], 2*np.sqrt(σ2_f[0]), \n",
    "                 fmt='none', capsize=5, color='blue')\n",
    "else:\n",
    "    σ2_f = σ2_f_pre\n",
    "    \n",
    "bounds = arviz.hpd(f_samples, credible_interval=0.95)\n",
    "for i in range(1,80):\n",
    "    f_i = f_samples[-i]\n",
    "#     plt.plot(f_i)\n",
    "#     f_i[0] = 0\n",
    "    kwargs = {}\n",
    "    if i == 1:\n",
    "        kwargs = {'label':'Samples'}\n",
    "    plt.plot(τ, f_i, c='blue', alpha=0.5, **kwargs)\n",
    "\n",
    "# if plot_barenco:\n",
    "#     barenco_f, _ = scaled_barenco_data(np.mean(f_samples[-10:], axis=0))\n",
    "#     plt.scatter(τ[common_indices], barenco_f, marker='x', s=60, linewidth=3, label='Barenco et al.')\n",
    "\n",
    "plt.scatter(τ[common_indices], f_observed[0], marker='x', s=70, linewidth=4, label='Observed')\n",
    "\n",
    "plt.fill_between(τ, bounds[:, 0], bounds[:, 1], color='grey', alpha=0.3, label='95% credibility interval')\n",
    "plt.xticks(t)\n",
    "fig.axes[0].set_xticklabels(t)\n",
    "plt.ylim((-1,5))\n",
    "plt.xlabel('Time (h)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot genes\n",
    "plt.figure(figsize=(14, 17))\n",
    "\n",
    "m_preds = list()\n",
    "for i in range(1, 20):\n",
    "    m_preds.append(model.likelihood.predict_m(kbar[-i], δbar[-i], w[-1], fbar[-i], w_0[-1])) #todo w[-1]\n",
    "m_preds = np.array(m_preds)\n",
    "\n",
    "for j in range(num_genes):\n",
    "    ax = plt.subplot(531+j)\n",
    "    plt.title(m_df.index[j])\n",
    "    plt.scatter(common_indices, m_observed[j], marker='x', label='Observed')\n",
    "    # plt.errorbar([n*10+n for n in range(7)], Y[j], 2*np.sqrt(Y_var[j]), fmt='none', capsize=5)\n",
    "    for i in range(1, 20):\n",
    "        plt.plot(m_preds[-i][j,:], color='grey', alpha=0.5)\n",
    "        \n",
    "    # HPD:\n",
    "    bounds = arviz.hpd(m_preds[:, j,:], credible_interval=0.95)\n",
    "    plt.fill_between(np.arange(N_p), bounds[:, 0], bounds[:, 1], color='grey', alpha=0.3, label='95% credibility interval')\n",
    "\n",
    "    plt.xticks(np.arange(N_p)[common_indices])\n",
    "    ax.set_xticklabels(np.arange(t[-1]))\n",
    "    plt.xlabel('Time (h)')\n",
    "    plt.legend()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decay\n",
    "plt.figure(figsize=(10, 8))\n",
    "i = 0\n",
    "for name, param in (zip(['δbar', 'L', 'V'], [δbar, *rbf_params])):\n",
    "    ax = plt.subplot(331+i)\n",
    "    plt.plot(param)\n",
    "    ax.set_title(name)\n",
    "    i+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot transcription ODE kinetic params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotters.plot_kinetics_convergence(kbar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotters.plot_kinetics(m_df, kbar, plot_barenco, num_avg=20)\n",
    "\n",
    "plt.figure()\n",
    "num_genes = kbar.shape[1]\n",
    "k_latest = np.exp(np.mean(kbar[-100:], axis=0))\n",
    "print(k_latest)\n",
    "B = k_latest[:,1]\n",
    "D = k_latest[:,2]\n",
    "S = k_latest[:,3]\n",
    "\n",
    "plt.bar(np.arange(num_genes)-0.2, B, width=0.2, tick_label=m_df.index, label='Basal rate')\n",
    "plt.bar(np.arange(num_genes), D, width=0.2, tick_label=m_df.index, label='Sensitivity')\n",
    "plt.bar(np.arange(num_genes)+0.2, S, width=0.2, tick_label=m_df.index, label='Decay rate')\n",
    "plt.yscale('log')\n",
    "plt.title('Mechanistic Parameters')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for j in range(num_genes):\n",
    "    plt.plot(w[:, j], label=m_df.index[j])\n",
    "plt.legend()\n",
    "plt.title('Interaction weights')\n",
    "\n",
    "plt.figure()\n",
    "for j in range(num_genes):\n",
    "    plt.plot(w_0[:,j])\n",
    "plt.title('Interaction bias')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.title('Noise variances')\n",
    "for i, j in enumerate(range(num_genes)):\n",
    "    ax = plt.subplot(num_genes, num_genes-2, i+1)\n",
    "    plt.title(m_df.index[j])\n",
    "    plt.plot(σ2_m[:,j])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TranscriptionHMC(MetropolisHastings):\n",
    "\n",
    "    def iterate(self):\n",
    "        trace_fn = lambda _, pkr: pkr.is_accepted\n",
    "        params = self.params\n",
    "        # Compute likelihood for comparison\n",
    "        old_m_likelihood, sq_diff_m  = self.likelihood.genes(params, return_sq_diff=True)\n",
    "        old_f_likelihood = 0\n",
    "        if self.options.tf_mrna_present:\n",
    "            old_f_likelihood, sq_diff_f  = self.likelihood.tfs(params, params.fbar.value, return_sq_diff=True)\n",
    "\n",
    "        # Noise variances\n",
    "        if self.options.preprocessing_variance:\n",
    "        else: # Use Gibbs sampling\n",
    "            # Prior parameters\n",
    "            α = params.σ2_m.prior.concentration\n",
    "            β = params.σ2_m.prior.scale\n",
    "            # Conditional posterior of inv gamma parameters:\n",
    "            α_post = α + 0.5*self.N_m\n",
    "            β_post = β + 0.5*np.sum(sq_diff_m)\n",
    "            # print(α.shape, sq_diff.shape)\n",
    "            # print('val', β_post.shape, params.σ2_m.value)\n",
    "            params.σ2_m.value = np.repeat(tfd.InverseGamma(α_post, β_post).sample(), self.num_genes)\n",
    "            self.acceptance_rates['σ2_m'] += 1\n",
    "            \n",
    "            if self.options.tf_mrna_present: # (Step 5)\n",
    "                # Prior parameters\n",
    "                α = params.σ2_f.prior.concentration\n",
    "                β = params.σ2_f.prior.scale\n",
    "                # Conditional posterior of inv gamma parameters:\n",
    "                α_post = α + 0.5*self.N_m\n",
    "                β_post = β + 0.5*np.sum(sq_diff_f)\n",
    "                # print(α.shape, sq_diff.shape)\n",
    "                # print('val', β_post.shape, params.σ2_m.value)\n",
    "                params.σ2_f.value = np.repeat(tfd.InverseGamma(α_post, β_post).sample(), self.num_tfs)\n",
    "                self.acceptance_rates['σ2_f'] += 1\n",
    "\n",
    "            # print('val', params.σ2_m.value)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialise_from_state(args, state):\n",
    "        model = TranscriptionMCMC(*args)\n",
    "        model.acceptance_rates = state.acceptance_rates\n",
    "        model.samples = state.samples\n",
    "        return model\n",
    "\n",
    "    def predict_m(self, kbar, δbar, w, fbar, w_0):\n",
    "        return self.likelihood.predict_m(kbar, δbar, w, fbar, w_0)\n",
    "\n",
    "    def predict_m_with_current(self):\n",
    "        return self.likelihood.predict_m(self.params.kbar.value, \n",
    "                                         self.params.δbar.value, \n",
    "                                         self.params.w.value, \n",
    "                                         self.params.fbar.value,\n",
    "                                         self.params.w_0.value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chains\n",
    "job = create_chains(\n",
    "    transcription.TranscriptionMCMC, \n",
    "    [data, opt], \n",
    "    {\n",
    "        'T': T, \n",
    "        'store_every': store_every, \n",
    "        'burn_in': burn_in,\n",
    "        'report_every': report_every,\n",
    "        'tune_every':tune_every\n",
    "    }, \n",
    "    num_chains=num_chains)\n",
    "\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = job[0].acceptance_rates.keys()\n",
    "\n",
    "variables = {key : np.empty((0, T, *job[0].samples[key].get().shape[1:])) for key in keys}\n",
    "\n",
    "for res in job:\n",
    "    for key in keys:\n",
    "        variables[key] = np.append(variables[key], np.expand_dims(res.samples[key].get(), 0), axis=0)\n",
    "\n",
    "plt.plot(variables['L'][:,-100:].T)\n",
    "\n",
    "mixes = {key: arviz.convert_to_inference_data(variables[key]) for key in keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rhat\n",
    "Rhat is the ratio of posterior variance and within-chain variance. If the ratio exceeds 1.1 then we consider the chains have not mixed well. As the between-chain variance tends to the within-chain then R tends to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rhat = arviz.rhat(mixes['fbar'])\n",
    "\n",
    "Rhats = np.array([np.mean(arviz.rhat(mixes[key]).x.values) for key in keys])\n",
    "\n",
    "rhat_df = pd.DataFrame([[*Rhats], [*(Rhats < 1.1)]], columns=keys)\n",
    "\n",
    "display(rhat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank plots\n",
    "\n",
    "Rank plots are histograms of the ranked posterior draws (ranked over all\n",
    "    chains) plotted separately for each chain.\n",
    "    If all of the chains are targeting the same posterior, we expect the ranks in each chain to be\n",
    "    uniform, whereas if one chain has a different location or scale parameter, this will be\n",
    "    reflected in the deviation from uniformity. If rank plots of all chains look similar, this\n",
    "    indicates good mixing of the chains.\n",
    "\n",
    "Rank-normalization, folding, and localization: An improved R-hat\n",
    "    for assessing convergence of MCMC. arXiv preprint https://arxiv.org/abs/1903.08008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_rank(L_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effective sample sizes\n",
    "\n",
    "Plot quantile, local or evolution of effective sample sizes (ESS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_ess(L_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte-Carlo Standard Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_mcse(L_mix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Plot\n",
    "Plot parallel coordinates plot showing posterior points with and without divergences.\n",
    "\n",
    "Described by https://arxiv.org/abs/1709.01449, suggested by Ari Hartikainen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_parallel(azl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step size is standard dev, too small means it takes long time to reach high density areas. too long means we reject many of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = job[0].samples\n",
    "acceptance_rates = job[0].acceptance_rates\n",
    "\n",
    "model = transcription.TranscriptionMCMC.initialise_from_state([data, opt], job[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = transcription_model.samples\n",
    "acceptance_rates = model.acceptance_rates\n",
    "plt.figure(figsize=(10,14))\n",
    "parameter_names = acceptance_rates.keys()\n",
    "acc_rates = samples['acc_rates']\n",
    "\n",
    "for i, name in enumerate(parameter_names):\n",
    "    plt.subplot(len(parameter_names), 3, i+1)\n",
    "    deltas = acc_rates[i]\n",
    "    plt.plot(deltas)\n",
    "    plt.title(name)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit374b75da0e1b40de8b7922d3f142c01d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
