{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis Hastings Custom HMC MC Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from reggae.data_loaders import load_barenco_puma, load_3day_dros, DataHolder\n",
    "from reggae.mcmc import create_chains, MetropolisHastings\n",
    "from reggae.models import transcription_mh\n",
    "from reggae.utilities import get_rbf_dist, exp, mult, discretise\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz\n",
    "from multiprocessing import Pool\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df, genes, genes_se, m_observed, f_observed, σ2_m_pre, σ2_f_pre, t = load_barenco_puma()\n",
    "m_observed, f_observed, σ2_m_pre, σ2_f_pre, t = load_barenco_puma()\n",
    "\n",
    "# m_observed, f_observed, t = load_3day_dros()\n",
    "\n",
    "replicate = 0\n",
    "\n",
    "m_df, m_observed = m_observed \n",
    "f_df, f_observed = f_observed\n",
    "\n",
    "# Shape of m_observed = (replicates, genes, times)\n",
    "m_observed = m_observed[replicate]\n",
    "f_observed = np.atleast_2d(f_observed[replicate])\n",
    "σ2_m_pre = σ2_m_pre[0]\n",
    "σ2_f_pre = σ2_f_pre[0]\n",
    "\n",
    "num_genes = m_observed.shape[0]\n",
    "τ, common_indices = discretise(t)\n",
    "N_p = τ.shape[0]\n",
    "N_m = m_observed.shape[1]\n",
    "\n",
    "data = (m_observed, f_observed)\n",
    "noise_data = (σ2_m_pre, σ2_f_pre)\n",
    "time = (t, τ, tf.constant(common_indices))\n",
    "\n",
    "data = DataHolder(data, noise_data, time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import math as tfm\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import tensorflow_probability as tfp\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "from reggae.mcmc import MetropolisHastings, Parameter\n",
    "from reggae.data_loaders import DataHolder\n",
    "from reggae.utilities import get_rbf_dist, exp, mult, jitter_cholesky\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "f64 = np.float64\n",
    "class Options():\n",
    "    def __init__(self, preprocessing_variance=True, tf_mrna_present=True):\n",
    "        self.preprocessing_variance = preprocessing_variance\n",
    "        self.tf_mrna_present = tf_mrna_present\n",
    "        \n",
    "class TranscriptionLikelihood():\n",
    "    def __init__(self, data: DataHolder, options: Options):\n",
    "        self.options = options\n",
    "        self.data = data\n",
    "        self.preprocessing_variance = options.preprocessing_variance\n",
    "        self.num_genes = data.m_obs.shape[0]\n",
    "\n",
    "    @tf.function\n",
    "    def predict_m(self, kbar, δbar, w, fbar, w_0):\n",
    "        # Take relevant parameters out of log-space\n",
    "        print(kbar)\n",
    "        a_j, b_j, d_j, s_j = (tf.reshape(tfm.exp(kbar[:, i]), (-1, 1)) for i in range(4))\n",
    "        δ = tfm.exp(δbar)\n",
    "        f_i = tfm.log(1+tfm.exp(fbar))\n",
    "        τ = self.data.τ\n",
    "        N_p = self.data.τ.shape[0]\n",
    "\n",
    "        # Calculate p_i vector\n",
    "        Δ = τ[1]-τ[0]\n",
    "        sum_term = tfm.multiply(tfm.exp(δ*τ), f_i)\n",
    "        p_i = tf.concat([[f64(0)], 0.5*Δ*tfm.cumsum(sum_term[:-1] + sum_term[1:])], axis=0) # Trapezoid rule\n",
    "        p_i = tfm.multiply(tfm.exp(-δ*τ), p_i)\n",
    "\n",
    "        # Calculate m_pred\n",
    "        integrals = tf.zeros((self.num_genes, N_p))\n",
    "        interactions = w[:, 0][:, None]*tfm.log(p_i+1e-100) + w_0[:, None]\n",
    "        G = tfm.sigmoid(interactions) # TF Activation Function (sigmoid)\n",
    "        sum_term = G * tfm.exp(d_j*τ)\n",
    "        integrals = tf.concat([tf.zeros((5, 1), dtype='float64'), 0.5*Δ*tfm.cumsum(sum_term[:, :-1] + sum_term[:, 1:], axis=1)], axis=1) # Trapezoid rule\n",
    "\n",
    "        exp_dt = tfm.exp(-d_j*τ)\n",
    "        integrals = tfm.multiply(exp_dt, integrals)\n",
    "        m_pred = b_j/d_j + tfm.multiply((a_j-b_j/d_j), exp_dt) + s_j*integrals\n",
    "\n",
    "        return m_pred\n",
    "\n",
    "    def genes(self, params=None, δbar=None,\n",
    "                     fbar=None, \n",
    "                     kbar=None, \n",
    "                     w=None,\n",
    "                     w_0=None,\n",
    "                     σ2_m=None, return_sq_diff=False):\n",
    "        '''\n",
    "        Computes likelihood of the genes.\n",
    "        If any of the optional args are None, they are replaced by their current value in params.\n",
    "        '''\n",
    "        if δbar is None:\n",
    "            δbar = params.δbar.value\n",
    "        if fbar is None:\n",
    "            fbar = params.fbar.value\n",
    "        if kbar is None:\n",
    "            kbar = params.kbar.value\n",
    "        w = params.w.value if w is None else w\n",
    "        σ2_m = params.σ2_m.value if σ2_m is None else σ2_m\n",
    "\n",
    "        w_0 = params.w_0.value if w_0 is None else w_0\n",
    "        lik, sq_diff = self._genes(δbar, fbar, kbar, w, w_0, σ2_m)\n",
    "\n",
    "        if return_sq_diff:\n",
    "            return lik, sq_diff\n",
    "        return lik\n",
    "\n",
    "    @tf.function\n",
    "    def _genes(self, δbar, fbar, kbar, w, w_0, σ2_m):\n",
    "#         tf.print(δbar)\n",
    "        m_pred = self.predict_m(kbar, δbar, w, fbar, w_0)\n",
    "\n",
    "        sq_diff = tfm.square(self.data.m_obs - tf.transpose(tf.gather(tf.transpose(m_pred),self.data.common_indices)))\n",
    "        variance = tf.reshape(σ2_m, (-1, 1))\n",
    "        if self.preprocessing_variance:\n",
    "            variance = variance + self.data.σ2_m_pre # add PUMA variance\n",
    "#         print(variance.shape, sq_diff.shape)\n",
    "        log_lik = -0.5*tfm.log(2*np.pi*(variance)) - 0.5*sq_diff/variance\n",
    "        log_lik = tf.reduce_sum(log_lik, axis=1)\n",
    "        return log_lik, sq_diff\n",
    "\n",
    "    def tfs(self, params, fbar, return_sq_diff=False): \n",
    "        '''\n",
    "        Computes log-likelihood of the transcription factors.\n",
    "        TODO this should be for the i-th TF\n",
    "        '''\n",
    "        assert self.options.tf_mrna_present\n",
    "        if not self.preprocessing_variance:\n",
    "            σ2_f = params.σ2_f.value\n",
    "            variance = σ2_f.reshape(-1, 1)\n",
    "        else:\n",
    "            variance = self.data.σ2_f_pre\n",
    "        f_pred = tfm.log(1+np.exp(fbar))\n",
    "        f_pred = tf.reshape(f_pred, (1, -1)) #np.atleast_2d f_pred[:, self.data.common_indices]\n",
    "        sq_diff = tfm.square(self.data.f_obs - tf.transpose(tf.gather(tf.transpose(f_pred),self.data.common_indices)))\n",
    "\n",
    "        log_lik = -0.5*tfm.log(2*np.pi*variance) - 0.5*sq_diff/variance\n",
    "        log_lik = tf.reduce_sum(log_lik, axis=1)\n",
    "        if return_sq_diff:\n",
    "            return log_lik, sq_diff\n",
    "        return log_lik\n",
    "    \n",
    "opt = Options(preprocessing_variance=True, tf_mrna_present=True)\n",
    "lik = TranscriptionLikelihood(data, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "import collections\n",
    "MixedKernelResults = collections.namedtuple('MixedKernelResults', [\n",
    "    'inner_results',\n",
    "#     'grads_target_log_prob',\n",
    "#     'step_size',\n",
    "#     'log_accept_ratio',\n",
    "#     'is_accepted',\n",
    "])\n",
    "\n",
    "GenericResults = collections.namedtuple('GenericResults', [\n",
    "    'target_log_prob',\n",
    "    'is_accepted',\n",
    "])\n",
    "\n",
    "class MixedKernel(tfp.mcmc.TransitionKernel): # TODO simplify all states: just send all states and keep dict of indices\n",
    "    def __init__(self, kernels, send_all_states):\n",
    "        self.kernels = kernels\n",
    "        self.send_all_states = send_all_states\n",
    "        self.num_kernels = len(kernels)\n",
    "        \n",
    "    def one_step(self, current_state, previous_kernel_results):\n",
    "#         print('running', current_state, previous_kernel_results)\n",
    "        new_state = list()\n",
    "        is_accepted = list()\n",
    "        inner_results = list()\n",
    "\n",
    "        for i in range(self.num_kernels):\n",
    "            if self.send_all_states[i]:\n",
    "                result_state, kernel_results = self.kernels[i].one_step(\n",
    "                    current_state[i], previous_kernel_results.inner_results[i], current_state)\n",
    "            else:\n",
    "                result_state, kernel_results = self.kernels[i].one_step(\n",
    "                    current_state[i], previous_kernel_results.inner_results[i])\n",
    "\n",
    "            if i == 2:\n",
    "                print(result_state, kernel_results)\n",
    "            '''\n",
    "            kernel_results: NUTSKernelResults(\n",
    "                target_log_prob=0.248428136, grads_target_log_prob=[0.0792938471], \n",
    "                step_size=[1], log_accept_ratio=0, leapfrogs_taken=50, is_accepted=1, energy=0.0327872932)\n",
    "            and more...\n",
    "            '''\n",
    "            new_state.append(result_state)\n",
    "            inner_results.append(kernel_results)\n",
    "        \n",
    "        \n",
    "        return new_state, MixedKernelResults(inner_results)\n",
    "\n",
    "    def bootstrap_results(self, init_state):\n",
    "        \"\"\"Returns an object with the same type as returned by `one_step(...)[1]`.\n",
    "        Args:\n",
    "        init_state: `Tensor` or Python `list` of `Tensor`s representing the\n",
    "        initial state(s) of the Markov chain(s).\n",
    "        Returns:\n",
    "        kernel_results: A (possibly nested) `tuple`, `namedtuple` or `list` of\n",
    "        `Tensor`s representing internal calculations made within this function.\n",
    "        \"\"\"\n",
    "        inner_kernels_bootstraps = list()\n",
    "        for i in range(self.num_kernels):\n",
    "            if self.send_all_states[i]:\n",
    "                inner_kernels_bootstraps.append(\n",
    "                    self.kernels[i].bootstrap_results(init_state[i], init_state))\n",
    "            else:\n",
    "                inner_kernels_bootstraps.append(\n",
    "                    self.kernels[i].bootstrap_results(init_state[i]))\n",
    "\n",
    "        return MixedKernelResults(inner_kernels_bootstraps)\n",
    "\n",
    "    def is_calibrated(self):\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FKernel(tfp.mcmc.TransitionKernel):\n",
    "    def __init__(self, likelihood, fbar_prior_params, num_tfs):\n",
    "        self.fbar_prior_params = fbar_prior_params\n",
    "        self.num_tfs = num_tfs\n",
    "        self.likelihood = likelihood\n",
    "        self.h_f = 0.35*tf.ones(self.N_p, dtype='float64')\n",
    "        \n",
    "    def one_step(self, current_state, previous_kernel_results, all_states):\n",
    "        # Untransformed tf mRNA vectors F (Step 1)\n",
    "        fbar = current_state\n",
    "        for i in range(self.num_tfs):\n",
    "            # Gibbs step\n",
    "            z_i = tf.reshape(tfd.MultivariateNormalDiag(fbar, self.h_f).sample(), (1, -1))\n",
    "            # MH\n",
    "            m, K = self.fbar_prior_params(params.V.value, params.L.value)\n",
    "            invKsigmaK = tf.matmul(tf.linalg.inv(K+tf.linalg.diag(self.h_f)), K) # (C_i + hI)C_i\n",
    "            L = jitter_cholesky(K-tf.matmul(K, invKsigmaK))\n",
    "            c_mu = tf.matmul(z_i, invKsigmaK)\n",
    "            fstar = tf.matmul(tf.random.normal((1, L.shape[0]), dtype='float64'), L) + c_mu\n",
    "            fstar = tf.reshape(fstar, (-1, ))\n",
    "            new_m_likelihood = self.likelihood.genes(params, fbar=fstar)\n",
    "            new_f_likelihood = 0 \n",
    "            if self.options.tf_mrna_present:\n",
    "                new_f_likelihood = self.likelihood.tfs(params, fstar)\n",
    "            new_prob = tf.reduce_sum(new_m_likelihood) + new_f_likelihood\n",
    "            old_prob = previous_kernel_results.target_log_prob #tf.reduce_sum(old_m_likelihood) + old_f_likelihood\n",
    "            \n",
    "            if self.is_accepted(new_prob, old_prob):\n",
    "                params.fbar.value = fstar\n",
    "                old_m_likelihood = new_m_likelihood\n",
    "                old_f_likelihood = new_f_likelihood\n",
    "                self.acceptance_rates['fbar'] += 1/self.num_tfs\n",
    "\n",
    "def metropolis_is_accepted(new_log_prob, old_log_prob):\n",
    "    alpha = tfm.exp(new_log_prob - old_log_prob)\n",
    "    return tf.random.uniform((1,), dtype='float64') < tfm.minimum(f64(1), alpha)\n",
    "#     if is_tensor(alpha):\n",
    "#         alpha = alpha.numpy()\n",
    "#     return not np.isnan(alpha) and random.random() < min(1, alpha)\n",
    "\n",
    "δbar_state_index = 1\n",
    "N_p = τ.shape[0]\n",
    "\n",
    "class KbarKernel(tfp.mcmc.TransitionKernel):\n",
    "    def __init__(self, likelihood, prop_dist, prior_dist, num_genes):\n",
    "        self.prop_dist = prop_dist\n",
    "        self.prior_dist = prior_dist\n",
    "        self.num_genes = num_genes\n",
    "        self.likelihood = likelihood\n",
    "        \n",
    "    def one_step(self, current_state, previous_kernel_results, all_states):\n",
    "\n",
    "        kbar = current_state\n",
    "        kstar = tf.identity(kbar)\n",
    "        old_probs = list()\n",
    "        is_accepteds = list()\n",
    "        for j in range(self.num_genes):\n",
    "            sample = self.prop_dist(kstar[j]).sample()\n",
    "#             sample = params.kbar.constrain(sample, j)\n",
    "            kstar = tf.concat([kstar[:j], [sample], kstar[j+1:]], axis=0)\n",
    "            \n",
    "            new_prob = self.likelihood.genes(\n",
    "                δbar=all_states[δbar_state_index],\n",
    "                fbar=0.5*tf.ones(N_p, dtype='float64'),       # TODO\n",
    "                kbar=kstar, \n",
    "                w=1*tf.ones((self.num_genes, 1), dtype='float64'), # TODO\n",
    "                w_0=tf.zeros(self.num_genes, dtype='float64'),     # TODO\n",
    "                σ2_m=1e-4*tf.ones(self.num_genes, dtype='float64') # TODO\n",
    "            )[j] + tf.reduce_sum(self.prior_dist.log_prob(sample))\n",
    "            \n",
    "            old_prob = previous_kernel_results.target_log_prob[j] #old_m_likelihood[j] + sum(params.kbar.prior.log_prob(kbar[j]))\n",
    "\n",
    "            is_accepted = metropolis_is_accepted(new_prob, old_prob)\n",
    "            is_accepteds.append(is_accepted)\n",
    "            \n",
    "            prob = tf.cond(tf.equal(is_accepted, tf.constant(True)), lambda:new_prob, lambda:old_prob)\n",
    "            kstar = tf.cond(tf.equal(is_accepted, tf.constant(False)), \n",
    "                                     lambda:tf.concat([kstar[:j], [current_state[j]], kstar[j+1:]], axis=0), lambda:kstar)\n",
    "            old_probs.append(prob)\n",
    "#                 \n",
    "        return kstar, GenericResults(old_probs, True) #TODO not just return true\n",
    "    \n",
    "    def bootstrap_results(self, init_state, all_states):\n",
    "        probs = list()\n",
    "        for j in range(self.num_genes):\n",
    "            prob = self.likelihood.genes(\n",
    "                δbar=all_states[δbar_state_index],\n",
    "                fbar=0.5*tf.ones(N_p, dtype='float64'),       # TODO\n",
    "                kbar=init_state, \n",
    "                w=1*tf.ones((self.num_genes, 1), dtype='float64'), # TODO\n",
    "                w_0=tf.zeros(self.num_genes, dtype='float64'),     # TODO\n",
    "                σ2_m=1e-4*tf.ones(self.num_genes, dtype='float64') # TODO\n",
    "            )[j] + tf.reduce_sum(self.prior_dist.log_prob(init_state[j]))\n",
    "            probs.append(prob)\n",
    "\n",
    "        return GenericResults(probs, True) #TODO automatically adjust\n",
    "    \n",
    "    def is_calibrated(self):\n",
    "        return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TupleParams_pre = collections.namedtuple('TupleParams_pre', [\n",
    "    'fbar','δbar','kbar','σ2_m','w','w_0','L','V','σ2_f'\n",
    "])\n",
    "TupleParams = collections.namedtuple('TupleParams', [\n",
    "    'fbar','δbar','kbar','σ2_m','w','w_0','L','V'\n",
    "])\n",
    "\n",
    "class TranscriptionCustom():\n",
    "    '''\n",
    "    Data is a tuple (m, f) of shapes (num, time)\n",
    "    time is a tuple (t, τ, common_indices)\n",
    "    '''\n",
    "    def __init__(self, data: DataHolder, options: Options):\n",
    "        self.data = data\n",
    "        min_dist = min(data.t[1:]-data.t[:-1])\n",
    "        self.N_p = data.τ.shape[0]\n",
    "        self.N_m = data.t.shape[0]      # Number of observations\n",
    "\n",
    "        self.num_tfs = data.f_obs.shape[0] # Number of TFs\n",
    "        self.num_genes = data.m_obs.shape[0]\n",
    "\n",
    "        self.likelihood = TranscriptionLikelihood(data, options)\n",
    "        self.options = options\n",
    "        # Adaptable variances\n",
    "        a = tf.constant(-0.5, dtype='float64')\n",
    "        b2 = tf.constant(2., dtype='float64')\n",
    "        \n",
    "\n",
    "        # Interaction weights\n",
    "        w_0 = Parameter('w_0', tfd.Normal(f64(0), f64(2)), np.zeros(self.num_genes), step_size=0.5*tf.ones(self.num_genes, dtype='float64'))\n",
    "        w_0.proposal_dist=lambda mu, j:tfd.Normal(mu, w_0.step_size[j])\n",
    "        w = Parameter('w', tfd.Normal(f64(0), f64(2)), 1*np.ones((self.num_genes, self.num_tfs)), step_size=0.5*tf.ones(self.num_genes, dtype='float64'))\n",
    "        w.proposal_dist=lambda mu, j:tfd.Normal(mu, w.step_size[j]) #) w_j) # At the moment this is the same as w_j0 (see pg.8)\n",
    "#         def w_log_prob(wstar, w_0star):\n",
    "#             new_prob = self.likelihood.genes(self.params, w=wstar, w_0=w_0star)[j]\n",
    "#             new_prob += tf.reduce_sum(self.params.w.prior.log_prob(wstar), axis=1) + self.params.w_0.prior.log_prob(w_0star)\n",
    "#             return tf.reduce_sum(new_prob)\n",
    "#         w = Parameter('w', tfd.Normal(f64(0), f64(2)), 1*np.ones((self.num_genes, self.num_tfs)), \n",
    "#                       step_size=0.1, hmc_log_prob=w_log_prob)\n",
    "        # Latent function\n",
    "        fbar = Parameter('fbar', self.fbar_prior, 0.5*np.ones(self.N_p))\n",
    "\n",
    "        # GP hyperparameters\n",
    "        @tf.function\n",
    "        def V_log_prob(vstar, l2star):\n",
    "            new_prob = self.params.fbar.prior(self.params.fbar.value, vstar, l2star)\n",
    "            new_prob += self.params.V.prior.log_prob(vstar)\n",
    "            new_prob += self.params.L.prior.log_prob(l2star)\n",
    "            return tf.reduce_sum(new_prob)\n",
    "        V = Parameter('V', tfd.InverseGamma(f64(0.01), f64(0.01)), f64(1), step_size=0.05, \n",
    "                      fixed=not options.tf_mrna_present, hmc_log_prob=V_log_prob)\n",
    "        L = Parameter('L', tfd.Uniform(f64(min_dist**2-0.5), f64(data.t[-1]**2)), f64(4), step_size=0.1)\n",
    "        L.proposal_dist=lambda l2: tfd.TruncatedNormal(l2, L.step_size, low=0, high=100) #l2_i\n",
    "        self.t_dist = get_rbf_dist(data.τ, self.N_p)\n",
    "\n",
    "        # Translation kinetic parameters\n",
    "        def δbar_log_prob(state):\n",
    "            new_prob = tf.reduce_sum(self.likelihood.genes(self.params, δbar=state)) \n",
    "#             tf.print(new_prob)\n",
    "            new_prob += self.params.δbar.prior.log_prob(state)\n",
    "            return new_prob\n",
    "\n",
    "        δbar = Parameter('δbar', tfd.Normal(a, b2), f64(-0.3), step_size=0.3, hmc_log_prob=δbar_log_prob)\n",
    "\n",
    "        # White noise for genes\n",
    "        σ2_m = Parameter('σ2_m', tfd.InverseGamma(f64(0.01), f64(0.01)), 1e-4*np.ones(self.num_genes), step_size=0.5)\n",
    "        σ2_m.proposal_dist=lambda mu: tfd.TruncatedNormal(mu, σ2_m.step_size, low=0, high=5)\n",
    "        # Transcription kinetic parameters\n",
    "        def constrain_kbar(kbar, gene):\n",
    "            '''Constrains a given row in kbar'''\n",
    "#             if gene == 3:\n",
    "#                 kbar[2] = np.log(0.8)\n",
    "#                 kbar[3] = np.log(1.0)\n",
    "            kbar[kbar < -10] = -10\n",
    "            kbar[kbar > 3] = 3\n",
    "            return kbar\n",
    "        kbar_initial = -0.1*np.float64(np.c_[\n",
    "            np.ones(self.num_genes), # a_j\n",
    "            np.ones(self.num_genes), # b_j\n",
    "            np.ones(self.num_genes), # d_j\n",
    "            np.ones(self.num_genes)  # s_j\n",
    "        ])\n",
    "        def kbar_log_prob(*kstar):\n",
    "            new_prob = 0\n",
    "            for j in range(num_genes):\n",
    "                new_prob += self.likelihood.genes(self.params, kbar=np.array(kstar))[j]\n",
    "                new_prob += tf.reduce_sum(self.params.kbar.prior.log_prob(kstar[j]))\n",
    "            return tf.reduce_sum(new_prob)\n",
    "\n",
    "        for j, k in enumerate(kbar_initial):\n",
    "            kbar_initial[j] = constrain_kbar(k, j)\n",
    "        kbar = Parameter('kbar',\n",
    "            tfd.Normal(a, b2), \n",
    "            kbar_initial,\n",
    "            constraint=constrain_kbar, step_size=0.25*tf.ones(4, dtype='float64'))\n",
    "        kbar.proposal_dist=lambda mu: tfd.MultivariateNormalDiag(mu, kbar.step_size)\n",
    "        \n",
    "        if not options.preprocessing_variance:\n",
    "            σ2_f = Parameter('σ2_f', tfd.InverseGamma(f64(0.01), f64(0.01)), 1e-4*np.ones(self.num_tfs), step_size=tf.constant(0.5, dtype='float64'))\n",
    "            self.params = TupleParams_pre(fbar, δbar, kbar, σ2_m, w, w_0, L, V, σ2_f)\n",
    "        else:\n",
    "            self.params = TupleParams(fbar, δbar, kbar, σ2_m, w, w_0, L, V)\n",
    "            \n",
    "    def fbar_prior_params(self, v, l2):\n",
    "    #     print('vl2', v, l2)\n",
    "        jitter = tf.linalg.diag(1e-5 * tf.ones(self.N_p, dtype='float64'))\n",
    "        K = tfm.multiply(v, tfm.exp(-tfm.square(self.t_dist)/(2*l2))) + jitter\n",
    "        m = np.zeros(self.N_p)\n",
    "        return m, K\n",
    "\n",
    "    def fbar_prior(self, fbar, v, l2):\n",
    "        m, K = self.fbar_prior_params(v, l2)\n",
    "        try:\n",
    "            return tfd.MultivariateNormalFullCovariance(m, K).log_prob(fbar)\n",
    "        except:\n",
    "            jitter = tf.linalg.diag(1e-4 *tf.ones(self.N_p))\n",
    "            try:\n",
    "                return np.float64(tfd.MultivariateNormalFullCovariance(m, K+jitter).log_prob(fbar))\n",
    "            except:\n",
    "                return tf.constant(-np.inf)\n",
    "\n",
    "\n",
    "    def sample(self, T=20000, store_every=10, burn_in=1000, report_every=100, tune_every=50):\n",
    "        print('----- Sampling Begins -----')\n",
    "        \n",
    "        self.acceptance_rates = {param.name: 0. for param in self.params} # Reset acceptance rates\n",
    "        f = IntProgress(description='Running', min=0, max=T) # instantiate the bar\n",
    "        display(f)\n",
    "        params = self.params\n",
    "\n",
    "        a = tf.constant(-0.5, dtype='float64')\n",
    "        b2 = tf.constant(2., dtype='float64')\n",
    "\n",
    "        def unnormalized_log_prob(x):\n",
    "            return -x - x**2.\n",
    "\n",
    "        test_kernel = tfp.mcmc.NoUTurnSampler(target_log_prob_fn=unnormalized_log_prob, step_size=1.)\n",
    "\n",
    "        kernels = [\n",
    "            test_kernel,\n",
    "            params.δbar.kernel, # Translation ODE parameters\n",
    "            KbarKernel(self.likelihood, params.kbar.proposal_dist, tfd.Normal(a, b2), self.num_genes),\n",
    "            FKernel(self.likelihood, self.fbar_prior_params, self.num_tfs):\n",
    "        ]\n",
    "        send_all_states = [\n",
    "            False,\n",
    "            False,\n",
    "            True,\n",
    "            True,\n",
    "        ]\n",
    "\n",
    "        mixed_kern = MixedKernel(kernels, send_all_states)\n",
    "        def trace_fn(a, pkr):\n",
    "#             print(pkr) #TODO\n",
    "            return pkr.inner_results[0].is_accepted\n",
    "        num_results = int(1e2)\n",
    "        num_burnin_steps = int(1e1)\n",
    "\n",
    "        # Run the chain (with burn-in).\n",
    "        @tf.function\n",
    "        def run_chain():\n",
    "            # Run the chain (with burn-in).\n",
    "            samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "                  num_results=num_results,\n",
    "                  num_burnin_steps=num_burnin_steps,\n",
    "                  current_state=[1., params.δbar.value, params.kbar.value],\n",
    "                  kernel=mixed_kern,\n",
    "                  trace_fn=trace_fn)\n",
    "\n",
    "            return samples, is_accepted\n",
    "\n",
    "        samples, is_accepted = run_chain()\n",
    "        \n",
    "\n",
    "        print(samples)\n",
    "        sample_mean = tf.reduce_mean(samples[0])\n",
    "        sample_stddev = tf.math.reduce_std(samples[0])\n",
    "        is_accepted = tf.reduce_mean(tf.cast(is_accepted, dtype=tf.float32))\n",
    "\n",
    "        # for key in self.acceptance_rates:\n",
    "        #     self.acceptance_rates[key] /= T\n",
    "        # rates = np.array(self.samples['acc_rates']).T/np.arange(1, T-burn_in+1, store_every)\n",
    "        # self.samples['acc_rates'] = rates\n",
    "        f.value = T\n",
    "        print('----- Finished -----')\n",
    "        return sample_mean, sample_stddev, is_accepted\n",
    "        \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = TranscriptionCustom(data, opt)\n",
    "\n",
    "sample_mean, sample_stddev, is_accepted = model.sample()\n",
    "\n",
    "print('mean:{:.4f}  stddev:{:.4f}  acceptance:{:.4f}'.format(\n",
    "    sample_mean.numpy(), sample_stddev.numpy(), is_accepted.numpy()))\n",
    "#mean:-0.4878  stddev:0.7141  acceptance:0.8933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TranscriptionHMC(MetropolisHastings):\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def iter_delta(self, δbar, kernel):\n",
    "        trace_fn = lambda _, pkr: pkr.is_accepted\n",
    "        for i in range(self.num_tfs):# TODO make for self.num_tfs > 1\n",
    "\n",
    "            samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "                num_results=2,\n",
    "                num_burnin_steps=0,\n",
    "                current_state=δbar,\n",
    "                kernel=kernel,\n",
    "                trace_fn=trace_fn)\n",
    "\n",
    "        return samples, is_accepted\n",
    "    \n",
    "    @tf.function\n",
    "    def iter_rbf_params(self, v, l2, kernel):\n",
    "        return tfp.mcmc.sample_chain(\n",
    "            num_results=2,\n",
    "            num_burnin_steps=0,\n",
    "            current_state=[v, l2],\n",
    "            kernel=kernel,\n",
    "            trace_fn=lambda _, pkr: pkr.is_accepted)\n",
    "        \n",
    "    def iterate(self):\n",
    "        trace_fn = lambda _, pkr: pkr.is_accepted\n",
    "        params = self.params\n",
    "        # Compute likelihood for comparison\n",
    "        old_m_likelihood, sq_diff_m  = self.likelihood.genes(params, return_sq_diff=True)\n",
    "        old_f_likelihood = 0\n",
    "        if self.options.tf_mrna_present:\n",
    "            old_f_likelihood, sq_diff_f  = self.likelihood.tfs(params, params.fbar.value, return_sq_diff=True)\n",
    "        \n",
    "\n",
    "\n",
    "        if self.options.tf_mrna_present: # (Step 2)\n",
    "            # Log of translation ODE degradation rates\n",
    "            δbar = params.δbar.value\n",
    "            samples, is_accepted = self.iter_delta(δbar, params.δbar.kernel)\n",
    "            if is_accepted[-1]:\n",
    "                params.δbar.value = samples[-1] #δstar\n",
    "                self.acceptance_rates['δbar'] += 1/self.num_tfs\n",
    "\n",
    "#             δbar = params.δbar.value\n",
    "#             for i in range(self.num_tfs):# TODO make for self.num_tfs > 1\n",
    "#                 # Proposal distribution\n",
    "#                 δstar = params.δbar.propose(δbar) # δstar is in log-space, i.e. δstar = δbar*\n",
    "#                 new_prob = np.sum(self.likelihood.genes(params, δbar=δstar)) + params.δbar.prior.log_prob(δstar)\n",
    "#                 old_prob = np.sum(old_m_likelihood) + params.δbar.prior.log_prob(δbar)\n",
    "#                 if self.is_accepted(new_prob, old_prob):\n",
    "#                     params.δbar.value = δstar\n",
    "#                     self.acceptance_rates['δbar'] += 1/self.num_tfs\n",
    "\n",
    "\n",
    "        # Log of transcription ODE kinetic params (Step 3)\n",
    "#         kbar = params.kbar.value\n",
    "#         samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "#             num_results=2,\n",
    "#             num_burnin_steps=0,\n",
    "#             current_state=[kbar[j] for j in range(self.num_genes)],\n",
    "#             kernel=params.kbar.kernel,\n",
    "#             trace_fn=trace_fn)\n",
    "\n",
    "#         for j in range(self.num_genes):\n",
    "#             if is_accepted[-1]:\n",
    "#                 params.kbar.value[j] = samples[j][-1]\n",
    "#                 self.acceptance_rates['kbar'] += 1\n",
    "\n",
    "\n",
    "\n",
    "        # Interaction weights and biases (note: should work for self.num_tfs > 1) (Step 4)\n",
    "#         w = params.w.value\n",
    "#         w_0 = params.w_0.value\n",
    "#         samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "#             num_results=2,\n",
    "#             num_burnin_steps=0,\n",
    "#             current_state=[w, w_0],\n",
    "#             kernel=params.w.kernel,\n",
    "#             trace_fn=trace_fn)\n",
    "\n",
    "#         if is_accepted[-1]:\n",
    "#             params.w.value = samples[0][-1]\n",
    "#             params.w_0.value = samples[1][-1]\n",
    "#             self.acceptance_rates['w'] += 1/self.num_genes\n",
    "#             self.acceptance_rates['w_0'] += 1/self.num_genes\n",
    "        w = params.w.value\n",
    "        w_0 = params.w_0.value\n",
    "        wstar = w.copy()\n",
    "        w_0star = w_0.copy()\n",
    "        for j in range(self.num_genes):\n",
    "            sample_0 = params.w_0.propose(w_0[j], j)\n",
    "            sample = params.w.propose(wstar[j], j)\n",
    "            wstar[j] = sample\n",
    "            w_0star[j] = sample_0\n",
    "            new_prob = self.likelihood.genes(params, w=wstar, w_0=w_0star)[j] + sum(params.w.prior.log_prob(sample)) + params.w_0.prior.log_prob(sample_0)\n",
    "            old_prob = old_m_likelihood[j] + sum(params.w.prior.log_prob(w[j,:])) + params.w_0.prior.log_prob(w_0[j])\n",
    "            if self.is_accepted(new_prob, old_prob):\n",
    "                params.w.value[j] = sample\n",
    "                params.w_0.value[j] = sample_0\n",
    "                self.acceptance_rates['w'] += 1/self.num_genes\n",
    "                self.acceptance_rates['w_0'] += 1/self.num_genes\n",
    "            else:\n",
    "                wstar[j] = params.w.value[j]\n",
    "\n",
    "\n",
    "        # Noise variances\n",
    "        if self.options.preprocessing_variance:\n",
    "            σ2_m = params.σ2_m.value\n",
    "            σ2_mstar = σ2_m.copy()\n",
    "            for j in range(self.num_genes):\n",
    "                sample = params.σ2_m.propose(σ2_m[j])\n",
    "                σ2_mstar[j] = sample\n",
    "                old_q = params.σ2_m.proposal_dist(σ2_mstar[j]).log_prob(σ2_m[j])\n",
    "                new_prob = self.likelihood.genes(params, σ2_m=σ2_mstar)[j] +params.σ2_m.prior.log_prob(σ2_mstar[j])\n",
    "                \n",
    "                new_q = params.σ2_m.proposal_dist(σ2_m[j]).log_prob(σ2_mstar[j])\n",
    "                old_prob = self.likelihood.genes(params, σ2_m=σ2_m)[j] + params.σ2_m.prior.log_prob(σ2_m[j])\n",
    "                    \n",
    "                if self.is_accepted(new_prob + old_q, old_prob + new_q):\n",
    "                    params.σ2_m.value[j] = sample\n",
    "                    self.acceptance_rates['σ2_m'] += 1/self.num_genes\n",
    "                else:\n",
    "                    σ2_mstar[j] = σ2_m[j]\n",
    "        else: # Use Gibbs sampling\n",
    "            # Prior parameters\n",
    "            α = params.σ2_m.prior.concentration\n",
    "            β = params.σ2_m.prior.scale\n",
    "            # Conditional posterior of inv gamma parameters:\n",
    "            α_post = α + 0.5*self.N_m\n",
    "            β_post = β + 0.5*np.sum(sq_diff_m)\n",
    "            # print(α.shape, sq_diff.shape)\n",
    "            # print('val', β_post.shape, params.σ2_m.value)\n",
    "            params.σ2_m.value = np.repeat(tfd.InverseGamma(α_post, β_post).sample(), self.num_genes)\n",
    "            self.acceptance_rates['σ2_m'] += 1\n",
    "            \n",
    "            if self.options.tf_mrna_present: # (Step 5)\n",
    "                # Prior parameters\n",
    "                α = params.σ2_f.prior.concentration\n",
    "                β = params.σ2_f.prior.scale\n",
    "                # Conditional posterior of inv gamma parameters:\n",
    "                α_post = α + 0.5*self.N_m\n",
    "                β_post = β + 0.5*np.sum(sq_diff_f)\n",
    "                # print(α.shape, sq_diff.shape)\n",
    "                # print('val', β_post.shape, params.σ2_m.value)\n",
    "                params.σ2_f.value = np.repeat(tfd.InverseGamma(α_post, β_post).sample(), self.num_tfs)\n",
    "                self.acceptance_rates['σ2_f'] += 1\n",
    "\n",
    "            # print('val', params.σ2_m.value)\n",
    "        # Length scales and variances of GP kernels\n",
    "        l2 = params.L.value\n",
    "        v = params.V.value\n",
    "        for i in range(self.num_tfs):\n",
    "            # Proposal distributions\n",
    "            # Acceptance probabilities            \n",
    "            samples, is_accepted = self.iter_rbf_params(v, l2, params.V.kernel)\n",
    "\n",
    "            if is_accepted[-1]:\n",
    "                params.V.value = samples[0][-1]\n",
    "                params.L.value = samples[1][-1]\n",
    "        \n",
    "#             if accepted:\n",
    "#                 params.L.value = l2star\n",
    "                self.acceptance_rates['V'] += 1/self.num_tfs\n",
    "                self.acceptance_rates['L'] += 1/self.num_tfs\n",
    "\n",
    "    @staticmethod\n",
    "    def initialise_from_state(args, state):\n",
    "        model = TranscriptionMCMC(*args)\n",
    "        model.acceptance_rates = state.acceptance_rates\n",
    "        model.samples = state.samples\n",
    "        return model\n",
    "\n",
    "    def predict_m(self, kbar, δbar, w, fbar, w_0):\n",
    "        return self.likelihood.predict_m(kbar, δbar, w, fbar, w_0)\n",
    "\n",
    "    def predict_m_with_current(self):\n",
    "        return self.likelihood.predict_m(self.params.kbar.value, \n",
    "                                         self.params.δbar.value, \n",
    "                                         self.params.w.value, \n",
    "                                         self.params.fbar.value,\n",
    "                                         self.params.w_0.value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options(preprocessing_variance=True, tf_mrna_present=True)\n",
    "model = TranscriptionHMC(data, opt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T = 600\n",
    "model.sample(T, 1, 0, 1)\n",
    "\n",
    "print(model.acceptance_rates)\n",
    "samples = model.samples\n",
    "acceptance_rates = model.acceptance_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "store_every = 1\n",
    "burn_in = 0\n",
    "report_every = 20\n",
    "num_chains = 4\n",
    "tune_every = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chains\n",
    "job = create_chains(\n",
    "    transcription.TranscriptionMCMC, \n",
    "    [data, opt], \n",
    "    {\n",
    "        'T': T, \n",
    "        'store_every': store_every, \n",
    "        'burn_in': burn_in,\n",
    "        'report_every': report_every,\n",
    "        'tune_every':tune_every\n",
    "    }, \n",
    "    num_chains=num_chains)\n",
    "\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = job[0].acceptance_rates.keys()\n",
    "\n",
    "variables = {key : np.empty((0, T, *job[0].samples[key].get().shape[1:])) for key in keys}\n",
    "\n",
    "for res in job:\n",
    "    for key in keys:\n",
    "        variables[key] = np.append(variables[key], np.expand_dims(res.samples[key].get(), 0), axis=0)\n",
    "\n",
    "plt.plot(variables['L'][:,-100:].T)\n",
    "\n",
    "mixes = {key: arviz.convert_to_inference_data(variables[key]) for key in keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rhat\n",
    "Rhat is the ratio of posterior variance and within-chain variance. If the ratio exceeds 1.1 then we consider the chains have not mixed well. As the between-chain variance tends to the within-chain then R tends to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rhat = arviz.rhat(mixes['fbar'])\n",
    "\n",
    "Rhats = np.array([np.mean(arviz.rhat(mixes[key]).x.values) for key in keys])\n",
    "\n",
    "rhat_df = pd.DataFrame([[*Rhats], [*(Rhats < 1.1)]], columns=keys)\n",
    "\n",
    "display(rhat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank plots\n",
    "\n",
    "Rank plots are histograms of the ranked posterior draws (ranked over all\n",
    "    chains) plotted separately for each chain.\n",
    "    If all of the chains are targeting the same posterior, we expect the ranks in each chain to be\n",
    "    uniform, whereas if one chain has a different location or scale parameter, this will be\n",
    "    reflected in the deviation from uniformity. If rank plots of all chains look similar, this\n",
    "    indicates good mixing of the chains.\n",
    "\n",
    "Rank-normalization, folding, and localization: An improved R-hat\n",
    "    for assessing convergence of MCMC. arXiv preprint https://arxiv.org/abs/1903.08008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_rank(L_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effective sample sizes\n",
    "\n",
    "Plot quantile, local or evolution of effective sample sizes (ESS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_ess(L_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte-Carlo Standard Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_mcse(L_mix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Plot\n",
    "Plot parallel coordinates plot showing posterior points with and without divergences.\n",
    "\n",
    "Described by https://arxiv.org/abs/1709.01449, suggested by Ari Hartikainen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_parallel(azl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step size is standard dev, too small means it takes long time to reach high density areas. too long means we reject many of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = job[0].samples\n",
    "acceptance_rates = job[0].acceptance_rates\n",
    "\n",
    "model = transcription.TranscriptionMCMC.initialise_from_state([data, opt], job[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin MCMC\n",
    "T = 1000\n",
    "model.sample(T, 1, 0, 1)\n",
    "\n",
    "print(model.acceptance_rates)\n",
    "samples = model.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = transcription_model.samples\n",
    "acceptance_rates = model.acceptance_rates\n",
    "plt.figure(figsize=(10,14))\n",
    "parameter_names = acceptance_rates.keys()\n",
    "acc_rates = samples['acc_rates']\n",
    "\n",
    "for i, name in enumerate(parameter_names):\n",
    "    plt.subplot(len(parameter_names), 3, i+1)\n",
    "    deltas = acc_rates[i]\n",
    "    plt.plot(deltas)\n",
    "    plt.title(name)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decay\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, param in enumerate(['δbar', 'L', 'V']):\n",
    "    ax = plt.subplot(331+i)\n",
    "    plt.plot(samples[param].get())\n",
    "    ax.set_title(param)\n",
    "#'σ', 'w']):\n",
    "\n",
    "plt.figure()\n",
    "for j in range(num_genes):\n",
    "    plt.plot(samples['w'].get()[:, j], label=m_df.index[j])\n",
    "plt.legend()\n",
    "plt.title('Interaction weights')\n",
    "\n",
    "plt.figure()\n",
    "for j in range(num_genes):\n",
    "    plt.plot(samples['w_0'].get()[:,j])\n",
    "plt.title('Interaction bias')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot transcription ODE kinetic params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 14))\n",
    "plt.title('Transcription ODE kinetic parameters')\n",
    "labels = ['a', 'b', 'd', 's']\n",
    "for j in range(num_genes):\n",
    "    ax = plt.subplot(num_genes, 2, j+1)\n",
    "    k_param = samples['kbar'].get()[:, j]\n",
    "#     print(k_param)\n",
    "    \n",
    "    for k in range(4):\n",
    "        plt.plot(k_param[-20000:, k], label=labels[k])\n",
    "    plt.axhline(np.mean(k_param[-200:, 3]))\n",
    "    plt.legend()\n",
    "    ax.set_title(f'Gene {j}')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_barenco = True\n",
    "def plot_kinetics(kbar, plot_barenco=False):\n",
    "    plt.figure(figsize=(14, 14))\n",
    "    k_latest = np.exp(np.mean(kbar[-100:], axis=0))\n",
    "    B = k_latest[:,1]\n",
    "    D = k_latest[:,2]\n",
    "    S = k_latest[:,3]\n",
    "    data = [B, S, D]\n",
    "    barenco_data = [None, None, None]\n",
    "\n",
    "    if plot_barenco:\n",
    "        # From Martino paper ... do a rough rescaling so that the scales match.\n",
    "        B_barenco = np.array([2.6, 1.5, 0.5, 0.2, 1.35])[[0, 4, 2, 3, 1]]\n",
    "        B_barenco = B_barenco/np.mean(B_barenco)*np.mean(B)\n",
    "        S_barenco = (np.array([3, 0.8, 0.7, 1.8, 0.7])/1.8)[[0, 4, 2, 3, 1]]\n",
    "        S_barenco = S_barenco/np.mean(S_barenco)*np.mean(S)\n",
    "        D_barenco = (np.array([1.2, 1.6, 1.75, 3.2, 2.3])*0.8/3.2)[[0, 4, 2, 3, 1]]\n",
    "        D_barenco = D_barenco/np.mean(D_barenco)*np.mean(D)\n",
    "        barenco_data = [B_barenco, S_barenco, D_barenco]\n",
    "\n",
    "    labels = ['Basal rates', 'Sensitivities', 'Decay rates']\n",
    "\n",
    "    plotnum = 331\n",
    "    for A, B, label in zip(data, barenco_data, labels):\n",
    "        plt.subplot(plotnum)\n",
    "        plotnum+=1\n",
    "        plt.bar(np.arange(num_genes)-0.2, A, width=0.4, tick_label=m_df.index, label='Model')\n",
    "        if B is not None:\n",
    "            plt.bar(np.arange(num_genes)+0.2, B, width=0.4, color='blue', align='center', label='Barenco et al.')\n",
    "        plt.title(label)\n",
    "        plt.legend()\n",
    "\n",
    "kbar = samples['kbar'].get()\n",
    "plot_kinetics(kbar, plot_barenco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot genes\n",
    "plt.figure(figsize=(14, 17))\n",
    "kbar = samples['kbar'].get()[-1]\n",
    "δbar = samples['δbar'].get()[-1]\n",
    "w = samples['w'].get()[-1]\n",
    "fbar = samples['fbar'].get()[-1]\n",
    "w_0 = samples['w_0'].get()[-1]\n",
    "m_pred = model.predict_m(kbar, δbar, w, fbar, w_0)\n",
    "print(np.arange(N_p)[common_indices])\n",
    "for j in range(num_genes):\n",
    "    ax = plt.subplot(531+j)\n",
    "    plt.title(m_df.index[j])\n",
    "    plt.scatter(common_indices, m_observed[j], marker='x')\n",
    "    # plt.errorbar([n*10+n for n in range(7)], Y[j], 2*np.sqrt(Y_var[j]), fmt='none', capsize=5)\n",
    "    plt.plot(m_pred[j,:], color='grey')\n",
    "    plt.xticks(np.arange(N_p)[common_indices])\n",
    "    ax.set_xticklabels(np.arange(t[-1]))\n",
    "    plt.xlabel('Time (h)')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.title('Noise variances')\n",
    "for i, j in enumerate(range(num_genes)):\n",
    "    ax = plt.subplot(num_genes, num_genes-2, i+1)\n",
    "    plt.title(m_df.index[j])\n",
    "    plt.plot(samples['σ2_m'].get()[:,j])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_barenco_data(f):\n",
    "    scale_pred = np.sqrt(np.var(f))\n",
    "    barencof = np.array([[0.0, 200.52011, 355.5216125, 205.7574913, 135.0911372, 145.1080997, 130.7046969],\n",
    "                         [0.0, 184.0994134, 308.47592, 232.1775328, 153.6595161, 85.7272235, 168.0910562],\n",
    "                         [0.0, 230.2262511, 337.5994811, 276.941654, 164.5044287, 127.8653452, 173.6112139]])\n",
    "\n",
    "    barencof = barencof[0]/(np.sqrt(np.var(barencof[0])))*scale_pred\n",
    "    # measured_p53 = df[df.index.isin(['211300_s_at', '201746_at'])]\n",
    "    # measured_p53 = measured_p53.mean(0)\n",
    "    # measured_p53 = measured_p53*scale_pred\n",
    "    measured_p53 = 0\n",
    "    \n",
    "    return barencof, measured_p53\n",
    "\n",
    "def plot_f(f):\n",
    "    fig = plt.figure(figsize=(13, 7))\n",
    "\n",
    "    barencof = scaled_barenco_data(f)\n",
    "    lb = len(barencof)\n",
    "    plt.plot(np.arange(N_p), f, color='grey')\n",
    "    plt.scatter(np.arange(0, N_p)[common_indices], barencof, marker='x')\n",
    "    plt.xticks(np.arange(N_p)[common_indices])\n",
    "    fig.axes[0].set_xticklabels(np.arange(N_m)*2)\n",
    "    plt.xlabel('Time (h)')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 7))\n",
    "f_samples = np.log(1+np.exp(np.array(samples['fbar'].get()[-50:])))\n",
    "if 'σ2_f' in model.params._fields:\n",
    "    σ2_f = model.params.σ2_f.value\n",
    "    plt.errorbar(τ[common_indices], f_observed[0], 2*np.sqrt(σ2_f[0]), \n",
    "                 fmt='none', capsize=5, color='blue')\n",
    "else:\n",
    "    σ2_f = σ2_f_pre\n",
    "    \n",
    "bounds = arviz.hpd(f_samples, credible_interval=0.95)\n",
    "for i in range(1,20):\n",
    "    f_i = f_samples[-i]\n",
    "#     plt.plot(f_i)\n",
    "#     f_i[0] = 0\n",
    "    kwargs = {}\n",
    "    if i == 1:\n",
    "        kwargs = {'label':'Samples'}\n",
    "    plt.plot(τ, f_i, c='blue', alpha=0.5, **kwargs)\n",
    "\n",
    "if plot_barenco:\n",
    "    barenco_f, _ = scaled_barenco_data(np.mean(f_samples[-10:], axis=0))\n",
    "    plt.scatter(τ[common_indices], barenco_f, marker='x', s=60, linewidth=3, label='Barenco et al.')\n",
    "\n",
    "plt.scatter(τ[common_indices], f_observed[0], marker='x', s=70, linewidth=4, label='Observed')\n",
    "\n",
    "plt.fill_between(τ, bounds[:, 0], bounds[:, 1], color='grey', alpha=0.5, label='95% credibility interval')\n",
    "plt.xticks(np.arange(N_m)*2)\n",
    "fig.axes[0].set_xticklabels(t)\n",
    "plt.xlabel('Time (h)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,12,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit374b75da0e1b40de8b7922d3f142c01d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
