{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis Hastings Custom HMC MC Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from reggae.data_loaders import load_barenco_puma, load_3day_dros, DataHolder, scaled_barenco_data\n",
    "from reggae.mcmc import create_chains, MetropolisHastings, Parameter\n",
    "from reggae.utilities import get_rbf_dist, discretise, logit, LogisticNormal\n",
    "from reggae.plot import plotters\n",
    "from reggae.models import TranscriptionLikelihood, Options\n",
    "from reggae.models import transcription_nuts_merge\n",
    "from reggae.models.results import GenericResults\n",
    "from reggae.models.kernels import MixedKernel, FKernel, KbarKernel\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import math as tfm\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "f64 = np.float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LogisticNormal(-1, 1)\n",
    "\n",
    "xs = np.linspace(-2, 2, 100)\n",
    "ll = [l.log_prob(x).numpy() for x in xs]\n",
    "print(ll)\n",
    "plt.plot(xs, ll)\n",
    "# plt.ylim(0, 0.5)\n",
    "# plt.ylim(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df, genes, genes_se, m_observed, f_observed, σ2_m_pre, σ2_f_pre, t = load_barenco_puma()\n",
    "m_observed, f_observed, σ2_m_pre, σ2_f_pre, t = load_barenco_puma()\n",
    "\n",
    "# m_observed, f_observed, t = load_3day_dros()\n",
    "\n",
    "replicate = 0\n",
    "\n",
    "m_df, m_observed = m_observed \n",
    "f_df, f_observed = f_observed\n",
    "\n",
    "# Shape of m_observed = (replicates, genes, times)\n",
    "m_observed = m_observed[replicate]\n",
    "f_observed = np.atleast_2d(f_observed[replicate])\n",
    "σ2_m_pre = σ2_m_pre[0]\n",
    "σ2_f_pre = σ2_f_pre[0]\n",
    "\n",
    "num_genes = m_observed.shape[0]\n",
    "τ, common_indices = discretise(t)\n",
    "N_p = τ.shape[0]\n",
    "N_m = m_observed.shape[1]\n",
    "\n",
    "data = (m_observed, f_observed)\n",
    "noise_data = (σ2_m_pre, σ2_f_pre)\n",
    "time = (t, τ, tf.constant(common_indices))\n",
    "\n",
    "data = DataHolder(data, noise_data, time)\n",
    "N_p = τ.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options(preprocessing_variance=True, tf_mrna_present=True)\n",
    "lik = transcription_nuts_merge.TranscriptionLikelihood(data, opt)\n",
    "\n",
    "print(max(np.var(data.f_obs, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GibbsKernel(tfp.mcmc.TransitionKernel):\n",
    "    \n",
    "    def one_step(self, current_state, previous_kernel_results, all_states):\n",
    "\n",
    "        if self.options.preprocessing_variance:\n",
    "            pass\n",
    "        else: # Use Gibbs sampling\n",
    "            # Prior parameters\n",
    "            α = params.σ2_m.prior.concentration\n",
    "            β = params.σ2_m.prior.scale\n",
    "            # Conditional posterior of inv gamma parameters:\n",
    "            α_post = α + 0.5*self.N_m\n",
    "            β_post = β + 0.5*np.sum(sq_diff_m)\n",
    "            # print(α.shape, sq_diff.shape)\n",
    "            # print('val', β_post.shape, params.σ2_m.value)\n",
    "            params.σ2_m.value = np.repeat(tfd.InverseGamma(α_post, β_post).sample(), self.num_genes)\n",
    "            self.acceptance_rates['σ2_m'] += 1\n",
    "            \n",
    "            if self.options.tf_mrna_present: # (Step 5)\n",
    "                # Prior parameters\n",
    "                α = params.σ2_f.prior.concentration\n",
    "                β = params.σ2_f.prior.scale\n",
    "                # Conditional posterior of inv gamma parameters:\n",
    "                α_post = α + 0.5*self.N_m\n",
    "                β_post = β + 0.5*np.sum(sq_diff_f)\n",
    "                # print(α.shape, sq_diff.shape)\n",
    "                # print('val', β_post.shape, params.σ2_m.value)\n",
    "                params.σ2_f.value = np.repeat(tfd.InverseGamma(α_post, β_post).sample(), self.num_tfs)\n",
    "                self.acceptance_rates['σ2_f'] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "TupleParams_pre = collections.namedtuple('TupleParams_pre', [\n",
    "    'fbar','δbar','kbar','σ2_m','w','w_0','L','V', 'kinetics', 'σ2_f',\n",
    "])\n",
    "TupleParams = collections.namedtuple('TupleParams', [\n",
    "    'fbar','δbar','kbar','σ2_m','w','w_0','L','V', 'kinetics'\n",
    "])\n",
    "\n",
    "class TranscriptionCustom():\n",
    "    '''\n",
    "    Data is a tuple (m, f) of shapes (num, time)\n",
    "    time is a tuple (t, τ, common_indices)\n",
    "    '''\n",
    "    def __init__(self, data: DataHolder, options: Options):\n",
    "        self.data = data\n",
    "        self.samples = None\n",
    "        min_dist = min(data.t[1:]-data.t[:-1])\n",
    "        self.N_p = data.τ.shape[0]\n",
    "        self.N_m = data.t.shape[0]      # Number of observations\n",
    "\n",
    "        self.num_tfs = data.f_obs.shape[0] # Number of TFs\n",
    "        self.num_genes = data.m_obs.shape[0]\n",
    "\n",
    "        self.likelihood = transcription_nuts_merge.TranscriptionLikelihood(data, options)\n",
    "        self.options = options\n",
    "        # Adaptable variances\n",
    "        a = tf.constant(-0.5, dtype='float64')\n",
    "        b2 = tf.constant(2., dtype='float64')\n",
    "        self.state_indices = {\n",
    "            'kinetics': 0,\n",
    "            'fbar': 1, \n",
    "            'rbf_params': 2,\n",
    "            'σ2_m': 3,\n",
    "            'w': 4,\n",
    "        }\n",
    "        logistic_step_size = 0.001\n",
    "\n",
    "        # Interaction weights\n",
    "        def w_log_prob(all_states):\n",
    "            def w_log_prob_fn(wstar, w_0star):\n",
    "                new_prob = tf.reduce_sum(self.likelihood.genes(\n",
    "                    all_states=all_states, \n",
    "                    state_indices=self.state_indices,\n",
    "                     w=wstar))\n",
    "                new_prob += tf.reduce_sum(self.params.w.prior.log_prob(wstar)) \n",
    "                new_prob += tf.reduce_sum(self.params.w_0.prior.log_prob(w_0star))\n",
    "                return tf.reduce_sum(new_prob)\n",
    "            return w_log_prob_fn\n",
    "        w = Parameter('w', tfd.Normal(f64(0), f64(2)), \n",
    "                      1*tf.ones((self.num_genes, self.num_tfs), dtype='float64'), \n",
    "                      step_size=0.04, hmc_log_prob=w_log_prob, requires_all_states=True)\n",
    "        w_0 = Parameter('w_0', tfd.Normal(f64(0), f64(2)), tf.zeros(self.num_genes, dtype='float64'))\n",
    "\n",
    "        # Latent function\n",
    "        fbar_kernel = FKernel(self.likelihood, \n",
    "                              self.fbar_prior_params, \n",
    "                              self.num_tfs, self.num_genes, \n",
    "                              self.options.tf_mrna_present, \n",
    "                              self.state_indices,\n",
    "                              0.2*tf.ones(N_p, dtype='float64'))\n",
    "        fbar = Parameter('fbar', self.fbar_prior, 0.5*tf.ones((self.num_tfs, self.N_p), dtype='float64'),\n",
    "                         kernel=fbar_kernel, requires_all_states=False)\n",
    "\n",
    "        # GP hyperparameters\n",
    "        def rbf_params_log_prob(all_states):\n",
    "            def rbf_params_log_prob(vbar, l2bar):\n",
    "                v = logit(vbar, nan_replace=self.params.V.prior.b)\n",
    "                l2 = logit(l2bar, nan_replace=self.params.L.prior.b)\n",
    "\n",
    "                new_prob = self.params.fbar.prior(all_states[self.state_indices['fbar']], vbar, l2bar)\n",
    "#                 tf.print(new_prob)\n",
    "                new_prob += self.params.V.prior.log_prob(v)\n",
    "                new_prob += self.params.L.prior.log_prob(l2)\n",
    "#                 tf.print('new prob', new_prob)\n",
    "#                 if new_prob < -1e3:\n",
    "#                     tf.print(all_states[self.state_indices['fbar']], v, l2)\n",
    "                return tf.reduce_sum(new_prob)\n",
    "            return rbf_params_log_prob\n",
    "\n",
    "        V = Parameter('rbf_params', LogisticNormal(f64(1e-4), f64(1+max(np.var(data.f_obs, axis=1))),allow_nan_stats=False), \n",
    "                      [tf.constant(f64(0.8)), tf.constant(f64(0.98))], step_size=logistic_step_size, \n",
    "                      fixed=not options.tf_mrna_present, hmc_log_prob=rbf_params_log_prob, requires_all_states=True)\n",
    "        L = Parameter('L', LogisticNormal(f64(min_dist**2-0.2), f64(data.t[-1]**2), allow_nan_stats=False), None)\n",
    "\n",
    "        self.t_dist = get_rbf_dist(data.τ, self.N_p)\n",
    "\n",
    "        # Translation kinetic parameters\n",
    "        def δbar_log_prob(all_states):\n",
    "            def δbar_log_prob_fn(state):\n",
    "#                 chain_probs = list()\n",
    "#                 new_prob = None\n",
    "#                 for chain in range(state.shape[0]):\n",
    "                new_prob = tf.reduce_sum(self.likelihood.genes(\n",
    "                    all_states=all_states, \n",
    "                    state_indices=self.state_indices,\n",
    "                    δbar=state\n",
    "                ))\n",
    "                new_prob += self.params.δbar.prior.log_prob(logit(state))\n",
    "#                 chain_probs.append(new_prob)\n",
    "                    \n",
    "#                 tf.print(tf.constant(chain_probs))\n",
    "                return new_prob\n",
    "            return δbar_log_prob_fn\n",
    "        δbar = Parameter('δbar', LogisticNormal(0.1, 8), tf.reshape(f64(0.6), (self.num_tfs,)), step_size=logistic_step_size, \n",
    "                         hmc_log_prob=δbar_log_prob, requires_all_states=True)\n",
    "\n",
    "        # White noise for genes\n",
    "        def σ2_m_log_prob(all_states):\n",
    "            def σ2_m_log_prob_fn(σ2_mstar):\n",
    "#                 tf.print('star:',σ2_mstar)\n",
    "                new_prob = self.likelihood.genes(\n",
    "                    all_states=all_states, \n",
    "                    state_indices=self.state_indices,\n",
    "                    σ2_m=σ2_mstar \n",
    "                ) + self.params.σ2_m.prior.log_prob(logit(σ2_mstar))\n",
    "#                 tf.print('prob', tf.reduce_sum(new_prob))\n",
    "                return tf.reduce_sum(new_prob)                \n",
    "            return σ2_m_log_prob_fn\n",
    "        σ2_m = Parameter('σ2_m', LogisticNormal(f64(1e-5), f64(max(np.var(data.f_obs, axis=1)))), 1e-4*tf.ones(self.num_genes, dtype='float64'), \n",
    "                         hmc_log_prob=σ2_m_log_prob, requires_all_states=True, step_size=logistic_step_size)\n",
    "        # Transcription kinetic parameters\n",
    "        def constrain_kbar(kbar, gene):\n",
    "            '''Constrains a given row in kbar'''\n",
    "#             if gene == 3:\n",
    "#                 kbar[2] = np.log(0.8)\n",
    "#                 kbar[3] = np.log(1.0)\n",
    "            kbar[kbar < -10] = -10\n",
    "            kbar[kbar > 3] = 3\n",
    "            return kbar\n",
    "        kbar_initial = 0.6*np.float64(np.c_[ # was -0.1\n",
    "            np.ones(self.num_genes), # a_j\n",
    "            np.ones(self.num_genes), # b_j\n",
    "            np.ones(self.num_genes), # d_j\n",
    "            np.ones(self.num_genes)  # s_j\n",
    "        ])\n",
    "        def kbar_log_prob(all_states):\n",
    "            def kbar_log_prob_fn(kbar, δbar):\n",
    "#                 tf.print(kstar)\n",
    "                k = logit(kbar)\n",
    "#                 tf.print(k)\n",
    "                new_prob = self.likelihood.genes(\n",
    "                    all_states=all_states, \n",
    "                    state_indices=self.state_indices,\n",
    "                    kbar=kbar,\n",
    "                    δbar=δbar\n",
    "                )\n",
    "                new_prob += tf.reduce_sum(self.params.kbar.prior.log_prob(k))\n",
    "    \n",
    "                new_prob += self.params.δbar.prior.log_prob(logit(δbar))\n",
    "\n",
    "#                 tf.print(new_prob)\n",
    "                return tf.reduce_sum(new_prob)\n",
    "            return kbar_log_prob_fn\n",
    "        for j, k in enumerate(kbar_initial):\n",
    "            kbar_initial[j] = constrain_kbar(k, j)\n",
    "        kbar = Parameter('kbar', LogisticNormal(0.01, 8), \n",
    "                         kbar_initial, constraint=constrain_kbar)\n",
    "        \n",
    "        kinetics = Parameter('kinetics', None, \n",
    "                         [kbar.value, δbar.value],\n",
    "                         hmc_log_prob=kbar_log_prob,\n",
    "                         constraint=constrain_kbar, step_size=logistic_step_size, requires_all_states=True)\n",
    "        \n",
    "        if not options.preprocessing_variance:\n",
    "            σ2_f = Parameter('σ2_f', tfd.InverseGamma(f64(0.01), f64(0.01)), 1e-4*np.ones(self.num_tfs), step_size=tf.constant(0.5, dtype='float64'))\n",
    "            self.params = TupleParams_pre(fbar, δbar, kbar, σ2_m, w, w_0, L, V, kinetics, σ2_f)\n",
    "        else:\n",
    "            self.params = TupleParams(fbar, δbar, kbar, σ2_m, w, w_0, L, V, kinetics)\n",
    "            \n",
    "    def fbar_prior_params(self, vbar, l2bar):\n",
    "        v = logit(vbar, nan_replace=self.params.V.prior.b)\n",
    "        l2 = logit(l2bar, nan_replace=self.params.L.prior.b)\n",
    "\n",
    "#         tf.print('vl2', v, l2)\n",
    "        jitter = tf.linalg.diag(1e-10 * tf.ones(self.N_p, dtype='float64'))\n",
    "        K = tfm.multiply(v, tfm.exp(-tfm.square(self.t_dist)/(2*l2))) + jitter\n",
    "        m = tf.zeros((self.N_p), dtype='float64')\n",
    "        return m, K\n",
    "\n",
    "    def fbar_prior(self, fbar, v, l2):\n",
    "        m, K = self.fbar_prior_params(v, l2)\n",
    "        jitter = tf.linalg.diag(1e-6 *tf.ones(self.N_p, dtype='float64'))\n",
    "\n",
    "#         try:\n",
    "        return tfd.MultivariateNormalTriL(loc=m, scale_tril=tf.linalg.cholesky(K+jitter)).log_prob(fbar)\n",
    "#         except:\n",
    "#             jitter = tf.linalg.diag(1e-4 *tf.ones(self.N_p, dtype='float64'))\n",
    "#             try:\n",
    "#                 return tfd.MultivariateNormalFullCovariance(m, K+jitter).log_prob(fbar)\n",
    "#             except Exception as e:\n",
    "#                 tf.print(\"Fbar prior error\", e)\n",
    "#                 raise e\n",
    "#                 return tf.constant(-np.inf, dtype='float64')\n",
    "\n",
    "\n",
    "    def sample(self, T=2000, store_every=10, burn_in=1000, report_every=100, num_chains=4):\n",
    "        print('----- Sampling Begins -----')\n",
    "        \n",
    "        f = IntProgress(description='Running', min=0, max=T) # instantiate the bar\n",
    "        display(f)\n",
    "        params = self.params\n",
    "        progbar = tf.keras.utils.Progbar(\n",
    "            100, width=30, verbose=1, interval=0.05, stateful_metrics=None,\n",
    "            unit_name='step'\n",
    "        )\n",
    "\n",
    "        active_params = [\n",
    "            params.kinetics,\n",
    "            params.fbar,\n",
    "            params.V,\n",
    "            params.σ2_m,\n",
    "            #params.w,\n",
    "        ]\n",
    "        kernels = [param.kernel for param in active_params]\n",
    "#         if self.options.tf_mrna_present:\n",
    "        send_all_states = [param.requires_all_states for param in active_params]\n",
    "\n",
    "        current_state = [\n",
    "#             tf.stack([params.δbar.value for _ in range(num_chains)], axis=0),\n",
    "            params.kinetics.value, \n",
    "            params.fbar.value, \n",
    "            [*params.V.value],\n",
    "            params.σ2_m.value,\n",
    "            #[params.w.value, params.w_0.value]\n",
    "        ]\n",
    "        mixed_kern = MixedKernel(kernels, send_all_states)\n",
    "        \n",
    "        def trace_fn(a, previous_kernel_results):\n",
    "            return previous_kernel_results.is_accepted\n",
    "\n",
    "        # Run the chain (with burn-in).\n",
    "        @tf.function\n",
    "        def run_chain():\n",
    "            # Run the chain (with burn-in).\n",
    "            samples, is_accepted = tfp.mcmc.sample_chain(\n",
    "                  num_results=T,\n",
    "                  num_burnin_steps=burn_in,\n",
    "                  current_state=current_state,\n",
    "                  kernel=mixed_kern,\n",
    "                  trace_fn=trace_fn)\n",
    "\n",
    "            return samples, is_accepted\n",
    "\n",
    "        samples, is_accepted = run_chain()\n",
    "\n",
    "        add_to_previous = (self.samples is not None)\n",
    "        for param in active_params:\n",
    "            index = self.state_indices[param.name]\n",
    "            param_samples = samples[index]\n",
    "            if type(param_samples) is list:\n",
    "                param_samples = [[param_samples[i][-1] for i in range(len(param_samples))]]\n",
    "            \n",
    "            param.value = param_samples[-1]\n",
    "\n",
    "            if add_to_previous:\n",
    "                self.samples[index] = tf.concat([self.samples[index], samples[index]], axis=0)\n",
    "        \n",
    "        if not add_to_previous:\n",
    "            self.samples = samples     \n",
    "        self.is_accepted = is_accepted\n",
    "        f.value = T\n",
    "        print('----- Finished -----')\n",
    "        return samples, is_accepted\n",
    "        \n",
    "            \n",
    "    @staticmethod\n",
    "    def initialise_from_state(args, state):\n",
    "        model = TranscriptionMCMC(*args)\n",
    "        model.acceptance_rates = state.acceptance_rates\n",
    "        model.samples = state.samples\n",
    "        return model\n",
    "\n",
    "    def predict_m(self, kbar, δbar, w, fbar, w_0):\n",
    "        return self.likelihood.predict_m(kbar, δbar, w, fbar, w_0)\n",
    "\n",
    "    def predict_m_with_current(self):\n",
    "        return self.likelihood.predict_m(self.params.kbar.value, \n",
    "                                         self.params.δbar.value, \n",
    "                                         self.params.w.value, \n",
    "                                         self.params.fbar.value,\n",
    "                                         self.params.w_0.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.4f}\".format(x)})\n",
    "T = 1000\n",
    "store_every = 1\n",
    "burn_in = 0\n",
    "report_every = 20\n",
    "num_chains = 4\n",
    "tune_every = 50\n",
    "\n",
    "model = TranscriptionCustom(data, opt)\n",
    "\n",
    "samples, is_accepted = model.sample(T=200, burn_in=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = list()\n",
    "for i, param in enumerate(model.state_indices):\n",
    "    print(i)\n",
    "    if i == 4:\n",
    "        break\n",
    "    pcs.append(tf.reduce_mean(tf.cast(is_accepted[i], dtype=tf.float32)).numpy())\n",
    "\n",
    "display(pd.DataFrame([[f'{100*pc:.02f}%' for pc in pcs]], columns=list(model.state_indices)[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kbar = model.samples[model.state_indices['kinetics']][0]\n",
    "δbar = model.samples[model.state_indices['kinetics']][1]\n",
    "fbar = model.samples[model.state_indices['fbar']]\n",
    "print(fbar.shape, δbar.shape)\n",
    "σ2_m = model.samples[model.state_indices['σ2_m']]\n",
    "rbf_params = model.samples[model.state_indices['rbf_params']]\n",
    "\n",
    "# w = model.samples[model.state_indices['w']][0]\n",
    "# w_0 = model.samples[model.state_indices['w']][1]\n",
    "\n",
    "w = [1*tf.ones((num_genes, 1), dtype='float64')] # TODO\n",
    "w_0 = [tf.zeros(num_genes, dtype='float64')] # TODO\n",
    "\n",
    "m_preds = list()\n",
    "for i in range(1, 20):\n",
    "    m_preds.append(model.likelihood.predict_m(kbar[-i], δbar[-i], w[-1], fbar[-i][0], w_0[-1])) #todo w[-1]\n",
    "m_preds = np.array(m_preds)\n",
    "\n",
    "f_samples = np.log(1+np.exp(fbar))\n",
    "δ_samples = logit(δbar)\n",
    "k_samples = logit(kbar)\n",
    "rbf_params_samples = [logit(rbf_params[0]), logit(rbf_params[1])] \n",
    "\n",
    "\n",
    "\n",
    "plotters.generate_report(data, k_samples, δ_samples, f_samples, \n",
    "                         σ2_m, rbf_params_samples, m_preds, \n",
    "                         plot_barenco=True, gene_names=m_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barenco = True\n",
    "plt.plot(fbar[:, 0, 0])\n",
    "fig = plt.figure(figsize=(13, 7))\n",
    "f_samples = np.log(1+np.exp(fbar[-100:, 0,:]))\n",
    "print(f_samples.shape)\n",
    "if 'σ2_f' in model.params._fields:\n",
    "    σ2_f = model.params.σ2_f.value\n",
    "    plt.errorbar(τ[common_indices], f_observed[0], 2*np.sqrt(σ2_f[0]), \n",
    "                 fmt='none', capsize=5, color='blue')\n",
    "else:\n",
    "    σ2_f = σ2_f_pre\n",
    "    \n",
    "bounds = arviz.hpd(f_samples, credible_interval=0.95)\n",
    "for i in range(1,20):\n",
    "    f_i = f_samples[-i]\n",
    "#     plt.plot(f_i)\n",
    "#     f_i[0] = 0\n",
    "    kwargs = {}\n",
    "    if i == 1:\n",
    "        kwargs = {'label':'Samples'}\n",
    "    plt.plot(τ, f_i, c='cadetblue', alpha=0.5, **kwargs)\n",
    "\n",
    "# if plot_barenco:\n",
    "#     barenco_f, _ = scaled_barenco_data(np.mean(f_samples[-10:], axis=0))\n",
    "#     plt.scatter(τ[common_indices], barenco_f, marker='x', s=60, linewidth=3, label='Barenco et al.')\n",
    "\n",
    "plt.scatter(τ[common_indices], f_observed[0], marker='x', s=70, linewidth=3, label='Observed')\n",
    "\n",
    "plt.fill_between(τ, bounds[:, 0], bounds[:, 1], color='grey', alpha=0.3, label='95% credibility interval')\n",
    "plt.xticks(t)\n",
    "fig.axes[0].set_xticklabels(t)\n",
    "plt.ylim((-1,5))\n",
    "plt.xlabel('Time (h)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "num_genes = kbar.shape[1]\n",
    "k_latest = np.mean(logit(kbar[-10:]), axis=0)\n",
    "print(k_latest)\n",
    "B = k_latest[:,1]\n",
    "D = k_latest[:,2]\n",
    "S = k_latest[:,3]\n",
    "\n",
    "plt.bar(np.arange(num_genes)-0.2, B, width=0.2, tick_label=m_df.index, label='Basal rate')\n",
    "plt.bar(np.arange(num_genes), D, width=0.2, tick_label=m_df.index, label='Sensitivity')\n",
    "plt.bar(np.arange(num_genes)+0.2, S, width=0.2, tick_label=m_df.index, label='Decay rate')\n",
    "plt.yscale('log')\n",
    "plt.title('Mechanistic Parameters')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for j in range(num_genes):\n",
    "    plt.plot(w[:, j], label=m_df.index[j])\n",
    "plt.legend()\n",
    "plt.title('Interaction weights')\n",
    "\n",
    "plt.figure()\n",
    "for j in range(num_genes):\n",
    "    plt.plot(w_0[:,j])\n",
    "plt.title('Interaction bias')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.title('Noise variances')\n",
    "for i, j in enumerate(range(num_genes)):\n",
    "    ax = plt.subplot(num_genes, num_genes-2, i+1)\n",
    "    plt.title(m_df.index[j])\n",
    "    plt.plot(σ2_m[:,j])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = job[0].acceptance_rates.keys()\n",
    "\n",
    "variables = {key : np.empty((0, T, *job[0].samples[key].get().shape[1:])) for key in keys}\n",
    "\n",
    "for res in job:\n",
    "    for key in keys:\n",
    "        variables[key] = np.append(variables[key], np.expand_dims(res.samples[key].get(), 0), axis=0)\n",
    "\n",
    "plt.plot(variables['L'][:,-100:].T)\n",
    "\n",
    "mixes = {key: arviz.convert_to_inference_data(variables[key]) for key in keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rhat\n",
    "Rhat is the ratio of posterior variance and within-chain variance. If the ratio exceeds 1.1 then we consider the chains have not mixed well. As the between-chain variance tends to the within-chain then R tends to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rhat = arviz.rhat(mixes['fbar'])\n",
    "\n",
    "Rhats = np.array([np.mean(arviz.rhat(mixes[key]).x.values) for key in keys])\n",
    "\n",
    "rhat_df = pd.DataFrame([[*Rhats], [*(Rhats < 1.1)]], columns=keys)\n",
    "\n",
    "display(rhat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank plots\n",
    "\n",
    "Rank plots are histograms of the ranked posterior draws (ranked over all\n",
    "    chains) plotted separately for each chain.\n",
    "    If all of the chains are targeting the same posterior, we expect the ranks in each chain to be\n",
    "    uniform, whereas if one chain has a different location or scale parameter, this will be\n",
    "    reflected in the deviation from uniformity. If rank plots of all chains look similar, this\n",
    "    indicates good mixing of the chains.\n",
    "\n",
    "Rank-normalization, folding, and localization: An improved R-hat\n",
    "    for assessing convergence of MCMC. arXiv preprint https://arxiv.org/abs/1903.08008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_rank(L_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effective sample sizes\n",
    "\n",
    "Plot quantile, local or evolution of effective sample sizes (ESS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_ess(L_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte-Carlo Standard Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_mcse(L_mix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Plot\n",
    "Plot parallel coordinates plot showing posterior points with and without divergences.\n",
    "\n",
    "Described by https://arxiv.org/abs/1709.01449, suggested by Ari Hartikainen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arviz.plot_parallel(azl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step size is standard dev, too small means it takes long time to reach high density areas. too long means we reject many of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TranscriptionCustom(data, opt)\n",
    "fbar = tf.constant([[0.54880, 0.56062, 0.54973, 0.53923, 0.53923, 0.53892, 0.54077, 0.54334, 0.54586, 0.54953,\n",
    "  0.55446, 0.55897, 0.56173, 0.56379, 0.56612, 0.56790, 0.56781, 0.56570, 0.56276, 0.5914,\n",
    "  0.55507, 0.55068, 0.533, 0.54217, 0.53848, 0.53549, 0.53362, 0.53301, 0.53338, 0.53438,\n",
    "  0.53593, 0.53800, 0.54037, 0.54267, 0.54481, 0.54645, 0.54760, 0.54818, 0.54817, 0.54774,\n",
    "  0.54694, 0.54583, 0.5455, 0.54336, 0.54219, 0.54136, 0.54083, 0.54063, 0.54097, 0.54148,\n",
    "  0.54237, 0.54342, 0.54465, 0.510, 0.54804, 0.557, 0.5285, 0.5545, 0.55770, 0.5932,\n",
    "  0.56021, 0.56030, 0.55959, 0.534, 0.5654, 0.5417, 0.5162]], dtype='float64')\n",
    "print(fbar.shape)\n",
    "#  1.4018160215788704 4.4271686734681195\n",
    "print(model.params.V.prior.log_prob(1.3818811998078957))\n",
    "print(model.params.L.prior.log_prob(3.9525443383480283))\n",
    "\n",
    "# fbar = model.params.fbar.value\n",
    "print(model.params.fbar.prior(fbar, f64(0.6), f64(0.99)))\n",
    "#  3.9525443383480283"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit374b75da0e1b40de8b7922d3f142c01d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
